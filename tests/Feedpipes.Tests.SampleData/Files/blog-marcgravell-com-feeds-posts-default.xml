<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:georss="http://www.georss.org/georss" xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr="http://purl.org/syndication/thread/1.0" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><id>tag:blogger.com,1999:blog-8184237816669520763</id><updated>2019-05-07T01:15:02.653-07:00</updated><category term="c#" /><category term="protobuf-net" /><category term=".net" /><category term="4.0" /><category term="serialization" /><category term="linq" /><category term="expression" /><category term="me" /><category term="community" /><category term="redis" /><category term="reflection" /><category term="cf" /><category term="ide" /><category term="metaprogramming" /><category term="pipelines" /><category term="dynamic" /><category term="generics" /><category term="mvc" /><category term="5.0" /><category term="asp.net" /><category term="async" /><category term="disposable" /><category term="phone-7" /><category term="stackexchange" /><category term="stackoverflow" /><category term="surface" /><category term="autism" /><category term="c# .net 5.0 async redis" /><category term="c# linq" /><category term="c# profiling" /><category term="ikvm" /><category term="immutability" /><category term="linq expression c#" /><category term="tpl" /><title type="text">Code, code and more code.</title><subtitle type="html" /><link rel="alternate" type="text/html" href="http://blog.marcgravell.com/" /><link rel="next" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default?start-index=26&amp;max-results=25&amp;redirect=false" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><generator version="7.00" uri="http://www.blogger.com">Blogger</generator><openSearch:totalResults>168</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/CodeCodeAndMoreCode" /><feedburner:info uri="codecodeandmorecode" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><geo:lat>51.8167</geo:lat><geo:long>-2.4833</geo:long><link rel="license" type="text/html" href="http://creativecommons.org/licenses/by/2.0/" /><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-8908076159438940521</id><published>2019-02-21T05:17:00.002-08:00</published><updated>2019-02-21T09:17:30.208-08:00</updated><title type="text">Fun with the Spiral of Death</title><content type="html">&lt;p&gt;Subtitled: "a cautionary tale of &lt;code&gt;SemaphoreSlim&lt;/code&gt;", an adventure in two parts:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;In part 1 I want to discuss a very fun series of problems we had in some asynchronous code - where "fun" here means "I took Stack Overflow offline, again". Partly because it is a fun story, but mostly because I think there's some really useful learning points in there for general adventures in asynchronicity&lt;/li&gt;&lt;li&gt;In part 2 I want to look at some of the &lt;em&gt;implementation details&lt;/em&gt; of our eventual fix, which covers some slightly more advanced themes around how to implement awaitable code in non-trivial scenarios&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;a id="user-content-i-took-stack-overflow-offline-again" class="anchor" aria-hidden="true" href="#i-took-stack-overflow-offline-again"&gt;&lt;/a&gt;I took Stack Overflow offline, again&lt;/h2&gt;&lt;p&gt;As a side note: many of the themes here run hand-in-hand with David and Damian's recent presentation "Why your ASP.NET Core application won't scale" at NDC; if you haven't seen it yet: &lt;a href="https://www.youtube.com/watch?v=-cJjnVTJ5og" rel="nofollow"&gt;&lt;em&gt;go watch it&lt;/em&gt;&lt;/a&gt; - in particular everything around "the application works fine until it suddenly doesn't" and "don't sync-over-async or async-over-sync".&lt;/p&gt;&lt;p&gt;A lot of this journey relates to our migration of &lt;a href="https://github.com/StackExchange/StackExchange.Redis" rel="nofollow"&gt;StackExchange.Redis&lt;/a&gt; to use "pipelines", the new IO layer in .NET (previously discussed &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html" rel="nofollow"&gt;here&lt;/a&gt;, &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html" rel="nofollow"&gt;here&lt;/a&gt;, &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-3.html" rel="nofollow"&gt;here&lt;/a&gt;, and &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-31.html" rel="nofollow"&gt;here&lt;/a&gt; - I love me some pipelines). One of the key design choices in StackExchange.Redis is for the library to implement &lt;a href="https://stackexchange.github.io/StackExchange.Redis/PipelinesMultiplexers" rel="nofollow"&gt;multiplexing&lt;/a&gt; to allow multiple concurrent calling threads to communicate over the same underlying socket to the server; this keeps the socket count low while also helping to reduce packet fragmentation, but it means that we need to do some synchronization around how the many caller threads access the underlying socket.&lt;/p&gt;&lt;p&gt;Before the pipeline migration, this code was &lt;em&gt;basically&lt;/em&gt; synchronous (it was a bit more complex, but… that's close enough), and the "write an actual command" code could be expressed (if we take some liberties for readability) as below:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-smi"&gt;syncLock&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-k"&gt;object&lt;/span&gt;(); &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; single writer&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;WriteMessage&lt;/span&gt;(&lt;span class="pl-en"&gt;Message&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-smi"&gt;haveLock&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;Monitor&lt;/span&gt;.&lt;span class="pl-en"&gt;TryEnter&lt;/span&gt;(&lt;span class="pl-smi"&gt;syncLock&lt;/span&gt;, &lt;span class="pl-smi"&gt;timeout&lt;/span&gt;, &lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;) &lt;span class="pl-en"&gt;ThrowTimeout&lt;/span&gt;();&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;ActuallyWriteTheThing&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-en"&gt;Flush&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;) &lt;span class="pl-smi"&gt;Monitor&lt;/span&gt;.&lt;span class="pl-en"&gt;Exit&lt;/span&gt;(&lt;span class="pl-smi"&gt;syncLock&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is a fairly normal style of coding - the &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;finally&lt;/code&gt;/&lt;code&gt;Monitor&lt;/code&gt;/&lt;code&gt;haveLock&lt;/code&gt; code here is just a standard implementation of "&lt;code&gt;lock&lt;/code&gt; with a timeout", so all this really does is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;try to acquire exclusive access to the socket, guarded by &lt;code&gt;syncLock&lt;/code&gt;&lt;/li&gt;&lt;li&gt;if successful, write and flush&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All reasonable. But then we moved to pipelines, and one of the defining features of the pipelines implementation is that key steps in it are &lt;code&gt;async&lt;/code&gt;. You might assume that it is the &lt;em&gt;write&lt;/em&gt; that is &lt;code&gt;async&lt;/code&gt; - but since you write to a buffer pool, this isn't actually the case - it's the &lt;em&gt;flush&lt;/em&gt; that is &lt;code&gt;async&lt;/code&gt;. The &lt;em&gt;flush&lt;/em&gt; in pipelines achieves a few different things:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;if necessary, it activates the &lt;em&gt;consumer&lt;/em&gt; that is pulling work from the pipe and sending it to the next step (a socket in our case)&lt;/li&gt;&lt;li&gt;it provides back-pressure to the &lt;em&gt;provider&lt;/em&gt; (&lt;code&gt;WriteMessage&lt;/code&gt; in this case), so that if the consumer is falling behind and there's too much backlog, we can slow down the provider (in an asynchronous way) so we don't get unbounded buffer growth&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All very neat.&lt;/p&gt;&lt;p&gt;But switching from synchronous code to an API that uses &lt;code&gt;async&lt;/code&gt; is not always trivial - &lt;code&gt;async&lt;/code&gt; begets &lt;code&gt;async&lt;/code&gt;, and once you start going &lt;code&gt;async&lt;/code&gt;, it &lt;em&gt;all&lt;/em&gt; goes &lt;code&gt;async&lt;/code&gt;. So… I did a bad thing; I was lazy, and figured "hey, flush will almost always complete synchronously anyway; we can probably get away with a sync-over-async here" (&lt;em&gt;narrator: they didn't get away with it&lt;/em&gt;).&lt;/p&gt;&lt;p&gt;So; what I did was something like:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-k"&gt;object&lt;/span&gt; &lt;span class="pl-smi"&gt;syncLock&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-k"&gt;object&lt;/span&gt;(); &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; single writer&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;WriteMessage&lt;/span&gt;(&lt;span class="pl-en"&gt;Message&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-smi"&gt;haveLock&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;Monitor&lt;/span&gt;.&lt;span class="pl-en"&gt;TryEnter&lt;/span&gt;(&lt;span class="pl-smi"&gt;syncLock&lt;/span&gt;, &lt;span class="pl-smi"&gt;timeout&lt;/span&gt;, &lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;) &lt;span class="pl-en"&gt;ThrowTimeout&lt;/span&gt;();&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;ActuallyWriteTheThing&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-en"&gt;FlushSync&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;haveLock&lt;/span&gt;) &lt;span class="pl-smi"&gt;Monitor&lt;/span&gt;.&lt;span class="pl-en"&gt;Exit&lt;/span&gt;(&lt;span class="pl-smi"&gt;syncLock&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;FlushSync&lt;/span&gt;() &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; evil hack, DO NOT USE&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompletedSuccessfully&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;flush&lt;/span&gt;.&lt;span class="pl-en"&gt;Wait&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;IsCompletedSuccessfully&lt;/code&gt; here is a check you can use on many task-like (awaitable) results to see if it completed synchronously and without faulting; if it did, you're safe to access the &lt;code&gt;.Result&lt;/code&gt; (etc.) and it will all be available already - a good trick for avoiding the &lt;code&gt;async&lt;/code&gt; state-machine complications in high-throughput code (typically library code, not application code). The bad bit is the &lt;code&gt;.Wait(…)&lt;/code&gt; when it &lt;em&gt;isn't&lt;/em&gt; already completed - this is a sync-over-async.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-what-happened-next" class="anchor" aria-hidden="true" href="#what-happened-next"&gt;&lt;/a&gt;What happened next?&lt;/h2&gt;&lt;p&gt;A key thing to keep in mind is that StackExchange.Redis exposes both synchronous and asynchronous APIs - i.e. there are twin methods, for example:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;RedisValue StringGet(RedisKey key)&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;Task&amp;lt;RedisValue&amp;gt; StringGetAsync(RedisKey key)&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Internally they are implemented very differently so that they both get the job done with the minimum of fuss and overhead, but they were both calling into the same &lt;code&gt;WriteMessage&lt;/code&gt; at some point. Actually, never afraid to double-down on the anti-patterns, this means that for the &lt;em&gt;async&lt;/em&gt; callers, they were effectively doing async-over-sync-over-async; ouch.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;WriteMessage&lt;/code&gt; code above is used from both the &lt;em&gt;synchronous&lt;/em&gt; and &lt;em&gt;asynchronous&lt;/em&gt; call paths. As it happens, much of our internal existing &lt;em&gt;application&lt;/em&gt; codebase mostly uses the synchronous paths (we're gradually adding more &lt;code&gt;async&lt;/code&gt;, but we need to complete our in-progress transition from .NET Framework to .NET Core to be able to do it more extensively), and on the synchronous paths you were always going to be blocked &lt;em&gt;anyway&lt;/em&gt;, so from the perspective of synchronous callers, there's not really that much wrong with the above. It does what it promises: execute synchronously.&lt;/p&gt;&lt;p&gt;The &lt;em&gt;problem&lt;/em&gt; here comes from &lt;em&gt;asynchronous&lt;/em&gt; callers, who thought they were calling &lt;code&gt;StringGetAsync&lt;/code&gt;, and their thread got blocked. The golden rule of &lt;code&gt;async&lt;/code&gt; is: don't block an &lt;code&gt;async&lt;/code&gt; caller unless you &lt;em&gt;really, really&lt;/em&gt; have to. We broke this rule, and we had reports from users about big thread-jams with &lt;code&gt;async&lt;/code&gt; call paths all stuck at &lt;code&gt;WriteMessage&lt;/code&gt;, because &lt;em&gt;one thread&lt;/em&gt; had paused for the flush, and all the other threads were trying to obtain the lock.&lt;/p&gt;&lt;p&gt;Note: the problem here isn't that "a backlog happened, and we had to delay" - that's just business as normal. That happens, especially when you need mutex-like semantics. The problem is that we &lt;em&gt;blocked the worker threads&lt;/em&gt; (although we did at least have the good grace to include a timeout), which &lt;em&gt;under heavy load&lt;/em&gt; caused thread-pool starvation and a cascading failure (again: watch the video above).&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-so-what-should-we-have-done-in-theory" class="anchor" aria-hidden="true" href="#so-what-should-we-have-done-in-theory"&gt;&lt;/a&gt;So what &lt;em&gt;should&lt;/em&gt; we have done &lt;strong&gt;in theory&lt;/strong&gt;?&lt;/h2&gt;&lt;p&gt;Given that we have both synchronous and asynchronous call-paths, what we &lt;em&gt;should&lt;/em&gt; do is have two versions of the write code:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;void WriteMessage(Message message)&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;ValueTask WriteMessageAsync(Message message)&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;but we get into immediate problems when we talk about our locking mechanism. We can see this more clearly if we use a simple &lt;code&gt;lock&lt;/code&gt; rather than the more complex &lt;code&gt;Monitor&lt;/code&gt; usage above - the following &lt;strong&gt;does not compile&lt;/strong&gt;:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;Foo&lt;/span&gt;()&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;lock&lt;/span&gt; (&lt;span class="pl-smi"&gt;syncLock&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; CS1996 - Cannot await in the body of a lock statement&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;Task&lt;/span&gt;.&lt;span class="pl-en"&gt;Delay&lt;/span&gt;(&lt;span class="pl-smi"&gt;SomeAmount&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The reason this doesn't work is that &lt;code&gt;lock&lt;/code&gt; (aka &lt;code&gt;Monitor&lt;/code&gt;) is &lt;strong&gt;thread-oriented&lt;/strong&gt;. You need to &lt;code&gt;[Try]Enter&lt;/code&gt; (take the lock) and &lt;code&gt;Exit&lt;/code&gt; (release the lock) the constrained region from the same thread. But the moment you &lt;code&gt;await&lt;/code&gt;, you're saying "this might continue synchronously, or it might resume later &lt;em&gt;on a different thread&lt;/em&gt;". This actually has two consequences:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it would mean that when we try to release the lock, it will fail because the resuming thread probably won't actually have it&lt;/li&gt;&lt;li&gt;when we &lt;code&gt;await&lt;/code&gt;, we're releasing the &lt;em&gt;current&lt;/em&gt; thread back to do &lt;em&gt;whatever else needs doing&lt;/em&gt;… which &lt;em&gt;could&lt;/em&gt; actually end up calling back into &lt;code&gt;Foo&lt;/code&gt;… and &lt;code&gt;Monitor&lt;/code&gt; is "re-entrant", meaning: if you have the lock &lt;em&gt;once&lt;/em&gt;, you can actually &lt;code&gt;lock&lt;/code&gt; &lt;strong&gt;again&lt;/strong&gt; successfully (it maintains a counter internally), which means that code in a completely unrelated execution context could incorrectly end up &lt;strong&gt;inside&lt;/strong&gt; the &lt;code&gt;lock&lt;/code&gt;, &lt;em&gt;before&lt;/em&gt; we've resumed from the &lt;code&gt;await&lt;/code&gt; and logically released it&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As a side note, it is worth knowing that the compiler &lt;em&gt;only&lt;/em&gt; spots this (CS1996) if you use &lt;code&gt;lock&lt;/code&gt;; if you use manual &lt;code&gt;Monitor&lt;/code&gt; code (because of timeouts), it won't warn you - you just need to know not to do this (which perhaps by itself is good motivation for "&lt;code&gt;lock&lt;/code&gt; with timeout" as a language feature). Fortunately, I &lt;em&gt;did&lt;/em&gt; know not to do this - and I moved to the next most obvious locking primitive: &lt;a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.semaphoreslim" rel="nofollow"&gt;&lt;code&gt;SemaphoreSlim&lt;/code&gt;&lt;/a&gt;. A semaphore is like &lt;code&gt;Monitor&lt;/code&gt;, but instead of being thread-based, it is purely counter-based. Theoretically you can use a semaphore to say "no more than 5 in here", but in reality it is often used as a mutex by saying "no more than 1". &lt;code&gt;SemaphoreSlim&lt;/code&gt; is particularly enticing because it has both synchronous and asynchronous APIs, allowing us to split our code in two fairly neatly:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-smi"&gt;SemaphoreSlim&lt;/span&gt; &lt;span class="pl-smi"&gt;singleWriter&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;SemaphoreSlim&lt;/span&gt;(&lt;span class="pl-c1"&gt;1&lt;/span&gt;); &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; single writer&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;WriteMessage&lt;/span&gt;(&lt;span class="pl-en"&gt;Message&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Wait&lt;/span&gt;(&lt;span class="pl-smi"&gt;timeout&lt;/span&gt;))&lt;br /&gt;        &lt;span class="pl-en"&gt;ThrowTimeout&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-en"&gt;ActuallyWriteTheThing&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-en"&gt;FlushSync&lt;/span&gt;(); &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; our hack from before&lt;/span&gt;&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Release&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteMessageAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;Message&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;WaitAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;timeout&lt;/span&gt;))&lt;br /&gt;        &lt;span class="pl-en"&gt;ThrowTimeout&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-en"&gt;ActuallyWriteTheThing&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Release&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This looks broadly similar to what we had before; the &lt;code&gt;new SemaphoreSlim(1)&lt;/code&gt; initializes the semaphore with a limit of 1, i.e. a mutex. In the synchronous path, it works mostly like it always did, but the asynchronous path (now used by the asynchronous callers) now &lt;em&gt;correctly&lt;/em&gt; releases worker threads back to wherever worker threads go - when either they can't get the lock yet, &lt;em&gt;or&lt;/em&gt; when they are waiting (or rather: awaiting) on the flush. We still have the sync-over-async in the sync path, but that's not really a problem in this case - but we've completely fixed the async path. Short of &lt;em&gt;removing&lt;/em&gt; or &lt;a href="https://github.com/aspnet/Announcements/issues/342" rel="nofollow"&gt;&lt;em&gt;optionally disabling&lt;/em&gt; the sync path&lt;/a&gt; (which is an idea I'm putting serious thought into doing, as an opt-in thing), that's probably about as good as we can get.&lt;/p&gt;&lt;p&gt;This looks like it should work, and the chances are that this would have completely solved the problems being seen by our consumers with heavily asynchronous workloads. But one of the nice things about working at Stack Overflow is that I have an opportunity to dogfood library releases under Stack Overflow load (which isn't "big big" by any stretch, but it is comfortably big enough to give me confidence that the library isn't pathologically broken). So, we dropped the above changes into production (after testing etc.), and: BOOM!&lt;/p&gt;&lt;p&gt;We went down.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-what-happened-there" class="anchor" aria-hidden="true" href="#what-happened-there"&gt;&lt;/a&gt;What happened there?&lt;/h2&gt;&lt;p&gt;Fortunately, we were lucky enough to manage to grab some process dumps from the production servers in their death-throes before we stood them back up (with the older version of the library), and the stack-traces in the doomed processes were very interesting; they are pretty verbose, but something that kept recurring (note: I've inverted and summarised this trace for readability):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;WriteMessage&lt;br /&gt;…&lt;br /&gt;System.Threading.SemaphoreSlim.Wait&lt;br /&gt;…&lt;br /&gt;System.Threading.SemaphoreSlim.WaitAsync&lt;br /&gt;…&lt;br /&gt;KERNELBASE!WaitForMultipleObjectsEx&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This was the case for 650+ threads - almost all of them; and critically, &lt;strong&gt;no-one&lt;/strong&gt; actually had the lock - nobody was doing anything useful. The semaphore had, in an edge case, failed to activate the lucky winner of the &lt;a href="https://en.wikipedia.org/wiki/Lord_of_the_Flies" rel="nofollow"&gt;conch&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-what-actually-went-wrong" class="anchor" aria-hidden="true" href="#what-actually-went-wrong"&gt;&lt;/a&gt;What actually went wrong?&lt;/h3&gt;&lt;p&gt;Looking at it, our &lt;em&gt;synchronous&lt;/em&gt; &lt;code&gt;WriteMessage&lt;/code&gt; implementation, when calling &lt;code&gt;Wait&lt;/code&gt; on the semaphore, was calling into &lt;code&gt;WaitAsync&lt;/code&gt;, and then blocking at the kernel for the object. Despite looking odd, this by itself &lt;em&gt;isn't actually&lt;/em&gt; a terrible idea. It turns out that &lt;code&gt;SemaphoreSlim&lt;/code&gt; has different strategies that it uses internally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;if you're just using synchronous &lt;code&gt;Wait&lt;/code&gt; calls, it can handle everything using regular synchronous code and syncronous blocks&lt;/li&gt;&lt;li&gt;if you're just using &lt;code&gt;WaitAsync&lt;/code&gt;, because it wants to release the caller promptly, it needs to maintain a queue (actually a linked-list) of waiting callers as &lt;code&gt;Task&amp;lt;bool&amp;gt;&lt;/code&gt;; when you release something, it takes the next item from one end of the list, and reactivates (&lt;code&gt;TrySetResult&lt;/code&gt;) that caller&lt;/li&gt;&lt;li&gt;if you're using a mixture of &lt;code&gt;Wait&lt;/code&gt; and &lt;code&gt;WaitAsync&lt;/code&gt;, if it can't get access immediately, then it uses the &lt;code&gt;WaitAsync&lt;/code&gt; approach so that the &lt;code&gt;Wait&lt;/code&gt; and &lt;code&gt;WaitAsync&lt;/code&gt; consumers are in the same queue - otherwise you'd have two separate queues and it gets very confusing and unpredictable&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now this &lt;em&gt;seems&lt;/em&gt; fine, but it turns out that the way it was using &lt;code&gt;TrySetResult&lt;/code&gt; was… problematic. It wasn't using &lt;code&gt;TrySetResult&lt;/code&gt; &lt;em&gt;directly&lt;/em&gt;, but instead was &lt;em&gt;enqueuing&lt;/em&gt; a work item to do the &lt;code&gt;TrySetResult&lt;/code&gt;. There's actually a good - albeit now legacy - reason for this: thread stealing, &lt;em&gt;another&lt;/em&gt; problem I've had to contend with many times.&lt;/p&gt;&lt;p&gt;When you call &lt;code&gt;TrySetResult&lt;/code&gt; etc. on a &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; (usually via &lt;code&gt;TaskCompletionSource&amp;lt;T&amp;gt;&lt;/code&gt;), it is possible (likely, even) that the &lt;em&gt;async continuation&lt;/em&gt; is going to run immediately and inline &lt;strong&gt;on the thread that called &lt;code&gt;TrySetResult&lt;/code&gt;.&lt;/strong&gt; This is something you need to be really careful about - it can lead to dedicated IO threads somehow ending up serving web requests; or more generally: just … not doing what you expected. But in the scenario presented we got into a "spiral of death": due to a very brief blip from the &lt;code&gt;FlushAsync&lt;/code&gt;, our workers had got stuck in the &lt;code&gt;Wait&lt;/code&gt;-&amp;gt;&lt;code&gt;WaitAsync&lt;/code&gt; path, and the &lt;strong&gt;very thing&lt;/strong&gt; that was meant to unblock everything: needed a worker. To release (resource) you need more of (resource), and (resource) is currently exhausted. It is &lt;strong&gt;almost impossible&lt;/strong&gt; to recover from that situation due to the growth limits on workers, and the servers became increasingly unstable until they stopped working completely.&lt;/p&gt;&lt;p&gt;This is clearly a dangerous scenario, so we reported it as an issue, and amazingly within a day Stephen Toub had a &lt;a href="https://github.com/dotnet/corefx/commit/ecba811b1438517ac90a957e2cfe4cef64a13861" rel="nofollow"&gt;surprisingly minimal and elegant fix for &lt;code&gt;SemaphoreSlim&lt;/code&gt;&lt;/a&gt;. The commit message (and code changes themselves) explain it in more depth, but by configuring the queued tasks with the &lt;code&gt;TaskCreationOptions.RunContinuationsAsynchronously&lt;/code&gt; flag, it means the "release" code can call &lt;code&gt;TrySetResult&lt;/code&gt; &lt;strong&gt;directly&lt;/strong&gt;, without needing an extra worker as an intermediary. In the specific case where the only thing waiting on the task is a synchronous &lt;code&gt;Wait&lt;/code&gt;, the task code already has specific detection to unblock that scenario directly without needing a worker &lt;em&gt;at all&lt;/em&gt;, and in the genuine &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; case, we just end up with the &lt;em&gt;actual work&lt;/em&gt; going to the queue, rather than the "call &lt;code&gt;TrySetResult&lt;/code&gt;" going to the queue. Tidy!&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-but-that-isnt-the-end-of-the-story" class="anchor" aria-hidden="true" href="#but-that-isnt-the-end-of-the-story"&gt;&lt;/a&gt;But that isn't the end of the story&lt;/h2&gt;&lt;p&gt;It would be nice to say "all's well that ends well; bug in &lt;code&gt;SemaphoreSlim&lt;/code&gt; fixed", but it isn't as easy as that. The fix for &lt;code&gt;SemaphoreSlim&lt;/code&gt; &lt;em&gt;has&lt;/em&gt; been merged, but a) that won't help "us" until the next .NET Framework service release, and b) as library authors, we can't rely on which service releases are on our &lt;em&gt;consumers'&lt;/em&gt; machines. We need a fix that works reliably everywhere. So whilst it is great to know that our pain has improved things for future users of &lt;code&gt;SemaphoreSlim&lt;/code&gt;, we needed something more immediate and framework-independent. So that's when I went away and created a bespoke synchronous/asynchronous &lt;code&gt;MutexSlim&lt;/code&gt; that we are now using in StackExchange.Redis.&lt;/p&gt;&lt;p&gt;It is &lt;em&gt;amazing&lt;/em&gt; how much simpler things become if you limit yourself to "0 or 1 people in the pool", so it wasn't &lt;em&gt;actually&lt;/em&gt; that much work; but: I thought I knew a lot about &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;, yet in writing &lt;code&gt;MutexSlim&lt;/code&gt; I dove deeper into that topic than I have usually had to; and in the second part I'll talk about some of what I learned.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/AsT3vpD-BnY" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/8908076159438940521" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/8908076159438940521" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/AsT3vpD-BnY/fun-with-spiral-of-death.html" title="Fun with the Spiral of Death" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2019/02/fun-with-spiral-of-death.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-2436815592271301061</id><published>2018-12-06T06:34:00.002-08:00</published><updated>2018-12-06T06:52:28.274-08:00</updated><title type="text">A Thanksgiving Carol</title><content type="html">&lt;p&gt;Normally I write about programming topics (usually .NET); today I'm going to veer very far from that track - and talk about society, mental health, individual and corporate responsibility, and personal relationships. I genuinely hope you hear me out, but if that isn't your thing ... well, then you probably need to read it more than most. I could try a clever reverse psychology trick to oblige you to see it through, but you'd see straight through it... or would you?&lt;/p&gt;&lt;p&gt;My apologies in advance if I seem to be on a negative tone through much of this - I'm pulling no punches in something that has been a quite deep - and painful - personal journey and realisation. I assure you that it ends much more positively than the body might suggest. Maybe for me this is mostly cathartic self-indulgence and rambling, but.. it's my personal blog and I get to do that if I want. But if it makes even one person think for a few minutes, it has been well worth my time.&lt;/p&gt;&lt;p&gt;So; on with the real title:&lt;/p&gt;&lt;h1&gt;&lt;a id="user-content-technology-is-outpacing-our-individual-and-societal-health" class="anchor" aria-hidden="true" href="#technology-is-outpacing-our-individual-and-societal-health"&gt;&lt;/a&gt;Technology is Outpacing our Individual and Societal Health&lt;/h1&gt;&lt;p&gt;This week, I've identified hugely with that famous (infamous?) festive favorite: Ebenezer Scrooge (humbug!). Not the usury part - but instead:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the familiar story of spending a long time making choices that cause harm&lt;/li&gt;&lt;li&gt;having some catastrophic event or events bring everything into focus&lt;/li&gt;&lt;li&gt;having a genuine yet painful inspection of those past (and present) choices&lt;/li&gt;&lt;li&gt;consideration of what those choices mean for the future&lt;/li&gt;&lt;li&gt;undergoing a fundamental transformation, a realignment of priorities and thinking, that should lead to a much happier future&lt;/li&gt;&lt;li&gt;actively walking that path with actions, not just hollow words&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;See, I got heavy and personal! Let's see how deep this rabbit hole goes. How to start...&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Recently I nearly destroyed my marriage and a relationship of nearly 25 years.&lt;/p&gt;&lt;p&gt;As opening lines go, it isn't quite up there with "Marley was dead: to begin with.", but it's all I've got. It wasn't anything huge and obvious like an affair or a huge violent argument. What I did was to make - over an extended period of time - a series of bad choices about my relationship with technology.&lt;/p&gt;&lt;p&gt;The reality of the era is that we are absolutely surrounded by technology - it encroaches and invades on every aspect of our lives, and it has progressed so fast that we haven't really had time to figure out where "healthy" lies. I must immediately stress that I don't say this to absolve myself of responsibility; we're adults, and we must own the choices that we make, even if we make those choices in an environment that normalises them. So what do I mean?&lt;/p&gt;&lt;p&gt;Ultimately, the heart of my personal failings here stem from how easy - and tempting - it can be to lose ourselves in a digital world. We live in such a hyper-connected time, surrounded by flashing instant updates at every turn. It is alarmingly easy to confuse the signals that this electronic phantom universe provides, prioritising them over the real world in front of us. I'm sure we can all relate to seeing a group of people out together, whether at a bar, a meal, or some other social gathering - and seeing the mobile phones come out regularly. Don't get me started on the idiots who think they can &lt;em&gt;drive&lt;/em&gt; while distracted by a phone. I'm certainly guilty of occasionally "parenting" by observing the digitial-tablet-infused face of one of my children, by half-watching them over the top of a mobile. And I'd be lying if I said I'd never treated my marriage with the same over-familiarity bordering on contempt.&lt;/p&gt;&lt;p&gt;The digital world is so &lt;em&gt;easy and tempting&lt;/em&gt;. Everything is immediate and easy. The real world takes effort, work, and time. When I was growing up, "allow 28 days for delivery" was a mantra; today, if something physical won't arrive within 28 &lt;em&gt;hours&lt;/em&gt; we look at alternative vendors; for purely virtual items, we'd get twitchy and worried if it took 28 minutes.&lt;/p&gt;&lt;p&gt;I've reached the conclusion that among other things, I was - for want of a better word - in an addictive and unhealthy relationship with the internet. The internet is amazing and brilliant - and I'm not proposing we need to nuke it from orbit, but it is at our great peril that we think that it is always (or ever) without harm. We have grown complacent, when we should be treating it with respect and, yes, at times: fear - or at least concern.&lt;/p&gt;&lt;p&gt;We build a global platform for communicating data - all the shared collective knowledge and wisdom of the world past and present, and how do we choose to use it? If only it was "sharing cat pics", maybe the world would be a better place. Instead, &lt;em&gt;as people&lt;/em&gt;, we mostly seem to use it for either validating ourselves in echo chambers (tip: nothing useful is ever achieved by listening to people you already agree with), or getting into angry anonymous rows with strangers. Either triggers a spurt of rewarding chemicals to the brain, but they're both usually entirely empty of any real achievement. If only that was the only mine to avoid.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-perverse-incentives-and-eroded-psychological-walls" class="anchor" aria-hidden="true" href="#perverse-incentives-and-eroded-psychological-walls"&gt;&lt;/a&gt;Perverse Incentives and Eroded Psychological Walls&lt;/h2&gt;&lt;p&gt;Again, I want to keep emphasizing that no personal responsibility is voided, but we haven't arrived at this place in isolation. At risk of sounding like a radical anti-capitalist (I'm not - really), corporate interests are actively averse to us having a healthy relationship with the internet. One way this materializes is in the notion of "engagement". Now; "engagement" by itself isn't an unreasonable measure, but as with most measures: the moment that we start treating it as a target, all hell breaks loose.&lt;/p&gt;&lt;p&gt;Because all genuine inspections should start at home, I'll start by talking about Stack Overflow. We have a measure there, on a user's profile page: consecutive days visited. We're not monsters, so we only display this on &lt;em&gt;your own&lt;/em&gt; profile, but: I can only see negative things about this number. On its own, it adds nothing (not least: you can't compare it to anything), but: I know that at some point in history &lt;em&gt;I cared&lt;/em&gt; about that number. I would try to do something, &lt;em&gt;anything&lt;/em&gt; to not lose this number, including checking in while on family holidays. And here's the thing: the more you maintain it, &lt;em&gt;the more it feels to lose&lt;/em&gt;. It is purely a psychological thing, but... when thinking about it, I can't think of a single positive use of this number. The &lt;em&gt;only&lt;/em&gt; thing it does is encourage &lt;strong&gt;wholly harmful&lt;/strong&gt; behaviours. I love our users, and I want them to be happy, rested, and healthy. Making users not want to &lt;em&gt;go even 24 hours without checking in with us&lt;/em&gt; - well, that doesn't seem good to me. If anything, it sounds like a great way to cause burnout and frustration. I would &lt;em&gt;love&lt;/em&gt; to start a conversation internally about whether we should just nuke that number entirely - or if anything, use it to prompt a user "hey, we really love you, but ... maybe take a day off? we'll see you next week!". As a counterpoint to that: we actively enforce a daily "rep cap", which I think is hugely positive thing towards sensible and healthy usage; I just want to call that out for balance and fairness.&lt;/p&gt;&lt;p&gt;Now consider: in the grand scheme of things: &lt;em&gt;we're cuddly kittens&lt;/em&gt;. Just think what the Facebooks, Googles, etc are doing with psychological factors to drive "engagement". We've already seen the disclosures about Facebook's manipulation of feeds to drive specific responses. Corporations are often perversely incentivized to be at odds with healthy engagement. We can see this most clearly in sectors like gambling, pornography, gaming (especially in-game/in-app purchases, "pay to win"), drugs (whether legal or illicit), "psychics" (deal with the air-quotes) etc. Healthy customers are all well and good, but you make most of your money from the customers with &lt;em&gt;unhealthy&lt;/em&gt; relationships. The idea of fast-eroding virtual "credit" is rife. If I can pick another example: I used to play quite a bit of &lt;em&gt;Elite: Dangerous&lt;/em&gt;; I stopped playing around the time of the "Powerplay" update, which involved a mechanic around "merits" with a &lt;em&gt;steep&lt;/em&gt; decay cycle: if you didn't play &lt;em&gt;significant&lt;/em&gt; amounts of grind &lt;em&gt;every&lt;/em&gt; week (without fail): you'd basically always have zero merits. This is far from unusual in today's games, especially where an online component exists. I've seen YouTube content creators talking about how they strongly feel that if they don't publish on a hard schedule, their channel tanks - and it doesn't even matter whether they're right: their behaviour is driven by the perception, not cold reality (whatever it may be).&lt;/p&gt;&lt;p&gt;I now accept that I had developed some unhealthy relationships with the internet. It hugely impacted my relationships at home, both in quality and quantity. I would either be unavailable, or when I was available, I would be... distracted. Checking my phone &lt;em&gt;way&lt;/em&gt; too often - essentially not really present, except in the "meat" sense. Over time, this eroded things. Important things.&lt;/p&gt;&lt;p&gt;And yet as a society we've normalized it.&lt;/p&gt;&lt;p&gt;Let's look at some of the worst examples from above - gambling, pornography, drugs, etc: it used to be that if you had a proclivity in those directions, there would be some psychological or physical barrier: you'd need to go to the book-maker or casino, or that seedy corner-shop, or find a dealer. Now we have all of those things in our pocket, 24/7, offering anonymous instant access to the best and worst of everything the world has to offer. How would you know that your colleague has a gambling problem, when placing a bet looks identical to responding to a work email? As if that wasn't enough, we've even invented new ways of paying - "crypto-currency" - the key purposes of which are (in no particular order) "to ensure we don't get caught" and "to burn electricity pointlessly". There is possibly some third option about "decentralization" (is that just another word for "crowd-sourced money-laundering"? I can't decide), but I think we all know that in reality for most regular crypto-currency users this is a very far third option; it is probably more important for the organised criminals using it, but... that's another topic.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-we-need-to-maintain-vigilance" class="anchor" aria-hidden="true" href="#we-need-to-maintain-vigilance"&gt;&lt;/a&gt;We Need to Maintain Vigilance&lt;/h2&gt;&lt;p&gt;I wouldn't be saying all this if I thought it was all doom. I &lt;em&gt;do&lt;/em&gt; think we've reached a fundamentally unhealthy place with technology; maybe we've been over-indulging in an initial excited binge, but: we &lt;em&gt;really&lt;/em&gt; need to &lt;strong&gt;get over it&lt;/strong&gt; and see where we're harming and being harmed. We &lt;em&gt;don't&lt;/em&gt; need to obsess over our phones - those little notifications mean &lt;em&gt;almost nothing&lt;/em&gt;. I'm &lt;strong&gt;absolutely not&lt;/strong&gt; saying that I'm detaching myself from the internet, but I &lt;em&gt;am&lt;/em&gt; treating it with a lot more respect - and caution. I'm actively limiting the times that I engage to times that &lt;em&gt;I am comfortable with&lt;/em&gt;. There are very few things that are important enough to need your constant attention; things can wait. For most things: if it is genuinely urgent, &lt;em&gt;someone will simply call you&lt;/em&gt;. I've completely and irrevocably blocked my access to a range of locations that (upon introspection) I found myself over-using, but which weren't helping me as a person - again, hollow validation like echo-chambers and empty arguments. I can limit my usage of things like "twitter" to useful professional interactions, not the uglier side of twitter politics. And I can ensure that in the time I spend with my family: I'm &lt;em&gt;actually there&lt;/em&gt;. In mind and person, not just body. I've completely removed technology from the bedroom - and no, I'm not being crude there - there is a lot of important and useful discussion and just closeness time to be had there, without touching on more ... "intimate" topics. You really, &lt;em&gt;really&lt;/em&gt; don't need to check your inbox while on the toilet - nobody deserves that; just leave the phone outside.&lt;/p&gt;&lt;p&gt;I got lucky; whatever problems I had, I was able to identify, isolate, and work through before they caused total destruction - and I need to be thankful for the support and patience of my wife. But it was genuinely close, and I need to acknowledge that. I'm happier today - and closer to my wife - than I have been in a long long time, mostly through my own casual fault. I'm cautious that the next person might not be so lucky. I'm also terrified about the upcoming generation of children who have very little baseline to compare to. What, for them, is "normal"? How much time at school and home are we dedicating to teaching these impressionable youths successful tactics for navigating the internet, and what that means for their "real" life? I think we can agree that when we hear of "Fortnite", "kids" and "rehab" being used in the same sentence: something is wrong somewhere.&lt;/p&gt;&lt;p&gt;Maybe somewhere along the line we (culture) threw the baby out with the bathwater. I'm not at all a religious person, but if I look at most established religions with that atheistic lens, I have to acknowledge that among the superstition: there are some good wisdoms about leading a good and healthy life - whether by way of moral codes (that vary hugely by religion), or by instilling a sense of personal accountability and responsibility, or by the simple act of finding time to sit quietly - regularly - and be honestly contemplative. To consider the consequences of our actions, even - perhaps especially - when we haven't had to do so directly. Humility, patience, empathy. I know in the past I've been somewhat dismissive of even non-theistic meditation, but: I suspect that it is something that I might now be in a position to appreciate.&lt;/p&gt;&lt;p&gt;To re-state: I'm OK; I am (and in terms of my marriage: we are) in a much better, stronger, healthier place than I (we) have been in a long time. I've had my Thanksgiving Miracle, and I've come out the other side with a renewed energy, and some fundamentally altered opinions. I'm interested in your thoughts here, but I'm not opening comments; again - we've made it too easy and anonymous! If you want to email me on this, please do (marc.gravell at gmail.com - if you could use "Thanksgiving Carol" in the subject, that'd really help me organize my inbox); I may respond, but I won't guarantee it, and I certainly won't guarantee an immediate response. I'm also deliciously conscious of the apparent irony of my blogging about the harms of the internet. But: if - as Joel assures me - "Developers are Writing the Script for the Future" - we need to start being a bit more outspoken about what that script says, and calling out when some measure of "success" of a product or service is likely impactful to healthy usage.&lt;/p&gt;&lt;p&gt;Closing: technology is great, the internet is great; but: we need to treat them with respect, and use them in sensible moderation. And pay &lt;em&gt;lots&lt;/em&gt; more attention to the real world.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/66RC95UXlnU" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2436815592271301061" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2436815592271301061" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/66RC95UXlnU/a-thanksgiving-carol.html" title="A Thanksgiving Carol" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/12/a-thanksgiving-carol.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-7083353498073719636</id><published>2018-09-08T02:35:00.001-07:00</published><updated>2018-09-08T02:35:28.785-07:00</updated><title type="text">Monotoolism</title><content type="html">&lt;h1&gt;&lt;a id="One_Tool_To_Rule_Them_All_0"&gt;&lt;/a&gt;One Tool To Rule Them All&lt;/h1&gt;&lt;p&gt;A recent twitter thread reminded me of a trope that I see frequently as a library author (and just as a general observer) - let’s call it “monotoolism”.&lt;/p&gt;&lt;p&gt;Examples of this might be examples like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;“wait, you’re still using ‘LINQ to SQL’? I thought you were using ‘Dapper’?”&lt;/li&gt;&lt;li&gt;“Google’s protobuf impementation provides opinionated JSON parsing, but my JSON doesn’t fit that layout - how do I get the library to parse my layout?”&lt;/li&gt;&lt;li&gt;&lt;a href="https://stackoverflow.com/a/1732454/23354"&gt;“how do I parse HTML with a regular expression?”&lt;/a&gt;&lt;/li&gt;&lt;li&gt;etc&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The common theme here being the expectation that once you have &lt;em&gt;one&lt;/em&gt; tool in a codebase that fits a particular category: &lt;em&gt;that’s it&lt;/em&gt; - there is one and only one tool against each category; one “data access tool”, one “string parsing tool”, etc.&lt;/p&gt;&lt;p&gt;This has always irked me. I &lt;em&gt;understand where people are coming from&lt;/em&gt; - they don’t want an &lt;em&gt;explosion&lt;/em&gt; of different tools to have to worry about:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;they don’t want an overly complex dependency tree&lt;/li&gt;&lt;li&gt;they don’t want to have to check licensing / compliance etc against a huge number of libraries&lt;/li&gt;&lt;li&gt;they don’t want to have to train everyone to use a plethora of tools&lt;/li&gt;&lt;li&gt;etc&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It absolutely makes sense to minimize the dependency count, and to remove unnecessary library overlap. But the key word in that sentence: “unnecessary” - and I mean that in a fairly loose sense: you can use the handle of a screwdriver to drive in a nail if you try hard enough, but it is &lt;em&gt;much easier&lt;/em&gt; (and you get a better outcome) if you use a hammer. I think I’d include a hammer as a “necessary” tool alongside a set of screwdrivers if you’re doing any form of construction (but is that a metric or imperial hammer?).&lt;/p&gt;&lt;p&gt;I often see people either expressing frustration that their chosen “one tool to rule them all” can’t do tangentially-related-feature-X, or bending their code &lt;em&gt;massively&lt;/em&gt; out of shape to try to make it do it; sometimes they even succeed, which is even scarier as a library author - because now there’s some completely undesigned-for, unspecified, undocumented and just &lt;em&gt;unknown&lt;/em&gt; usage in the wild (quite possibly abusing reflection to push buttons that aren’t exposed) that the library author is going to get yelled at when it breaks.&lt;/p&gt;&lt;p&gt;&lt;a href="https://xkcd.com/1172/"&gt;&lt;img src="https://imgs.xkcd.com/comics/workflow.png" alt="XKCD: Workflow"&gt;&lt;/a&gt;&lt;/p&gt;&lt;h1&gt;&lt;a id="It_is_OK_to_use_more_than_one_tool_26"&gt;&lt;/a&gt;It is OK to use more than one tool!&lt;/h1&gt;&lt;p&gt;Yes, it is desirable to minimize the number of unnecessary tools. But: &lt;strong&gt;it is OK to use more than one tool&lt;/strong&gt;. Expected, even. You &lt;em&gt;absolutely should&lt;/em&gt; be wary of uncontrolled tool propogation, but I strongly advocate &lt;em&gt;against&lt;/em&gt; being too aggressive with rebukes along the lines of:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;We already have a tool that does something kinda like that; can you just torture the tool and the domain model a bit and see if it works well enough to just about work?&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Remember, the options here are:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;two (or more) different tools, each used in their intended way, closely following their respective documented examples in ways that are “obviously right” and which it is easy to ask questions of the library authors or the community&lt;/li&gt;&lt;li&gt;one single tool, tortured and warped beyond recognition, looking nothing like… &lt;em&gt;anything&lt;/em&gt;, where even the tool’s authors can’t understand what you’re doing (let alone why, and they’re probably too afraid to ask), where you’re the &lt;em&gt;only usage like that, ever&lt;/em&gt;, and where your “elegant hack” might stop working in the next minor revision, because it wasn’t a tested scenario&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;I prefer “1”. It’ll keep your model cleaner. It’ll keep you relationship with the tool more successful. Yes, it will mean that you occasionally need more than one tool listed in a particular box. Deal with it! If the tool &lt;em&gt;really is&lt;/em&gt; complex enough that this is problematic, just move the ugly complexity behind some abstraction, then only a limited number of people need to worry about &lt;em&gt;how&lt;/em&gt; it works.&lt;/p&gt;&lt;p&gt;Always use the right tool for the job.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/uq7nzDb111k" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7083353498073719636" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7083353498073719636" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/uq7nzDb111k/monotoolism.html" title="Monotoolism" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/09/monotoolism.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-209000833784182537</id><published>2018-08-02T16:26:00.001-07:00</published><updated>2018-08-02T16:46:04.979-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="protobuf-net" /><title type="text">protobuf-net, August 2018 update</title><content type="html">&lt;h1&gt;An update on what's happening with &lt;code&gt;protobuf-net&lt;/code&gt;&lt;/h1&gt;&lt;p&gt;Headline: .proto processing now works directly from &lt;code&gt;dotnet build&lt;/code&gt; and MSBuild, without any need for DSL processing steps; and - new shiny things in the future.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;I haven't spoken about protobuf-net for quite a while, but: it is &lt;em&gt;very much&lt;/em&gt; alive and active. However, I really should do a catch-up, and I'm &lt;em&gt;really&lt;/em&gt; excited about where we are.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-level-100-primer-if-you-dont-know-what-protobuf-is" class="anchor" aria-hidden="true" href="#level-100-primer-if-you-dont-know-what-protobuf-is"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Level 100 primer, if you don't know what "protobuf" is&lt;/h2&gt;&lt;p&gt;"protobuf" is &lt;a href="https://developers.google.com/protocol-buffers/" rel="nofollow"&gt;Protocol Buffers&lt;/a&gt;, Google's cross-platform/language/OS/etc serialization format (and associated tools). It is &lt;em&gt;primarily&lt;/em&gt; a dense binary format, but a JSON variant also exists. A lot of Google's public and private APIs are protobuf, but it is used widely outside of Google too.&lt;/p&gt;&lt;p&gt;The data/schema is &lt;em&gt;often&lt;/em&gt; described via a custom DSL, &lt;a href="https://developers.google.com/protocol-buffers/docs/proto" rel="nofollow"&gt;.proto&lt;/a&gt; - which comes in 2 versions (proto2 and proto3). They both describe the same binary format.&lt;/p&gt;&lt;p&gt;Google provide implementations for a range of platforms including C# (note: "proto3" only), but ... I kinda find the "DSL first, always" approach limiting (I like the flexibility of "code first"), and ... the Google implementation is "Google-idiomatic", rather than ".NET idiomatic".&lt;/p&gt;&lt;p&gt;Hence &lt;a href="https://www.nuget.org/packages/protobuf-net/" rel="nofollow"&gt;protobuf-net exists&lt;/a&gt;; it is a fast/dense binary serializer that implements the protobuf specifiction, but which is .NET-idiomatic, and allows either code-first or DSL-first. I use it a lot.&lt;/p&gt;&lt;p&gt;Historically, it was biased towards "code first", with the "DSL first" tools a viable but more awkward option.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-whats-changed-lately" class="anchor" aria-hidden="true" href="#whats-changed-lately"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's changed lately?&lt;/h2&gt;&lt;h3&gt;&lt;a id="user-content-bespoke-managed-dsl-parser" class="anchor" aria-hidden="true" href="#bespoke-managed-dsl-parser"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bespoke managed DSL parser&lt;/h3&gt;&lt;p&gt;Just over a year ago now, back in 2.3.0, I released a new set of DSL parsing tools. In the past, protobuf-net's tooling (&lt;code&gt;protogen&lt;/code&gt;) made use of Google's &lt;code&gt;protoc&lt;/code&gt; tool - a binary executable that processes .proto files, but this was &lt;em&gt;incredibly&lt;/em&gt; akward to deploy between platforms. Essentially, the tools would &lt;em&gt;probably&lt;/em&gt; work on Windows, but that was about it. This wasn't a great option going forward, so I implemented a completely bespoke 100% managed-code parser and code-generator that didn't depend on &lt;code&gt;protoc&lt;/code&gt; at all. &lt;code&gt;protogen&lt;/code&gt; was reborn (and it works with both "proto2" and "proto3"), but it lacked a good deployment route.&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-playground-website" class="anchor" aria-hidden="true" href="#playground-website"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Playground website&lt;/h3&gt;&lt;p&gt;Next, I threw together &lt;a href="https://protogen.marcgravell.com/" rel="nofollow"&gt;protogen.marcgravell.com&lt;/a&gt;. This is an ASP.NET Core web app that uses the same library code as &lt;code&gt;protogen&lt;/code&gt;, but in an interactive web app. This makes for a pretty easy way to play with .proto files, including a code-editor and code generator. It also hosts &lt;code&gt;protoc&lt;/code&gt;, if you prefer that - and includes a &lt;em&gt;wide range&lt;/em&gt; of Google's API definitions available as &lt;code&gt;import&lt;/code&gt;s. This is a very easy way of working with casual .proto usage, and it provides a download location for the standalone &lt;code&gt;protogen&lt;/code&gt; tools. It isn't going to win any UI awards, but it works. It even &lt;a href="https://protogen.marcgravell.com/decode" rel="nofollow"&gt;includes a decoder&lt;/a&gt;, if you want to understand serialized protobuf data.&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-global-tools" class="anchor" aria-hidden="true" href="#global-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Global tools&lt;/h3&gt;&lt;p&gt;Having a download for the command-line tools is a great step forward, but ... it is still a lot of hassle. If only there were a way of installing managed-code developer tools in a convenient way. Well, there is: .NET "global tools"; so, a few months ago I added &lt;a href="https://www.nuget.org/packages/protobuf-net.Protogen/" rel="nofollow"&gt;&lt;code&gt;protobuf-net.Protogen&lt;/code&gt;&lt;/a&gt;. As a "global tool", this can be installed once via&lt;/p&gt;&lt;pre&gt;&lt;code&gt;dotnet tool install --global protobuf-net.Protogen&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and then &lt;code&gt;protogen&lt;/code&gt; will be available anywhere, as a development tool. Impressively, "global tools" work &lt;em&gt;between operating systems&lt;/em&gt;, so the exact same package will also work on linux (and presumably Mac). This starts to make .proto very friendly to work with, as a developer.&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-build-tools" class="anchor" aria-hidden="true" href="#build-tools"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build tools&lt;/h3&gt;&lt;p&gt;I'm going to be frank and honest: MSBuild scares the bejeezus out of me. I don't understand .targets files, etc. It is a huge blind-spot of mine, but I've made my peace with that reality. So... I was &lt;em&gt;genuinely delighted&lt;/em&gt; to receive a pull request from &lt;a href="https://twitter.com/markpflug" rel="nofollow"&gt;Mark Pflug&lt;/a&gt; that fills in the gaps! What this adds is &lt;a href="https://www.nuget.org/packages/protobuf-net.MSBuild" rel="nofollow"&gt;&lt;code&gt;protobuf-net.MSBuild&lt;/code&gt;&lt;/a&gt; - tools that tweak that build process from &lt;code&gt;dotnet build&lt;/code&gt; and &lt;code&gt;MSBuild&lt;/code&gt;. What this means is that you can just install &lt;code&gt;protobuf-net.MSBuild&lt;/code&gt; into a project, and it automatically runs the .proto → C# code-generation steps &lt;em&gt;as part of build&lt;/em&gt;. This means you can just maintain your .proto files without any need to generate the C# as a separate step. You can still extend the &lt;code&gt;partial&lt;/code&gt; types in the usualy way. All you need to do is make sure the .proto files are in the project. It even includes the common Google &lt;code&gt;import&lt;/code&gt; additions for free (without any extra files required), so: if you know what a &lt;code&gt;.google.protobuf.timestamp.Timestamp&lt;/code&gt; is - know that it'll work without you having to add the relevant .proto file manually (although you still need the &lt;code&gt;import&lt;/code&gt; statement).&lt;/p&gt;&lt;p&gt;I can't understate how awesome I think these tools are, and how much friendlier it makes the "DSL first" scenario. Finally, protobuf-net can use .proto as a truly first class experience. Thanks again, Mark Pflug!&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-what-next" class="anchor" aria-hidden="true" href="#what-next"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What next?&lt;/h2&gt;&lt;p&gt;That's where we are &lt;em&gt;today&lt;/em&gt;, but : to give an update on my plans and priorities going forwards...&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-spans-and-pipelines" class="anchor" aria-hidden="true" href="#spans-and-pipelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spans and Pipelines&lt;/h3&gt;&lt;p&gt;You might have noticed me talking about these a little lately; I've done lots of &lt;em&gt;research&lt;/em&gt; to look at what protobuf-net might do with these, but it is probably time to start looking at doing it "for real". The first step there is getting some real timings on the performance difference between a few different approaches&lt;/p&gt;&lt;h3&gt;&lt;a id="user-content-aot" class="anchor" aria-hidden="true" href="#aot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AOT&lt;/h3&gt;&lt;p&gt;In particular, platforms that don't allow IL-emit. This helps consumers like UWP, Unity, iOS, etc. They usually currently &lt;em&gt;work&lt;/em&gt; with protobuf-net, but via huge compromises. To do better, we need radically overhaul how we approach those platforms. I see two viable avenues to explore there.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;we can enhance the .proto codegen (the bits that &lt;code&gt;protobuf-net.MSBuild&lt;/code&gt; just made tons better), to include generation of &lt;em&gt;the actual serialization code&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;we can implement Roslyn-based tools that pull apart code-first usage to understand the model, and emit the serialization code at build time&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;All of these are going to keep me busy into the foreseeable!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/k0Usey-VLyE" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/209000833784182537" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/209000833784182537" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/k0Usey-VLyE/protobuf-net-august-2018-update.html" title="protobuf-net, August 2018 update" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/08/protobuf-net-august-2018-update.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-6419206115684687947</id><published>2018-07-30T03:52:00.003-07:00</published><updated>2018-07-30T06:27:11.319-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="pipelines" /><title type="text">Pipe Dreams, part 3.1</title><content type="html">&lt;h1&gt;&lt;a id="user-content-pipelines---a-guided-tour-of-the-new-io-api-in-net-part-31" class="anchor" aria-hidden="true" href="#pipelines---a-guided-tour-of-the-new-io-api-in-net-part-31"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipelines - a guided tour of the new IO API in .NET, part 3.1&lt;/h1&gt;&lt;p&gt;(&lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html" rel="nofollow"&gt;part 1&lt;/a&gt;, &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html" rel="nofollow"&gt;part 2&lt;/a&gt;, &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-3.html" rel="nofollow"&gt;part 3&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;After part 3, I got some great feedback - mostly requests to clarify things that I touched on, but could do with further explanation. Rather than make part 3 &lt;em&gt;even longer&lt;/em&gt;, I want to address those here! Yay, more words!&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-isnt-arraypoolownert-doing-the-same-thing-as-memorypoolt-why-dont-you-just-use-memorypoolt-" class="anchor" aria-hidden="true" href="#isnt-arraypoolownert-doing-the-same-thing-as-memorypoolt-why-dont-you-just-use-memorypoolt-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Isn't &lt;code&gt;ArrayPoolOwner&amp;lt;T&amp;gt;&lt;/code&gt; doing the same thing as &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt;? Why don't you just use &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt; ?&lt;/h3&gt;&lt;p&gt;Great question! I didn't actually mention &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt;, so I'd better introduce it.&lt;/p&gt;&lt;p&gt;&lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt; is an &lt;code&gt;abstract&lt;/code&gt; base type that offers an API of the form:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;MemoryPool&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; : &lt;span class="pl-en"&gt;IDisposable&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;Rent&lt;/span&gt;(&lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;minBufferSize&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; not shown: a few unrelated details&lt;/span&gt;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As you can see, this &lt;code&gt;Rent()&lt;/code&gt; method looks exactly like what we were looking for before - it takes a size and returns an &lt;code&gt;IMemoryOwner&amp;lt;T&amp;gt;&lt;/code&gt; (to provide a &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt;), with it being returned from whence it came upon disposal.&lt;/p&gt;&lt;p&gt;&lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt; also has a default implementation (&lt;code&gt;public static MemoryPool&amp;lt;T&amp;gt; Shared { get; }&lt;/code&gt;), which returns a &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt; that is based on the &lt;code&gt;ArrayPool&amp;lt;T&amp;gt;&lt;/code&gt; (i.e. &lt;a href="https://github.com/dotnet/corefx/blob/master/src/System.Memory/src/System/Buffers/ArrayMemoryPool.cs" rel="nofollow"&gt;&lt;code&gt;ArrayMemoryPool&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt;). The &lt;code&gt;Rent()&lt;/code&gt; method returns an &lt;a href="https://github.com/dotnet/corefx/blob/master/src/System.Memory/src/System/Buffers/ArrayMemoryPool.ArrayMemoryPoolBuffer.cs" rel="nofollow"&gt;&lt;code&gt;ArrayMemoryPoolBuffer&lt;/code&gt;&lt;/a&gt;, which looks &lt;em&gt;remarkably like&lt;/em&gt; the thing that I called &lt;code&gt;ArrayPoolOwner&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;So: a very valid question would be: "Marc, didn't you just re-invent the default memory pool?". The answer is "no", but it is for a &lt;em&gt;very&lt;/em&gt; subtle reason that I probably should have expounded upon at the time.&lt;/p&gt;&lt;p&gt;The problem is in the name &lt;code&gt;minBufferSize&lt;/code&gt;; well... not really the &lt;em&gt;name&lt;/em&gt;, but the &lt;em&gt;consequence&lt;/em&gt;. What this means is: when you &lt;code&gt;Rent()&lt;/code&gt; from the default &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;.Shared&lt;/code&gt;, the &lt;code&gt;.Memory&lt;/code&gt; that you get back will be &lt;em&gt;over-sized&lt;/em&gt;. Often this isn't a problem, but in our case we &lt;em&gt;really&lt;/em&gt; want the &lt;code&gt;.Memory&lt;/code&gt; to represent the actual number of bytes that were sent (&lt;em&gt;even if&lt;/em&gt; we are, behind the scenes, using a larger array from the pool to contain it).&lt;/p&gt;&lt;p&gt;We &lt;em&gt;could&lt;/em&gt; use an extension method on arbitrary memory pools to &lt;em&gt;wrap&lt;/em&gt; potentially oversized memory, i.e.&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;RentRightSized&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;(&lt;br /&gt;    &lt;span class="pl-k"&gt;this&lt;/span&gt; &lt;span class="pl-en"&gt;MemoryPool&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;pool&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;size&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;leased&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;pool&lt;/span&gt;.&lt;span class="pl-en"&gt;Rent&lt;/span&gt;(&lt;span class="pl-smi"&gt;size&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;leased&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-smi"&gt;size&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;leased&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; already OK&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;RightSizeWrapper&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;(&lt;span class="pl-smi"&gt;leased&lt;/span&gt;, &lt;span class="pl-smi"&gt;size&lt;/span&gt;);&lt;br /&gt;}&lt;br /&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;RightSizeWrapper&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; : &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;RightSizeWrapper&lt;/span&gt;(&lt;br /&gt;        &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;inner&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;_inner&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;inner&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-smi"&gt;_length&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;;&lt;br /&gt;    } &lt;br /&gt;    &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;_inner&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;_length&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;Dispose&lt;/span&gt;() &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_inner&lt;/span&gt;.&lt;span class="pl-en"&gt;Dispose&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;Memory&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_inner&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;_length&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;but... this would mean allocating &lt;em&gt;two&lt;/em&gt; objects for most leases - one for the &lt;em&gt;actual&lt;/em&gt; lease, and one for the thing that fixes the length. So, since we only &lt;em&gt;really&lt;/em&gt; care about the array-pool here, it is preferable IMO to cut out the extra layer, and write our own right-sized implementation from scratch.&lt;/p&gt;&lt;p&gt;So: that's the difference in the reasoning and implementation. As a side note, though: it prompts the question as to whether I should refactor my API to actually implement the &lt;code&gt;MemoryPool&amp;lt;T&amp;gt;&lt;/code&gt; API.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-you-might-not-want-to-complete-with-success-if-the-cancellation-token-is-cancelled" class="anchor" aria-hidden="true" href="#you-might-not-want-to-complete-with-success-if-the-cancellation-token-is-cancelled"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;You &lt;em&gt;might&lt;/em&gt; not want to complete with success if the cancellation token is cancelled&lt;/h3&gt;&lt;p&gt;This is in relation to the &lt;code&gt;while&lt;/code&gt; in the read loop:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCancellationRequested&lt;/span&gt;)&lt;br /&gt;{...}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;em&gt;more typical&lt;/em&gt; expectation for cancellation is for it to throw with a cancellation exception of some kind; therefore, if it &lt;em&gt;is&lt;/em&gt; cancelled, I might want to reflect that.&lt;/p&gt;&lt;p&gt;This is very valid feedback! Perhaps the most practical fix here is simply to use &lt;code&gt;while (true)&lt;/code&gt; and let the subsequent &lt;code&gt;await reader.ReadAsync(cancellationToken)&lt;/code&gt; worry about what cancellation should look like.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-you-should-clarify-about-testing-the-result-in-async-sync-path-scenarios" class="anchor" aria-hidden="true" href="#you-should-clarify-about-testing-the-result-in-async-sync-path-scenarios"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;You should clarify about testing the result in async "sync path" scenarios&lt;/h3&gt;&lt;p&gt;In my aside about &lt;code&gt;async&lt;/code&gt; uglification (optimizing when we &lt;em&gt;expect&lt;/em&gt; it to be synchronous in most cases), I ommitted to talk about getting results from the pseudo-awaited operation. Usually, this comes down to calling &lt;code&gt;.Result&lt;/code&gt; on an awaitable (something like a &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt;, &lt;code&gt;ValueTask&amp;lt;T&amp;gt;&lt;/code&gt;, or &lt;code&gt;.GetResult()&lt;/code&gt; on an awaiter (the thing you get from &lt;code&gt;.GetAwaiter()&lt;/code&gt;). I haven't done it &lt;em&gt;in the example&lt;/em&gt; because in &lt;code&gt;async&lt;/code&gt; terms this would simply have been an &lt;code&gt;await theThing;&lt;/code&gt; usage, not a &lt;code&gt;var local = await theThing;&lt;/code&gt; usage; but you can if you need that.&lt;/p&gt;&lt;p&gt;I must, however, clarify a few points that perhaps weren't clear:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;you &lt;strong&gt;should not&lt;/strong&gt; (usually) try to access the &lt;code&gt;.Result&lt;/code&gt; of a task &lt;strong&gt;unless&lt;/strong&gt; you know that it has already completed&lt;/li&gt;&lt;li&gt;knowing that it has completed &lt;em&gt;isn't enough&lt;/em&gt; to know that it has completed &lt;em&gt;successfully&lt;/em&gt;; if you only test "is completed", you can use &lt;code&gt;.GetResult()&lt;/code&gt; on the awaiter to &lt;em&gt;check for exceptions&lt;/em&gt; while also fetching the result (which you can then discard if you like)&lt;/li&gt;&lt;li&gt;in my case, I'm taking a shortcut by checking &lt;code&gt;IsCompletedSuccessfully&lt;/code&gt;; this exists on &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; (and on &lt;code&gt;Task[&amp;lt;T&amp;gt;]&lt;/code&gt; in .NET Core 2.0, else you can check &lt;code&gt;.Status == TaskStatus.RanToCompletion&lt;/code&gt;) - which is only &lt;code&gt;true&lt;/code&gt; in the "completed without an exception" case&lt;/li&gt;&lt;li&gt;because of expectations around how exceptions on &lt;code&gt;async&lt;/code&gt; operations are wrapped and surfaced, it is almost always preferable to just switch into the &lt;code&gt;async&lt;/code&gt; flow if you know a task has faulted, and just &lt;code&gt;await&lt;/code&gt; it; the compiler knows how to get the exception out in the most suitable way, so: let it do the hard work&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-you-should-explain-more-about-valuetaskt-vs-taskt---not-many-people-understand-them-well" class="anchor" aria-hidden="true" href="#you-should-explain-more-about-valuetaskt-vs-taskt---not-many-people-understand-them-well"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;You should explain more about &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; vs &lt;code&gt;Task[&amp;lt;T&amp;gt;]&lt;/code&gt; - not many people understand them well&lt;/h3&gt;&lt;p&gt;OK! Many moons ago, &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; became a thing, and all was well. &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; actually happened &lt;em&gt;long before&lt;/em&gt; C# had any kind of support for &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;, and the main scenarios it was concerned about were &lt;em&gt;genuinely asynchronous&lt;/em&gt; - it was &lt;em&gt;expected&lt;/em&gt; that the answer &lt;em&gt;would not be immediately available&lt;/em&gt;. So, the ovehead of allocating a placeholder object was fine and dandy, dandy and fine.&lt;/p&gt;&lt;p&gt;As the usage of &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; grew, and the language support came into effect, it started to become clear that there were many cases where:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the operation would often be available immediately (think: caches, buffered data, uncontested locking primitives, etc)&lt;/li&gt;&lt;li&gt;it was being used &lt;em&gt;inside a tight loop&lt;/em&gt;, or just at high frequency - i.e. something that happens thousands of times a second (file IO, network IO, synchronization over a collection, etc)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When you put those two things together, you find yourself allocating &lt;em&gt;large numbers&lt;/em&gt; of objects for something that &lt;em&gt;was only rarely actually asynchronous&lt;/em&gt; (so: when there &lt;em&gt;wasn't&lt;/em&gt; data available in the socket, or the file buffer was empty, or the lock was contested). For some scenarios, there are pre-completed reusable task instances available (such as &lt;code&gt;Task.CompletedTask&lt;/code&gt;, and inbuilt handling for some low integers), but this doesn't help if the return value is outside this very limited set. To help avoid the allocations in the general case, &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; was born. A &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; is a &lt;code&gt;struct&lt;/code&gt; that implements the "awaitable" pattern (a duck-typed pattern, like &lt;code&gt;foreach&lt;/code&gt;, but that's a story for another day), that essentially contains two fields:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;a &lt;code&gt;T&lt;/code&gt; if the value was known immediately (obviously not needed for the untyped &lt;code&gt;ValueTask&lt;/code&gt; case)&lt;/li&gt;&lt;li&gt;a &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; if the value is pending and the answer depends on the result of the incomplete operation&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That means that &lt;em&gt;if the value is known now&lt;/em&gt;,  no &lt;code&gt;Task[&amp;lt;T&amp;gt;]&lt;/code&gt; (and no corresponding &lt;code&gt;TaskCompletionSource&amp;lt;T&amp;gt;&lt;/code&gt;) &lt;em&gt;ever needs to be allocated&lt;/em&gt; - we just throw back the &lt;code&gt;struct&lt;/code&gt;, it gets unwrapped by the &lt;code&gt;async&lt;/code&gt; pattern, and life is good. Only in the case where the operation is &lt;em&gt;actually asynchronous&lt;/em&gt; does an object need to be allocated.&lt;/p&gt;&lt;p&gt;Now, there are three common views on what we should do with this:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;always expose &lt;code&gt;Task[&amp;lt;T&amp;gt;]&lt;/code&gt;, regardless of whether it is likely to be synchronous&lt;/li&gt;&lt;li&gt;expose &lt;code&gt;Task[&amp;lt;T&amp;gt;]&lt;/code&gt; if we know it will be async, expose &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; if we think it &lt;em&gt;may&lt;/em&gt; be synchronous&lt;/li&gt;&lt;li&gt;always expose &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Frankly, the only valid reason to use &lt;code&gt;1&lt;/code&gt; is because your API surface was baked and fixed back before &lt;code&gt;ValueTask[&amp;lt;T&amp;gt;]&lt;/code&gt; existed.&lt;/p&gt;&lt;p&gt;The choice between &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; is interesting; what we're actually talking about there is an implementation detail, so a good case &lt;em&gt;could be argued&lt;/em&gt; for &lt;code&gt;3&lt;/code&gt;, allowing you to amaze yourself later if you find a way of doing something synchronously (where it was previously asynchronous), without breaking the API. I went for &lt;code&gt;2&lt;/code&gt; in the code shown, but it would be something I'd be willing to change without much prodding.&lt;/p&gt;&lt;p&gt;You should also note that there is actually a fourth option: &lt;strong&gt;use custom awaitables&lt;/strong&gt; (meaning: a custom type that implements the "awaitable" duck-typed pattern). This is an advanced topic, and needs &lt;em&gt;very&lt;/em&gt; careful consideration. I'm not even going to give examples of &lt;em&gt;how&lt;/em&gt; to do that, but it is worth noting that &lt;code&gt;ReadAsync&lt;/code&gt; and &lt;code&gt;FlushAsync&lt;/code&gt; ("pipelines" methods that we've used extensively here) &lt;em&gt;do return&lt;/em&gt; custom awaitables. You'd need to &lt;em&gt;really, really&lt;/em&gt; understand your reasons before going down that path, though.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-i-spotted-a-bug-in-your-next-message-number-code" class="anchor" aria-hidden="true" href="#i-spotted-a-bug-in-your-next-message-number-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I spotted a bug in your "next message number" code&lt;/h3&gt;&lt;p&gt;Yes, the code shown in the post can generate two messages with id &lt;code&gt;1&lt;/code&gt;, after 4-billion-something messages:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;++&lt;/span&gt;&lt;span class="pl-smi"&gt;_nextMessageId&lt;/span&gt;;&lt;br /&gt;&lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that I didn't increment &lt;code&gt;_nextMessageId&lt;/code&gt; when I dodged the sentinel (zero). There's also a &lt;em&gt;very&lt;/em&gt; small chance that a previous message from 4-billion-ago &lt;em&gt;still hasn't been replied to&lt;/em&gt;. Both of these are fixed in the "real" code.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-you-might-be-leaking-your-lease-around-the-trysetresult" class="anchor" aria-hidden="true" href="#you-might-be-leaking-your-lease-around-the-trysetresult"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;You might be leaking your lease around the &lt;code&gt;TrySetResult&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;In the original blog code, I had&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-smi"&gt;tcs&lt;/span&gt;&lt;span class="pl-k"&gt;?&lt;/span&gt;.&lt;span class="pl-en"&gt;TrySetResult&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-en"&gt;Lease&lt;/span&gt;());&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If &lt;code&gt;tcs&lt;/code&gt; is not &lt;code&gt;null&lt;/code&gt; (via the "Elvis operator"), this allocates a lease and then invokes &lt;code&gt;TrySetResult&lt;/code&gt;. However, &lt;code&gt;TrySetResult&lt;/code&gt; &lt;em&gt;can return &lt;code&gt;false&lt;/code&gt;&lt;/em&gt; - meaning: it couldn't do that, because the underlying task was already completed in some other way (perhaps we added timeout code). The only time we should consider that we have &lt;em&gt;successfully&lt;/em&gt; transferred ownership of the lease to the task is if it returns &lt;code&gt;true&lt;/code&gt;. The real code fixes this, ensuring that it is disposed in all cases &lt;em&gt;except&lt;/em&gt; where &lt;code&gt;TrySetResult&lt;/code&gt; returns &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a id="user-content-what-about-incremental-frame-parsers" class="anchor" aria-hidden="true" href="#what-about-incremental-frame-parsers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What about incremental frame parsers?&lt;/h3&gt;&lt;p&gt;In my discussion of handling the frame, I was using an approach that processed a frame either in it's entirety, or not at all. This is not the only option, and you can consume &lt;em&gt;any amount of the frame that you want&lt;/em&gt;, as long as you write code to track the internal state. For example, if you are parsing http, you could parse the http headers into some container as long as you have at least one entire http header name/value pair (without requiring &lt;em&gt;all&lt;/em&gt; the headers to start parsing). Similarly, you could consume &lt;em&gt;some&lt;/em&gt; of the payload (perhaps writing what you have so far to disk). In both cases, you would simply need to &lt;code&gt;Advance&lt;/code&gt; past the bit that you consider consumed, and update your own state object.&lt;/p&gt;&lt;p&gt;So yes, that is absolutely possible - even highly desirable in some cases. In some cases it is highly desirable &lt;em&gt;not to start&lt;/em&gt; until you've got everything. Remember that parsing often means taking the data from a &lt;em&gt;streamed&lt;/em&gt; representation, and pushing it into a &lt;em&gt;model&lt;/em&gt; representation - you might actually need &lt;em&gt;more&lt;/em&gt; memory for the &lt;em&gt;model&lt;/em&gt; representation (especially if the source data is compressed or simply stored in a dense binary format). An &lt;em&gt;advantage&lt;/em&gt; of incremental parsing is that when the last few bytes dribble in, you might already have done &lt;em&gt;most&lt;/em&gt; of the parsing work - allowing you to overlap pre-processing the data with data receive - rather than "buffer, buffer, buffer; right - now &lt;em&gt;start&lt;/em&gt; parsing it".&lt;/p&gt;&lt;p&gt;However, in the case I was discussing: the header was 8 bytes, so there's not much point trying to over-optimize; if we don't have an entire header &lt;em&gt;now&lt;/em&gt;, we'll mostly likely have a complete header when the next packet arrives, or we'll &lt;em&gt;never&lt;/em&gt; have an entire header. Likewise, because we want to hand the entire payload to the consumer as a single chunk, we need all the data. We &lt;em&gt;could&lt;/em&gt; actually lease the target array as soon as we know the size, and start copying data into &lt;em&gt;that&lt;/em&gt; buffer and releasing the source buffers. We're not actually gaining much by this - we're simply exchanging data in one buffer for the same amount of data in another buffer; but we're actually exposing ourselves to an attack vector: a malicious (or buggy) client can sent a message-header that claims to be sending a large amount of data (maybe 1GiB), then just ... keeps the socket open and doesn't send anything more. In this scenario, the client has sent &lt;em&gt;almost nothing&lt;/em&gt; (maybe just 8 bytes!), but they've chewed up a &lt;b&gt;lot&lt;/b&gt; of server memory. Now imagine they do this from 10, 100, or 1000 parallel connections - and you can see how they've achieved &lt;em&gt;disproportionate&lt;/em&gt; harm to our server, for almost zero cost at the client. There are two pragmatic fixes for this:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Put an upper limit on the message size, and put an upper limit on the connections from a single endpoint &lt;li&gt;Make the client pay their dues: if they claim to be sending a large message (which may indeed have legitimate uses): don't lease any expensive resources until they've actually sent that much (which is what the code as-implemented achieves) &lt;/ul&gt;&lt;p&gt;Emphasis: your choice of frame parsing strategy is entirely contextual, and you can play with other implementations.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;So; that's the amendments. I hope they are useful. A huge "thanks" to the people who are keeping me honest here, including Shane Grüling, David Fowler, and Nick Craver.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/zhcGYTVP63M" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6419206115684687947" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6419206115684687947" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/zhcGYTVP63M/pipe-dreams-part-31.html" title="Pipe Dreams, part 3.1" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/07/pipe-dreams-part-31.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-6007133552883736143</id><published>2018-07-29T07:23:00.000-07:00</published><updated>2018-08-01T07:51:12.993-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="pipelines" /><title type="text">Pipe Dreams, part 3</title><content type="html">&lt;h1&gt;&lt;a id="user-content-pipelines---a-guided-tour-of-the-new-io-api-in-net-part-3" class="anchor" aria-hidden="true" href="#pipelines---a-guided-tour-of-the-new-io-api-in-net-part-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipelines - a guided tour of the new IO API in .NET, part 3&lt;/h1&gt;&lt;p&gt;Update: please also see &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-31.html"&gt;part 3.1&lt;/a&gt; for further clarifications on this post&lt;/p&gt;&lt;p&gt;Sorry, it has been longer than anticipated since &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html" rel="nofollow"&gt;part 2&lt;/a&gt; (also: &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html" rel="nofollow"&gt;part 1&lt;/a&gt;). A large part of the reason for that is that I've been trying to think how best to explain some of the inner guts of &lt;a href="https://github.com/StackExchange/StackExchange.Redis"&gt;&lt;code&gt;StackExchange.Redis&lt;/code&gt;&lt;/a&gt; in a way that makes it easy to understand, and is useful for someone trying to learn about "pipelines", not &lt;code&gt;StackExchange.Redis&lt;/code&gt;. I've also been thinking on ways to feed more practical "pipelines" usage guidance into the mix, which was something that came up &lt;em&gt;a lot&lt;/em&gt; in feedback to parts 1/2.&lt;/p&gt;&lt;p&gt;In the end, I decided that the best thing to do was to step back from &lt;code&gt;StackExchange.Redis&lt;/code&gt;, and use a &lt;em&gt;completely different example&lt;/em&gt;, but one that faces almost all of the same challenges.&lt;/p&gt;&lt;p&gt;So, with your kind permission, I'd like to deviate from our previously advertised agenda, and instead talk about a library by my colleague &lt;a href="https://twitter.com/haneycodes" rel="nofollow"&gt;David Haney&lt;/a&gt; - &lt;a href="https://github.com/haneytron/simplsockets"&gt;&lt;code&gt;SimplSockets&lt;/code&gt;&lt;/a&gt;. What I hope to convey is a range of both the &lt;em&gt;reasoning&lt;/em&gt; behind prefering pipelines, but also practical guidance that the reader can directly transfer to their own IO-based needs. In particular, I hope to discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;different ways to pass chunks of data between APIs&lt;/li&gt;&lt;li&gt;working effectively with the array-pool&lt;/li&gt;&lt;li&gt;&lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; optimization in the context of libraries&lt;/li&gt;&lt;li&gt;practical real-world examples of writing to and reading from pipelines&lt;/li&gt;&lt;li&gt;how to connect pipelines client and server types to the network&lt;/li&gt;&lt;li&gt;performance comparisons from pipelines, and tips on measuring performance&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I'll be walking through &lt;em&gt;a lot&lt;/em&gt; of code here, but I'll also be making the "real" code available for further exploration; this also includes some things I dodn't have time to cover here, such as how to host a pipelines server inside the Kestrel server.&lt;/p&gt;&lt;p&gt;Sound good?&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-what-is-simplsockets" class="anchor" aria-hidden="true" href="#what-is-simplsockets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is &lt;code&gt;SimplSockets&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;This is a network helper library designed to make it easier to implement basic client/server network comms over a socket:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it implements a simple framing protocol to separate messages&lt;/li&gt;&lt;li&gt;it allows for concurrent usage over a single client, with a message queuing mechanism&lt;/li&gt;&lt;li&gt;it embeds additional data in the framing data to allow responses to be tied back to requests, to complete operations&lt;/li&gt;&lt;li&gt;out-of-order and out-of-band replies are allowed - you might send requests &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt; - and get the responses &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; - i.e. two of the responses came in the opposite order (presumably &lt;code&gt;B&lt;/code&gt; took longer to execute), and &lt;code&gt;D&lt;/code&gt; came from the server unsolicited (broadcasts, etc)&lt;/li&gt;&lt;li&gt;individual messages are always complete in a single frame - there is no frame splitting&lt;/li&gt;&lt;li&gt;in terms of API surface: everything is synchronous and &lt;code&gt;byte[]&lt;/code&gt; based; for example the client has a &lt;code&gt;byte[] SendReceive(byte[])&lt;/code&gt; method that sends a payload and blocks until the corresponding response is received, and there is a &lt;code&gt;MessageReceived&lt;/code&gt; event for unsolicited messages that exposes a &lt;code&gt;byte[]&lt;/code&gt;&lt;/li&gt;&lt;li&gt;the server takes incoming requests via the same &lt;code&gt;MessageReceived&lt;/code&gt; event, and can (if required, not always) post replies via a &lt;code&gt;Reply(byte[], ...)&lt;/code&gt; method that also takes the incoming message (for pairing) - and has a &lt;code&gt;Broadcast(byte[])&lt;/code&gt; method for sending a message to all clients&lt;/li&gt;&lt;li&gt;there are some other nuances like heartbeats, but; that's probably enough&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So; we've probably got enough there to start talking about real-world - and very common - scenarios in network code, and we can use that to start thinking about how "pipelines" makes our life easier.&lt;/p&gt;&lt;p&gt;Also an important point: anything I say below is not meant to be critical of &lt;code&gt;SimplSockets&lt;/code&gt; - rather, it is to acknowledge that it was written when a &lt;em&gt;lot&lt;/em&gt; of pieces like "pipelines" and &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; &lt;em&gt;didn't exist&lt;/em&gt; - so it is more an exploration into how we &lt;em&gt;could&lt;/em&gt; implement this differently with today's tools.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-first-things-first-we-need-to-think-about-our-exchange-types" class="anchor" aria-hidden="true" href="#first-things-first-we-need-to-think-about-our-exchange-types"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;First things first: we need to think about our exchange types&lt;/h2&gt;&lt;p&gt;The first question I have here is - for received messages in particular: "how should we expose this data to consumers?". By this I mean: &lt;code&gt;SimplSockets&lt;/code&gt; went with &lt;code&gt;byte[]&lt;/code&gt; as the exchange type; can we improve on that? Unsurprisingly: yes. There are many approaches we can use here.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;at one extreme, we can stick with &lt;code&gt;byte[]&lt;/code&gt; - i.e. allocate a standalone copy of the data, that we can hand to the user; simple to work with, and very safe (nobody else sees that array - no risk of confusion), but it comes at the cost of allocations and copy time.&lt;/li&gt;&lt;li&gt;at the other extreme, we can use zero-copy - and stick with &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt; - this means we're consuming the non-contiguous buffers &lt;em&gt;in the pipe itself&lt;/em&gt;; this is &lt;em&gt;fast&lt;/em&gt;, but somewhat limiting - we can't &lt;em&gt;hand that out&lt;/em&gt;, because once we &lt;code&gt;Advance&lt;/code&gt; the pipe: that data is going to be recycled. This might be a good option for strictly controlled server-side processing (where the data never escapes the request context)&lt;/li&gt;&lt;li&gt;as an extension of &lt;code&gt;2&lt;/code&gt;, we could move the &lt;em&gt;payload&lt;/em&gt; parsing code into the library (based on the live &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt;), just exposing the &lt;em&gt;deconstructed&lt;/em&gt; data, perhaps using custom &lt;code&gt;struct&lt;/code&gt;s that map to the scenario; efficient, but requires lots more knowledge of the contents than a general message-passing API allows; this might be a good option if you can pair the library with a serializer that accepts input as &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt;, though - allowing the serializer to work on the data without any copies&lt;/li&gt;&lt;li&gt;we could return a &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; to a copy of the data, perhaps using an oversized &lt;code&gt;byte[]&lt;/code&gt; from the &lt;code&gt;ArrayPool&amp;lt;byte&amp;gt;.Shared&lt;/code&gt; pool; but it isn't necessarily obvious to the consumer that they should return it to the pool (and indeed: getting a &lt;code&gt;T[]&lt;/code&gt; array back from a &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt; is an advanced and "unsafe" operation - not all &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt; is based on &lt;code&gt;T[]&lt;/code&gt; - so we &lt;em&gt;really&lt;/em&gt; shouldn't encourage users to try)&lt;/li&gt;&lt;li&gt;we could compromise by returning something that &lt;em&gt;provides&lt;/em&gt; a &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; (or &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; etc), but which makes it &lt;em&gt;very obvious&lt;/em&gt; via a well-known API that the user is meant to do something when they're done with it - i.e. &lt;code&gt;IDisposable&lt;/code&gt; / &lt;code&gt;using&lt;/code&gt; - and have the exchange-type &lt;em&gt;itself&lt;/em&gt; return things to the pool when &lt;code&gt;Dispose()&lt;/code&gt; is called&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In the context of a general purpose messaging API, I think that &lt;code&gt;5&lt;/code&gt; is a reasonable option - it means the caller &lt;em&gt;can&lt;/em&gt; store the data for some period while they work with it, without jamming the pipe, while still allowing us to make good use of the array pool. And if someone forgets the &lt;code&gt;using&lt;/code&gt;, it is &lt;em&gt;less efficient&lt;/em&gt;, but nothing will actually explode - it just means it'll tend to run a bit more like option &lt;code&gt;1&lt;/code&gt;. But: this decision of exchange types needs careful consideration for your scenario. The &lt;code&gt;StackExchange.Redis&lt;/code&gt; client uses option &lt;code&gt;3&lt;/code&gt;, handing out deconstructed data; I also have a fake redis &lt;em&gt;server&lt;/em&gt; using the &lt;code&gt;StackExchange.Redis&lt;/code&gt; framing code, which uses option &lt;code&gt;2&lt;/code&gt; - never allowing live escape a request context. You need to take time in considering your exchange types, because it is basically impossible to change this later!&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;As a pro tip for option &lt;code&gt;2&lt;/code&gt; (using live &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt; data and not letting it escape the context - zero-copy for maxiumum efficiency), one way to &lt;em&gt;guarantee&lt;/em&gt; this is to wrap the buffer in a domain-specific &lt;code&gt;ref struct&lt;/code&gt; before handing it to the code that needs to consume it. It is impossible to store a &lt;code&gt;ref struct&lt;/code&gt;, which includes holding onto it in an &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; context, and includes basic reflection (since that requires "boxing", and you cannot "box" a &lt;code&gt;ref struct&lt;/code&gt;) - so you have confidence that when the method completes, they no longer have indirect access to the data.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;But, let's assume we're happy with option &lt;code&gt;5&lt;/code&gt; (&lt;em&gt;for this specific scenario&lt;/em&gt; - there is no general "here's the option you should use", except: not &lt;code&gt;1&lt;/code&gt; if you can help it). What might that look like? It turns out that this intent is already desribed in the framework, as &lt;code&gt;System.Buffers.IMemoryOwner&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;interface&lt;/span&gt; &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; : &lt;span class="pl-en"&gt;IDisposable&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;Memory&lt;/span&gt; { &lt;span class="pl-k"&gt;get&lt;/span&gt;; }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then implement this to put our leased arrays back into the array-pool when disposed, taking care to be thread-safe so that if it is disposed twice, we don't put the array into the pool twice (very bad):&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;sealed&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;ArrayPoolOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; : &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;_length&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-en"&gt;T&lt;/span&gt;[] &lt;span class="pl-smi"&gt;_oversized&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;internal&lt;/span&gt; &lt;span class="pl-en"&gt;ArrayPoolOwner&lt;/span&gt;(&lt;span class="pl-en"&gt;T&lt;/span&gt;[] &lt;span class="pl-smi"&gt;oversized&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;_length&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-smi"&gt;_oversized&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;oversized&lt;/span&gt;;                &lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;Memory&lt;/span&gt; &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;(&lt;span class="pl-en"&gt;GetArray&lt;/span&gt;(), &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;_length&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-en"&gt;T&lt;/span&gt;[] &lt;span class="pl-en"&gt;GetArray&lt;/span&gt;() &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-smi"&gt;Interlocked&lt;/span&gt;.&lt;span class="pl-en"&gt;CompareExchange&lt;/span&gt;(&lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-smi"&gt;_oversized&lt;/span&gt;, &lt;span class="pl-c1"&gt;null&lt;/span&gt;, &lt;span class="pl-c1"&gt;null&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;??&lt;/span&gt; &lt;span class="pl-k"&gt;throw&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;ObjectDisposedException&lt;/span&gt;(&lt;span class="pl-en"&gt;ToString&lt;/span&gt;());&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;Dispose&lt;/span&gt;()&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;arr&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Interlocked&lt;/span&gt;.&lt;span class="pl-en"&gt;Exchange&lt;/span&gt;(&lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-smi"&gt;_oversized&lt;/span&gt;, &lt;span class="pl-c1"&gt;null&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;arr&lt;/span&gt; &lt;span class="pl-k"&gt;!=&lt;/span&gt; &lt;span class="pl-c1"&gt;null&lt;/span&gt;) &lt;span class="pl-smi"&gt;ArrayPool&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;.&lt;span class="pl-smi"&gt;Shared&lt;/span&gt;.&lt;span class="pl-en"&gt;Return&lt;/span&gt;(&lt;span class="pl-smi"&gt;arr&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The key point here is in &lt;code&gt;Dispose()&lt;/code&gt;, where it swaps out the array field (using &lt;code&gt;Interlocked.Exchange&lt;/code&gt;), and puts the array back into the pool. Once we've done this, subsequent calls to &lt;code&gt;.Memory&lt;/code&gt; will fail, and calls to &lt;code&gt;Dispose()&lt;/code&gt; will do nothing.&lt;/p&gt;&lt;p&gt;Some important things to know about the array pool:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;the arrays it gives you are often &lt;em&gt;oversized&lt;/em&gt; (so that it can give you a larger array if it doesn't have one in exactly your size, but it has a larger one ready to go). This means we need to track the &lt;em&gt;expected&lt;/em&gt; length (&lt;code&gt;_length&lt;/code&gt;), and use that when constructing &lt;code&gt;.Memory&lt;/code&gt;.&lt;/li&gt;&lt;li&gt;the array &lt;em&gt;is not zeroed upon fetch&lt;/em&gt; - it can contain garbage. In our case, this isn't a problem because (below) we are &lt;em&gt;immediately&lt;/em&gt; going to overwrite it with the data we want to represent, so the external caller will never see this, but &lt;em&gt;in the general case&lt;/em&gt;, you might want to consider a: should I zero the contents on behalf of the receiver before giving it to them?, and b: is my data sensitive such that I don't want to accidentally leak it into the pool? (there is an existing "zero when &lt;em&gt;returning&lt;/em&gt; to the pool" option in the array-pool, for this reason)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As a side note, I wonder whether the above concept might be a worthy addition inside the framework itself, for usage directly from &lt;code&gt;ArrayPool&amp;lt;T&amp;gt;&lt;/code&gt; - i.e. a method like &lt;code&gt;IMemoryOwner&amp;lt;T&amp;gt; RentOwned(int length)&lt;/code&gt; alongside &lt;code&gt;T[] Rent(int minimumLength)&lt;/code&gt; - perhaps with the additions of flags for "zero upon fetch" and "zero upon return".&lt;/p&gt;&lt;p&gt;The idea here is that passing an &lt;code&gt;IMemoryOwner&amp;lt;T&amp;gt;&lt;/code&gt; expresses a transfer of ownership, so a typical usage might be:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;DoSomethingWith&lt;/span&gt;(&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;data&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-smi"&gt;data&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; ... other things here ...&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;DoTheThing&lt;/span&gt;(&lt;span class="pl-smi"&gt;data&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; ... more things here ...&lt;/span&gt;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The caller doesn't need to know about the implementation details (array-pool, etc). Note that we still have to allocate a &lt;em&gt;small&lt;/em&gt; object to represent this, but this is still hugely preferable to allocating a large &lt;code&gt;byte[]&lt;/code&gt; buffer each time, for our safety.&lt;/p&gt;&lt;p&gt;As a caveat, we should note that a badly written consumer could store the &lt;code&gt;.Memory&lt;/code&gt; somewhere, which would lead to undefined behaviour after it has been disposed; or they could use &lt;code&gt;MemoryMarshal&lt;/code&gt; to get an array from the memory. If we &lt;em&gt;really needed to prevent these problems&lt;/em&gt;, we could do so by implementing a custom &lt;code&gt;MemoryManager&amp;lt;T&amp;gt;&lt;/code&gt; (most likely, by making &lt;code&gt;ArrayPoolOwner&amp;lt;T&amp;gt; : MemoryManager&amp;lt;T&amp;gt;&lt;/code&gt;, since &lt;code&gt;MemoryManager&amp;lt;T&amp;gt; : IMemoryOwner&amp;lt;T&amp;gt;&lt;/code&gt;). We could then make &lt;code&gt;.Span&lt;/code&gt; fail just like &lt;code&gt;.Memory&lt;/code&gt; does above, and we could prevent &lt;code&gt;MemoryMarshal&lt;/code&gt; from being able to obtain the underlying array. It is almost certainly overkill here, but it is useful to know that this option exists, for more extreme scenarios.&lt;/p&gt;&lt;p&gt;At this point you're probably thinking "wow, Marc, you're really over-thinking this - just give them the data", but: getting the exchange types right is probably the single most important design decision you have to make, so: this bit matters!&lt;/p&gt;&lt;p&gt;OK, so how would we populate this? Fortunately, that is pretty simple, as &lt;code&gt;ReadOnlySequence&amp;lt;T&amp;gt;&lt;/code&gt; has a very handy &lt;code&gt;CopyTo&lt;/code&gt; method that does all the heavy lifting:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;Lease&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;(&lt;br /&gt;    &lt;span class="pl-k"&gt;this&lt;/span&gt; &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;source&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;source&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsEmpty&lt;/span&gt;) &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;Empty&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;();&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;len&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;checked&lt;/span&gt;((&lt;span class="pl-k"&gt;int&lt;/span&gt;)&lt;span class="pl-smi"&gt;source&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;arr&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;ArrayPool&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;.&lt;span class="pl-smi"&gt;Shared&lt;/span&gt;.&lt;span class="pl-en"&gt;Rent&lt;/span&gt;(&lt;span class="pl-smi"&gt;len&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;source&lt;/span&gt;.&lt;span class="pl-en"&gt;CopyTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;arr&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;ArrayPoolOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;(&lt;span class="pl-smi"&gt;arr&lt;/span&gt;, &lt;span class="pl-smi"&gt;len&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This shows how we can use &lt;code&gt;ArrayPool&amp;lt;T&amp;gt;&lt;/code&gt; to obtain a (possibly oversized) array that we can use to hold a &lt;em&gt;copy&lt;/em&gt; of the data; once we've copied it, we can hand the &lt;em&gt;copy&lt;/em&gt; to a consumer to use however they need (and being a flat vector here makes it simple to consume), while the network code can advance the pipe and discard / re-use the buffers. When they &lt;code&gt;Dispose()&lt;/code&gt; it, it goes back in the pool, and everyone is happy.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-starting-the-base-api" class="anchor" aria-hidden="true" href="#starting-the-base-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting the base API&lt;/h2&gt;&lt;p&gt;There is a &lt;em&gt;lot&lt;/em&gt; of overlap in the code between a client and server; both need thread-safe mechanisms to write data, and both need some kind of read-loop to check for received data; but what &lt;em&gt;happens&lt;/em&gt; is different. So - it sounds like a a base-class might be useful; let's start with a skeleton API that let's us hand in a pipe (or two: recall that an &lt;code&gt;IDuplexPipe&lt;/code&gt; is actually the ends of two &lt;em&gt;different&lt;/em&gt; pipes - &lt;code&gt;.Input&lt;/code&gt; and &lt;code&gt;.Output&lt;/code&gt;):&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipeline&lt;/span&gt; : &lt;span class="pl-en"&gt;IDisposable&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt; &lt;span class="pl-smi"&gt;_pipe&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipeline&lt;/span&gt;(&lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt; &lt;span class="pl-smi"&gt;pipe&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_pipe&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;pipe&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;Dispose&lt;/span&gt;() &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;Close&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;Close&lt;/span&gt;() {&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;/*&lt;/span&gt; burn the pipe&lt;span class="pl-c"&gt;*/&lt;/span&gt;&lt;/span&gt;}&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first thing we need after this is some mechanism to send a message in a thread-safe way that doesn't block the caller unduly. The way &lt;code&gt;SimplSockets&lt;/code&gt; handles this (and also how &lt;code&gt;StackExchange.Redis&lt;/code&gt; v1 works) is to have a &lt;em&gt;message queue&lt;/em&gt; of messages that &lt;em&gt;have not yet been written&lt;/em&gt;. When the caller calls &lt;code&gt;Send&lt;/code&gt;, the messages is added to the queue (synchronized, etc), and will &lt;em&gt;at some point&lt;/em&gt; be dequeued and written to the socket. This helps with perceived performance and can help avoid packet fragmentation in some scenarios, &lt;em&gt;but&lt;/em&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it has a lot of moving parts&lt;/li&gt;&lt;li&gt;it duplicates something that "pipelines" already provides&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For the latter, specifically: the pipe &lt;strong&gt;is the queue&lt;/strong&gt;; meaning: we &lt;em&gt;already have&lt;/em&gt; a buffer of data between the actual output. Adding a &lt;em&gt;second&lt;/em&gt; queue is just duplicating this and retaining complexity, so: the second major design change we can make is: &lt;em&gt;throw away the unsent queue&lt;/em&gt;; just write to the pipe (synchronized, etc), and let the pipe worry about the rest. One slight consequence of this is that the v1 code had a concept of prioritising messages that are expecting a reply - essentially queue-jumping. By treating the pipe as the outbound queue we &lt;em&gt;lose this ability&lt;/em&gt;, but in reality this is unlikely to make a huge difference, so I'm happy to lose it. For very similar reasons, &lt;code&gt;StackExchange.Redis&lt;/code&gt; v2 loses the concept of &lt;code&gt;CommandFlags.HighPriority&lt;/code&gt;, which is this exact same queue-jumping idea. I'm not concerned by this.&lt;/p&gt;&lt;p&gt;We also need to consider the &lt;em&gt;shape&lt;/em&gt; of this API, to allow a server or client to add a messagee&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we don't necessarily want to be synchronous; we don't need to block while waiting to access to write to the pipe, or while waiting for a response from the server&lt;/li&gt;&lt;li&gt;we might want to expose alternate APIs for whether the caller is simply giving us memory to write (&lt;code&gt;ReadOnlyMember&amp;lt;byte&amp;gt;&lt;/code&gt;), or &lt;em&gt;giving us owneship&lt;/em&gt; of the data, for us to clean up when we've written it (&lt;code&gt;IMemoryOwner&amp;lt;byte&amp;gt;&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;let's assume that write and read are decoupled - we don't want to worry about the issues of response messages here&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So; putting that together, I quite like:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-smi"&gt;payload&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here we're giving the caller the conveninence of passing us either an &lt;code&gt;IMemoryOwner&amp;lt;byte&amp;gt;&lt;/code&gt; (which we then dispose correctly), or a &lt;code&gt;ReadOnlyMemory&amp;lt;byte&amp;gt;&lt;/code&gt; if they don't need to convery ownership.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;ValueTask&lt;/code&gt; makes sense because a write &lt;em&gt;to a pipe&lt;/em&gt; is often synchronous; we &lt;em&gt;probably&lt;/em&gt; won't be contested for the single-writer access, and the only async part of writing to a pipe is flushing &lt;em&gt;if the pipe is backed up&lt;/em&gt; (flushing is very often always synchronous). The &lt;code&gt;messageId&lt;/code&gt; is the additional metadata in the frame header that lets us pair replies later. We'll worry about what it &lt;em&gt;is&lt;/em&gt; in a bit.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-writes-and-wrongs" class="anchor" aria-hidden="true" href="#writes-and-wrongs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Writes and wrongs&lt;/h2&gt;&lt;p&gt;So; let's implement that. The first thing we need is guaranteed single-writer access. It would be tempting to use a &lt;code&gt;lock&lt;/code&gt;, but &lt;code&gt;lock&lt;/code&gt; &lt;em&gt;doesn't play well with &lt;code&gt;async&lt;/code&gt;&lt;/em&gt; (&lt;a href="https://twitter.com/marcgravell/status/1023176337652109312" rel="nofollow"&gt;even if you don't screw it up&lt;/a&gt;). Because the flush &lt;em&gt;may&lt;/em&gt; be async, the continuation could come back on another thread, so we need an &lt;code&gt;async&lt;/code&gt;-compatible locking primitive; &lt;code&gt;SemaphoreSlim&lt;/code&gt; should suffice.&lt;/p&gt;&lt;p&gt;Next, I'm going to go off on one of my wild tangents. Premise:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;In general, application code should be optimized for readability; library code should be optimized for performance.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;You may or may not agree with this, but it is the general guidance that I code by. What I mean by this is that &lt;em&gt;library&lt;/em&gt; code tends to have a &lt;em&gt;single focused purpose&lt;/em&gt;, often being maintained by someone whose experience may be "deep but not necessarily wide"; your mind is focusing on that one area, and it is OK to go to bizarre lengths to optimize the code. Conversely, &lt;em&gt;application&lt;/em&gt; code tends to involve a lot more plumbing of &lt;em&gt;different&lt;/em&gt; concepts - "wide but not necessarily deep" (the depth being hidden in the various libraries). Application code often has more complex and unpredictable interactions, so the focus should be on maintainable and "obviously right".&lt;/p&gt;&lt;p&gt;Basically, my point here is that I tend to focus a lot on optimizations that you wouldn't normally put into application code, because &lt;em&gt;I know from experience and extensive benchmarking&lt;/em&gt; that they &lt;em&gt;really matter&lt;/em&gt;. So... I'm going to do some things that might look odd, and I want you to take that journey with me.&lt;/p&gt;&lt;p&gt;Let's start with the "obviously right" implementation:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-smi"&gt;SemaphoreSlim&lt;/span&gt; &lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;SemaphoreSlim&lt;/span&gt;(&lt;span class="pl-c1"&gt;1&lt;/span&gt;);&lt;br /&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;WaitAsync&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-en"&gt;WriteFrameHeader&lt;/span&gt;(&lt;span class="pl-smi"&gt;writer&lt;/span&gt;, &lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Release&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;code&gt;await&lt;/code&gt;s single-writer access to the pipe, writes the frame header using &lt;code&gt;WriteFrameHeader&lt;/code&gt; (which we'll show in a bit), then drops the &lt;code&gt;payload&lt;/code&gt; using the framework-provided &lt;code&gt;WriteAsync&lt;/code&gt; method, noting that this includes the &lt;code&gt;FlushAsync&lt;/code&gt; as well. There's nothing &lt;em&gt;wrong&lt;/em&gt; with this code, but... it does involve unnecessary state machine plumbing in the &lt;strong&gt;most likely case&lt;/strong&gt; - i.e. where everything completes synchronously (the writer is not contested, and the pipe is not backed up). We can tweak this code by asking:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;can I get the single-writer access uncontested?&lt;/li&gt;&lt;li&gt;was the flush synchronous?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Consider, instead - making the method we just wrote &lt;code&gt;private&lt;/code&gt; and renaming it to &lt;code&gt;WriteAsyncSlowPath&lt;/code&gt;, and adding a &lt;strong&gt;non-&lt;code&gt;async&lt;/code&gt;&lt;/strong&gt; method instead:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; try to get the conch; if not, switch to async&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Wait&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;))&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsyncSlowPath&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-smi"&gt;release&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-en"&gt;WriteFrameHeader&lt;/span&gt;(&lt;span class="pl-smi"&gt;writer&lt;/span&gt;, &lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;write&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;write&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompletedSuccessfully&lt;/span&gt;) &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-smi"&gt;release&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;AwaitFlushAndRelease&lt;/span&gt;(&lt;span class="pl-smi"&gt;write&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;release&lt;/span&gt;) &lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Release&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;AwaitFlushAndRelease&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;FlushResult&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;try&lt;/span&gt; { &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;; }&lt;br /&gt;    &lt;span class="pl-k"&gt;finally&lt;/span&gt; { &lt;span class="pl-smi"&gt;_singleWriter&lt;/span&gt;.&lt;span class="pl-en"&gt;Release&lt;/span&gt;(); }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;Wait(0)&lt;/code&gt; returns &lt;code&gt;true&lt;/code&gt; &lt;em&gt;if and only if&lt;/em&gt; we can take the semaphore synchronously without delay. If we can't: all bets are off, just switch to the &lt;code&gt;async&lt;/code&gt; version. Note once you've gone &lt;code&gt;async&lt;/code&gt;, there's no point doing any more of these "hot path" checks - you've already built a state machine (and probably boxed it): the meal is already paid for, so you might as well sit and eat.&lt;/p&gt;&lt;p&gt;However, if we &lt;em&gt;do&lt;/em&gt; get the semaphore for free, we can continue and do our &lt;em&gt;writing&lt;/em&gt; for free. The header is synchronous &lt;em&gt;anyway&lt;/em&gt;, so our next decision is: did the &lt;em&gt;flush&lt;/em&gt; complete synchronously? If it did (&lt;code&gt;IsCompletedSuccessfully&lt;/code&gt;), &lt;em&gt;we're done&lt;/em&gt; - away we go (&lt;code&gt;return default;&lt;/code&gt;). Otherwise, we'll need to &lt;code&gt;await&lt;/code&gt; the flush. Now, we can't do that from our non-&lt;code&gt;async&lt;/code&gt; method, but we can write a &lt;em&gt;separate&lt;/em&gt; method (&lt;code&gt;AwaitFlushAndRelease&lt;/code&gt;) that takes our incomplete flush, and &lt;code&gt;await&lt;/code&gt;s it. In particular, note that we only want the semaphore to be released &lt;em&gt;after&lt;/em&gt; the flush has completed, hence the &lt;code&gt;Release()&lt;/code&gt; in our helper method. This is also why we set &lt;code&gt;release&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in the calling method, so it doesn't get released prematurely.&lt;/p&gt;&lt;p&gt;We can apply similar techniques to &lt;em&gt;most&lt;/em&gt; &lt;code&gt;async&lt;/code&gt; operations if we know they're going to &lt;em&gt;often&lt;/em&gt; be synchronous, and it is a pattern you may wish to consider. Emphasis: it doesn't help you &lt;em&gt;at all&lt;/em&gt; if the result is usually or always &lt;em&gt;genuinely&lt;/em&gt; asynchronous - so: don't over-apply it.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Right; so - how do we write the header? What &lt;em&gt;is&lt;/em&gt; the header? &lt;code&gt;SimplSockets&lt;/code&gt; defines the header to be 8 bytes composed of two little-endian 32-bit integers. The first 4 bytes contains the payload length in bytes; the second 4 bytes is the &lt;code&gt;messageId&lt;/code&gt; used to correlate requests and responses. Writing this is remarkably simple:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;WriteFrameHeader&lt;/span&gt;(&lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;span&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;GetSpan&lt;/span&gt;(&lt;span class="pl-c1"&gt;8&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;BinaryPrimitives&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteInt32LittleEndian&lt;/span&gt;(&lt;br /&gt;        &lt;span class="pl-smi"&gt;span&lt;/span&gt;, &lt;span class="pl-smi"&gt;length&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;BinaryPrimitives&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteInt32LittleEndian&lt;/span&gt;(&lt;br /&gt;        &lt;span class="pl-smi"&gt;span&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;4&lt;/span&gt;), &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Advance&lt;/span&gt;(&lt;span class="pl-c1"&gt;8&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can ask a &lt;code&gt;PipeWriter&lt;/code&gt; for "reasonable" sized buffers with confidence, and &lt;code&gt;8&lt;/code&gt; bytes is certainly a reasonable size. The helpful framework-provided &lt;code&gt;BinaryPrimitives&lt;/code&gt; type provides explicit-endian tools, perfect for network code. The first call writes &lt;code&gt;length&lt;/code&gt; to the first 4 bytes of the span. After that, we need to &lt;code&gt;Slice&lt;/code&gt; the span so that the second call writes to the &lt;em&gt;next&lt;/em&gt; 4 bytes - and finally we call &lt;code&gt;Advance(8)&lt;/code&gt; which commits our header to the pipe &lt;em&gt;without&lt;/em&gt; flushing it. Normally, you might have to write lots of pieces manually, then call &lt;code&gt;FlushAsync&lt;/code&gt; explicitly, but this particular protocol is a good fit for simply calling &lt;code&gt;WriteAsync&lt;/code&gt; on the pipe to attach the payload, which &lt;em&gt;includes&lt;/em&gt; the flush. So; putting those pieces together, we've successfully written our message to the pipe.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-using-that-from-a-client" class="anchor" aria-hidden="true" href="#using-that-from-a-client"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using that from a client&lt;/h2&gt;&lt;p&gt;We have a &lt;code&gt;WriteAsync&lt;/code&gt; method in the base class; now let's add a concrete client class and start hooking pieces together. Consider:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineClient&lt;/span&gt; : &lt;span class="pl-en"&gt;SimplPipeline&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt;&amp;gt; &lt;span class="pl-en"&gt;SendReceiveAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;tcs&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;TaskCompletionSource&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt;&amp;gt;();&lt;br /&gt;        &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;lock&lt;/span&gt; (&lt;span class="pl-smi"&gt;_awaitingResponses&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;++&lt;/span&gt;&lt;span class="pl-smi"&gt;_nextMessageId&lt;/span&gt;;&lt;br /&gt;            &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;;&lt;br /&gt;            &lt;span class="pl-smi"&gt;_awaitingResponses&lt;/span&gt;.&lt;span class="pl-en"&gt;Add&lt;/span&gt;(&lt;span class="pl-smi"&gt;messageId&lt;/span&gt;, &lt;span class="pl-smi"&gt;tcs&lt;/span&gt;);&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;tcs&lt;/span&gt;.&lt;span class="pl-smi"&gt;Task&lt;/span&gt;;&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt;&amp;gt; &lt;span class="pl-en"&gt;SendReceiveAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;SendReceiveAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;);&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;code&gt;_awaitingResponses&lt;/code&gt; is a dictionary of &lt;code&gt;int&lt;/code&gt; message-ids to &lt;code&gt;TaskCompletionSource&amp;lt;IMemoryOwner&amp;lt;byte&amp;gt;&amp;gt;&lt;/code&gt;. This code invents a new &lt;code&gt;messageId&lt;/code&gt; (avoiding zero, which we'll use as a sentinel value), and creates a &lt;code&gt;TaskCompletionSource&amp;lt;T&amp;gt;&lt;/code&gt; to represent our in-progress operation. Since this definitely will involve network access, there's no benefit in exposing it as &lt;code&gt;ValueTask&amp;lt;T&amp;gt;&lt;/code&gt;, so this works well. Once we've added our placeholder for catching the reply we write our message (always do book-keeping &lt;em&gt;first&lt;/em&gt;, to avoid race conditions). Finally, expose the incomplete task to the caller.&lt;/p&gt;&lt;p&gt;Note that I've implemented this the "obvious" way, but we can optimize this like we did previously, by checking if &lt;code&gt;WriteAsync&lt;/code&gt; completed synchronously and simply &lt;code&gt;return&lt;/code&gt;ing the &lt;code&gt;tcs.Task&lt;/code&gt; without &lt;code&gt;await&lt;/code&gt;ing it. Note also that &lt;code&gt;SimplSockets&lt;/code&gt; used the &lt;em&gt;calling thread-id&lt;/em&gt; as the message-id; this works fine in a blocking scenario, but it isn't viable when we're using &lt;code&gt;async&lt;/code&gt; - but: the number is opaque to the "other end" &lt;em&gt;anyway&lt;/em&gt; - all it has to do is return the same number.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-programmed-to-receive" class="anchor" aria-hidden="true" href="#programmed-to-receive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Programmed to receive&lt;/h2&gt;&lt;p&gt;That's pretty-much it for write; next we need to think about receive. As mentioned in the previous posts, there's almost always a receive &lt;em&gt;loop&lt;/em&gt; - especially if we need to support out-of-band and out-of-order messages (so: we can't just read one frame immediately after writing). A basic read loop can be approximated by:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt; &lt;span class="pl-en"&gt;StartReceiveLoopAsync&lt;/span&gt;(&lt;br /&gt;   &lt;span class="pl-en"&gt;CancellationToken&lt;/span&gt; &lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;   &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;   {&lt;br /&gt;       &lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCancellationRequested&lt;/span&gt;)&lt;br /&gt;       {&lt;br /&gt;           &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;readResult&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;);&lt;br /&gt;           &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;readResult&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;           &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;readResult&lt;/span&gt;.&lt;span class="pl-smi"&gt;Buffer&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;           &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;makingProgress&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;           &lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-en"&gt;TryParseFrame&lt;/span&gt;(&lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;))&lt;br /&gt;           {&lt;br /&gt;               &lt;span class="pl-smi"&gt;makingProgress&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;&lt;br /&gt;               &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;OnReceiveAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;           }&lt;br /&gt;           &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;Start&lt;/span&gt;, &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;br /&gt;           &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-k"&gt;!&lt;/span&gt;&lt;span class="pl-smi"&gt;makingProgress&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-smi"&gt;readResult&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;       }&lt;br /&gt;       &lt;span class="pl-k"&gt;try&lt;/span&gt; { &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;Complete&lt;/span&gt;(); } &lt;span class="pl-k"&gt;catch&lt;/span&gt; { }&lt;br /&gt;   }&lt;br /&gt;   &lt;span class="pl-k"&gt;catch&lt;/span&gt; (&lt;span class="pl-en"&gt;Exception&lt;/span&gt; &lt;span class="pl-smi"&gt;ex&lt;/span&gt;)&lt;br /&gt;   {&lt;br /&gt;       &lt;span class="pl-k"&gt;try&lt;/span&gt; { &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;Complete&lt;/span&gt;(&lt;span class="pl-smi"&gt;ex&lt;/span&gt;); } &lt;span class="pl-k"&gt;catch&lt;/span&gt; { }&lt;br /&gt;   }&lt;br /&gt;}&lt;br /&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;OnReceiveAsync&lt;/span&gt;(&lt;br /&gt;   &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: since we are &lt;em&gt;bound&lt;/em&gt; to have an &lt;code&gt;async&lt;/code&gt; delay at some point (probably immediately), we might as well just jump straight to an "obvoious" &lt;code&gt;async&lt;/code&gt; implementation - we'll gain nothing from trying to be clever here. Key points to observe:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we get data from the pipe (note that we &lt;em&gt;might&lt;/em&gt; want to also consider &lt;code&gt;TryRead&lt;/code&gt; here, but only if we are making progress - otherwise we could find ourselves in a hot loop)&lt;/li&gt;&lt;li&gt;read (&lt;code&gt;TryParseFrame&lt;/code&gt;) and process (&lt;code&gt;OnReceiveAsync&lt;/code&gt;) as many frames as we can&lt;/li&gt;&lt;li&gt;advance the reader to report our progress, noting that &lt;code&gt;TryParseFrame&lt;/code&gt; will have updated &lt;code&gt;buffer.Start&lt;/code&gt;, and since we're actively reading as many frames as we can, it is true to say that we've "inspected" to &lt;code&gt;buffer.End&lt;/code&gt;&lt;/li&gt;&lt;li&gt;keep in mind that the pipelines code is dealing with all the back-buffer concerns re data that we haven't consumed yet (usually a significant amount of code repeated in lots of libraries)&lt;/li&gt;&lt;li&gt;check for exit conditions - if we aren't progressing and the pipe won't get any more data, we're done&lt;/li&gt;&lt;li&gt;report when we've finished reading - through success or failure&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unsurprisingly, &lt;code&gt;TryParseFrame&lt;/code&gt; is largely the reverse of &lt;code&gt;WriteAsync&lt;/code&gt;:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-en"&gt;TryParseFrame&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-k"&gt;ref&lt;/span&gt; &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;input&lt;/span&gt;,&lt;br /&gt;    &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;8&lt;/span&gt;)&lt;br /&gt;    {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; not enough data for the header&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-smi"&gt;payload&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-smi"&gt;First&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;8&lt;/span&gt;)&lt;br /&gt;    {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; already 8 bytes in the first segment&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-smi"&gt;length&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;ParseFrameHeader&lt;/span&gt;(&lt;br /&gt;            &lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-smi"&gt;First&lt;/span&gt;.&lt;span class="pl-smi"&gt;Span&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;else&lt;/span&gt;&lt;br /&gt;    {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; copy 8 bytes into a local span&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;Span&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;local&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;stackalloc&lt;/span&gt; &lt;span class="pl-smi"&gt;byte&lt;/span&gt;[&lt;span class="pl-c1"&gt;8&lt;/span&gt;];&lt;br /&gt;        &lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;).&lt;span class="pl-en"&gt;CopyTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;local&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-smi"&gt;length&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;ParseFrameHeader&lt;/span&gt;(&lt;br /&gt;            &lt;span class="pl-smi"&gt;local&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; do we have the "length" bytes?&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt; &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;8&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;payload&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; success!&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;payload&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-smi"&gt;length&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First we check whether we have enough data for the frame header (8 bytes); if we don't have that - we certainly don't have a frame. Once we know we have enough bytes for the frame header, we can parse it out to find the payload length. This is a little subtle, because we need to recall that &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt; can be &lt;em&gt;discontiguous&lt;/em&gt; multiple buffers. Since we're only talking about 8 bytes, the simplest thing to do is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;check whether the &lt;em&gt;first segment&lt;/em&gt; has 8 bytes; if so, parse from that&lt;/li&gt;&lt;li&gt;otherwise, &lt;code&gt;stackalloc&lt;/code&gt; a span (note that this doesn't need &lt;code&gt;unsafe&lt;/code&gt;), copy 8 bytes from &lt;code&gt;input&lt;/code&gt; into that, and parse &lt;em&gt;from there&lt;/em&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Once we know how much payload we're expecting, we can check whether we &lt;em&gt;have that too&lt;/em&gt;; if we don't: cede back to the read loop. But if we do:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;our &lt;em&gt;actual payload&lt;/em&gt; is the &lt;code&gt;length&lt;/code&gt; bytes &lt;em&gt;after&lt;/em&gt; the header - i.e. &lt;code&gt;input.Slice(8, length)&lt;/code&gt;&lt;/li&gt;&lt;li&gt;we want to update &lt;code&gt;input&lt;/code&gt; by cutting off everything up to the end of the frame, i.e. &lt;code&gt;input = input.Slice(payload.End)&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This means that when we return &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;payload&lt;/code&gt; now contains the bytes that were sent to us, as a discontiguous buffer.&lt;/p&gt;&lt;p&gt;We should also take a look at &lt;code&gt;ParseFrameHeader&lt;/code&gt;, which is a close cousin to &lt;code&gt;WriteFrameHeader&lt;/code&gt;:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-en"&gt;ParseFrameHeader&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlySpan&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;input&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;BinaryPrimitives&lt;/span&gt;&lt;br /&gt;            .&lt;span class="pl-en"&gt;ReadInt32LittleEndian&lt;/span&gt;(&lt;span class="pl-smi"&gt;input&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;BinaryPrimitives&lt;/span&gt;&lt;br /&gt;            .&lt;span class="pl-en"&gt;ReadInt32LittleEndian&lt;/span&gt;(&lt;span class="pl-smi"&gt;input&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;4&lt;/span&gt;));&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;length&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once again, &lt;code&gt;BinaryPrimitives&lt;/code&gt; is helping us out, and we are slicing the &lt;code&gt;input&lt;/code&gt; in exactly the same way as before to get the two halves.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;So; we can parse frames; now we need to act upon them; here's our client implementation:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;OnReceiveAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;messageId&lt;/span&gt; &lt;span class="pl-k"&gt;!=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;br /&gt;    {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; request/response&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;TaskCompletionSource&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt;&amp;gt; &lt;span class="pl-smi"&gt;tcs&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;lock&lt;/span&gt; (&lt;span class="pl-smi"&gt;_awaitingResponses&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;_awaitingResponses&lt;/span&gt;.&lt;span class="pl-en"&gt;TryGetValue&lt;/span&gt;(&lt;span class="pl-smi"&gt;messageId&lt;/span&gt;, &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-smi"&gt;tcs&lt;/span&gt;))&lt;br /&gt;            {&lt;br /&gt;                &lt;span class="pl-smi"&gt;_awaitingResponses&lt;/span&gt;.&lt;span class="pl-en"&gt;Remove&lt;/span&gt;(&lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;            }&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-smi"&gt;tcs&lt;/span&gt;&lt;span class="pl-k"&gt;?&lt;/span&gt;.&lt;span class="pl-en"&gt;TrySetResult&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-en"&gt;Lease&lt;/span&gt;());&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;else&lt;/span&gt;&lt;br /&gt;    {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; unsolicited&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-smi"&gt;MessageReceived&lt;/span&gt;&lt;span class="pl-k"&gt;?&lt;/span&gt;.&lt;span class="pl-en"&gt;Invoke&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-en"&gt;Lease&lt;/span&gt;());&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This code has two paths; it can be the request/response scenario, or it can be an out-of-band response message with no request. So; if we &lt;em&gt;have&lt;/em&gt; a non-zero &lt;code&gt;messageId&lt;/code&gt;, we check (synchronized) in our &lt;code&gt;_awaitingResponses&lt;/code&gt; dictionary to see if we have a message awaiting completion. If we do, we use &lt;code&gt;TrySetResult&lt;/code&gt; to complete the task (after exiting the &lt;code&gt;lock&lt;/code&gt;), giving it a lease with the data from the message. Otherwise, we check whether the &lt;code&gt;MessageReceived&lt;/code&gt; event is subscribed, and invoke that similarly. In both cases, the use of &lt;code&gt;?.&lt;/code&gt; here means that we don't populate a leased array if nobody is listening. It will be the receiver's job to ensure the lease is disposed, as only they can know the lifetime.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-service-please" class="anchor" aria-hidden="true" href="#service-please"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Service, please&lt;/h2&gt;&lt;p&gt;We need to think a little about how we orchestrate this at the server. The &lt;code&gt;SimplPipeline&lt;/code&gt; base type above relates to a &lt;em&gt;single&lt;/em&gt; connection - it is essentially a proxy to a socket. But servers usually have many clients. Because of that, we'll create a server type that does the &lt;em&gt;actual processing&lt;/em&gt;, that internally has a client-type that is our &lt;code&gt;SimplPipeline&lt;/code&gt;, and a set of connected clients; so:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt; : &lt;span class="pl-en"&gt;IDisposable&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;abstract&lt;/span&gt; ValueTask&amp;lt;IMemoryOwner&amp;lt;byte&amp;gt;&amp;gt; &lt;br /&gt;        &lt;span class="pl-en"&gt;OnReceiveForReplyAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;    &lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;ClientCount&lt;/span&gt; &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_clients&lt;/span&gt;.&lt;span class="pl-smi"&gt;Count&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt; &lt;span class="pl-en"&gt;RunClientAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt; &lt;span class="pl-smi"&gt;pipe&lt;/span&gt;,&lt;br /&gt;        &lt;span class="pl-en"&gt;CancellationToken&lt;/span&gt; &lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;Client&lt;/span&gt;(&lt;span class="pl-smi"&gt;pipe&lt;/span&gt;, &lt;span class="pl-k"&gt;this&lt;/span&gt;).&lt;span class="pl-en"&gt;RunAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;);&lt;br /&gt;    &lt;br /&gt;    &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Client&lt;/span&gt; : &lt;span class="pl-en"&gt;SimplPipeline&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt; &lt;span class="pl-en"&gt;RunAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;CancellationToken&lt;/span&gt; &lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;)&lt;br /&gt;            &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;StartReceiveLoopAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;readonly&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt; &lt;span class="pl-smi"&gt;_server&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;Client&lt;/span&gt;(&lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt; &lt;span class="pl-smi"&gt;pipe&lt;/span&gt;, &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt; &lt;span class="pl-smi"&gt;server&lt;/span&gt;)&lt;br /&gt;            : &lt;span class="pl-k"&gt;base&lt;/span&gt;(&lt;span class="pl-smi"&gt;pipe&lt;/span&gt;) &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_server&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;server&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;OnReceiveAsync&lt;/span&gt;(&lt;br /&gt;            &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;, &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;msg&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt;.&lt;span class="pl-en"&gt;Lease&lt;/span&gt;())&lt;br /&gt;            {&lt;br /&gt;                &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;response&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_server&lt;/span&gt;.&lt;span class="pl-en"&gt;OnReceiveForReplyAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;msg&lt;/span&gt;);&lt;br /&gt;                &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;response&lt;/span&gt;, &lt;span class="pl-smi"&gt;messageId&lt;/span&gt;);&lt;br /&gt;            }&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So; our &lt;em&gt;publicly visible server type&lt;/em&gt;, &lt;code&gt;SimplPipelineServer&lt;/code&gt; has an &lt;code&gt;abstract&lt;/code&gt; method for providing the implementation for &lt;em&gt;what we want to do with messages&lt;/em&gt;: &lt;code&gt;OnReceiveForReplyAsync&lt;/code&gt; - that takes a payload, and returns the response. Behind the scenes we have a set of clients, &lt;code&gt;_clients&lt;/code&gt;, although the details of that aren't interesting.&lt;/p&gt;&lt;p&gt;We accept new clients via the &lt;code&gt;RunClientAsync&lt;/code&gt; method; this might seem counter-intuitive, but the emerging architecture for pipelines servers (especially considering "Kestrel" hosts) is to let an external host deal with listening and accepting connections, and all we need to do is have something that accepts an &lt;code&gt;IDuplexPipe&lt;/code&gt; and returns a &lt;code&gt;Task&lt;/code&gt;. In this case, what that &lt;em&gt;does&lt;/em&gt; is create a new &lt;code&gt;Client&lt;/code&gt; and start the client's read loop, &lt;code&gt;StartReceiveLoopAsync&lt;/code&gt;. When the client receives a message (&lt;code&gt;OnReceiveAsync&lt;/code&gt;), it asks the server for a response (&lt;code&gt;_server.OnReceiveForReplyAsync&lt;/code&gt;), and then writes that response back via &lt;code&gt;WriteAsync&lt;/code&gt;. Note that the version of &lt;code&gt;OnReceiveAsync&lt;/code&gt; shown has the consequence of meaning that we can't handle multiple overlapped messages on the same connection at the same time; the "real" version has been aggressively uglified, to check whether &lt;code&gt;_server.OnReceiveForReplyAsync(msg)&lt;/code&gt; has completed synchronously; if it hasn't, then it schedules a &lt;em&gt;continuation&lt;/em&gt; to perform the &lt;code&gt;WriteAsync&lt;/code&gt; (also handling the disposal of &lt;code&gt;msg&lt;/code&gt;), and yields to the caller. It also optimizes for the "everything is synchronous" case.&lt;/p&gt;&lt;p&gt;The only other server API we need is a broadcast:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;int&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;BroadcastAsync&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;count&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;foreach&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt; &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-smi"&gt;_clients&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt;.&lt;span class="pl-smi"&gt;Key&lt;/span&gt;.&lt;span class="pl-en"&gt;SendAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;);&lt;br /&gt;            &lt;span class="pl-smi"&gt;count&lt;/span&gt;&lt;span class="pl-k"&gt;++&lt;/span&gt;;&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-k"&gt;catch&lt;/span&gt; { } &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; ignore failures on specific clients&lt;/span&gt;&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;count&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(again, possibly with an overload that takes &lt;code&gt;IMemoryOwner&amp;lt;byte&amp;gt;&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;where &lt;code&gt;SendAsync&lt;/code&gt; is simply:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;SendAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;ReadOnlyMemory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;    &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;message&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;);&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;a id="user-content-putting-it-all-together-implementing-a-client-and-server" class="anchor" aria-hidden="true" href="#putting-it-all-together-implementing-a-client-and-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Putting it all together; implementing a client and server&lt;/h2&gt;&lt;p&gt;So how can we &lt;em&gt;use&lt;/em&gt; all of this? How can we get a working client and server? Let's start with the simpler of the two, the client:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;using (var client = await SimplPipelineClient.ConnectAsync(&lt;br /&gt;    new IPEndPoint(IPAddress.Loopback, 5000)))&lt;br /&gt;{&lt;br /&gt;    // subscribe to broadcasts&lt;br /&gt;    client.MessageReceived += async msg =&amp;gt; {&lt;br /&gt;        if (!msg.Memory.IsEmpty)&lt;br /&gt;            await WriteLineAsync('*', msg);&lt;br /&gt;    };&lt;br /&gt;&lt;br /&gt;    string line;&lt;br /&gt;    while ((line = await Console.In.ReadLineAsync()) != null)&lt;br /&gt;    {&lt;br /&gt;        if (line == "q") break;&lt;br /&gt;&lt;br /&gt;        using (var leased = line.Encode())&lt;br /&gt;        {&lt;br /&gt;            var response = await client.SendReceiveAsync(leased.Memory);&lt;br /&gt;            await WriteLineAsync('&amp;lt;', response);&lt;br /&gt;        }     &lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;SimplPipelineClient.ConnectAsync&lt;/code&gt; here just uses &lt;code&gt;Pipelines.Sockets.Unofficial&lt;/code&gt; to spin up a client socket pipeline, and starts the &lt;code&gt;StartReceiveLoopAsync()&lt;/code&gt; method. Taking an additional dependency on &lt;code&gt;Pipelines.Sockets.Unofficial&lt;/code&gt; is vexing, but right now there is no framework-supplied client-socket API for pipelines, so: it'll do the job.&lt;/p&gt;&lt;p&gt;This code sets up a simple console client that takes keyboard input; if it receives a &lt;code&gt;"q"&lt;/code&gt; it quits; otherwise it sends the message to the server (&lt;code&gt;Encode&lt;/code&gt;, not shown, is just a simple text-encode into a leased buffer), and writes the response. The &lt;code&gt;WriteLineAsync&lt;/code&gt; method here takes a leased buffer, decodes it, and writes the output to the console - then disposes the buffer. We also listen for unsolicited messages via &lt;code&gt;MessageReceived&lt;/code&gt;, and write those to the console with a different prefix.&lt;/p&gt;&lt;p&gt;The server is a little more involved; first we need to implement a server; in this case let's simply reverse the bytes we get:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;ReverseServer&lt;/span&gt; : &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;override&lt;/span&gt; ValueTask&amp;lt;IMemoryOwner&amp;lt;byte&amp;gt;&amp;gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;OnReceiveForReplyAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; since the "message" outlives the response write,&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; we can do an in-place reverse and hand&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; the same buffer back&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;memory&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-en"&gt;Reverse&lt;/span&gt;(&lt;span class="pl-smi"&gt;memory&lt;/span&gt;.&lt;span class="pl-smi"&gt;Span&lt;/span&gt;); &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; details not shown&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;IMemoryOwner&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt;&amp;gt;(&lt;span class="pl-smi"&gt;memory&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;All this does is respond to messages by returning the same payload, but backwards. And yes, I realize that since we're dealing with text, this could go horribly wrong for grapheme-clusters and/or multi-byte code-points! I never said it was a &lt;em&gt;useful&lt;/em&gt; server...&lt;/p&gt;&lt;p&gt;Next up, we need a host. Kestrel (the "ASP.NET Core" server) is an excellent choice there, but implementing a Kestrel host requires introducing quite a few more concepts. But... since we already took a dependency on &lt;code&gt;Pipelines.Sockets.Unofficial&lt;/code&gt; for the client, we can use that for the server host with a few lines of code:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineSocketServer&lt;/span&gt; : &lt;span class="pl-en"&gt;SocketServer&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt; &lt;span class="pl-smi"&gt;Server&lt;/span&gt; { &lt;span class="pl-k"&gt;get&lt;/span&gt;; }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineSocketServer&lt;/span&gt;(&lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt; &lt;span class="pl-smi"&gt;server&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;Server&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;server&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt; &lt;span class="pl-en"&gt;OnClientConnectedAsync&lt;/span&gt;(&lt;br /&gt;        &lt;span class="pl-en"&gt;in&lt;/span&gt; &lt;span class="pl-smi"&gt;ClientConnection&lt;/span&gt; client)&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;Server&lt;/span&gt;.&lt;span class="pl-en"&gt;RunClientAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;client&lt;/span&gt;.&lt;span class="pl-smi"&gt;Transport&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineSocketServer&lt;/span&gt; &lt;span class="pl-en"&gt;For&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;T&lt;/span&gt;&amp;gt;()&lt;br /&gt;        &lt;span class="pl-k"&gt;where&lt;/span&gt; &lt;span class="pl-en"&gt;T&lt;/span&gt; : &lt;span class="pl-en"&gt;SimplPipelineServer&lt;/span&gt;, &lt;span class="pl-k"&gt;new&lt;/span&gt;()&lt;br /&gt;        &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;SimplPipelineSocketServer&lt;/span&gt;(&lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;T&lt;/span&gt;());&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;protected&lt;/span&gt; &lt;span class="pl-k"&gt;override&lt;/span&gt; &lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;Dispose&lt;/span&gt;(&lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-smi"&gt;disposing&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;disposing&lt;/span&gt;) &lt;span class="pl-smi"&gt;Server&lt;/span&gt;.&lt;span class="pl-en"&gt;Dispose&lt;/span&gt;();&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The key line in here is our &lt;code&gt;OnClientConnectedAsync&lt;/code&gt; method, which is how we accept new connections, simply by passing down the &lt;code&gt;client.Transport&lt;/code&gt; (an &lt;code&gt;IDuplexPipe&lt;/code&gt;). Hosting in Kestrel works very similarly, except you subclass &lt;code&gt;ConnectionHandler&lt;/code&gt; instead of &lt;code&gt;SocketServer&lt;/code&gt;, and &lt;code&gt;override&lt;/code&gt; the &lt;code&gt;OnConnectedAsync&lt;/code&gt; method - but there are a few more steps involved in plumbing everything together. Kestrel, however, has advantages such as supporting exotic socket APIs.&lt;/p&gt;&lt;p&gt;So, let's whack together a console that interacts with the server:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-en"&gt;var&lt;/span&gt; &lt;span class="pl-en"&gt;socket&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-en"&gt;SimplPipelineSocketServer&lt;/span&gt;.&lt;span class="pl-en"&gt;For&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;ReverseServer&lt;/span&gt;&amp;gt;())&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-en"&gt;socket&lt;/span&gt;.&lt;span class="pl-en"&gt;Listen&lt;/span&gt;(&lt;span class="pl-en"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;IPEndPoint&lt;/span&gt;(&lt;span class="pl-en"&gt;IPAddress&lt;/span&gt;.&lt;span class="pl-en"&gt;Loopback&lt;/span&gt;, 5000));&lt;br /&gt;    &lt;br /&gt;    &lt;span class="pl-k"&gt;string&lt;/span&gt; &lt;span class="pl-smi"&gt;line&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;while&lt;/span&gt; ((&lt;span class="pl-smi"&gt;line&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;Console&lt;/span&gt;.&lt;span class="pl-smi"&gt;In&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadLineAsync&lt;/span&gt;()) &lt;span class="pl-k"&gt;!=&lt;/span&gt; &lt;span class="pl-c1"&gt;null&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;line&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;q&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;clientCount&lt;/span&gt;, &lt;span class="pl-smi"&gt;len&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;leased&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;line&lt;/span&gt;.&lt;span class="pl-en"&gt;Encode&lt;/span&gt;())&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-smi"&gt;len&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;leased&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;;&lt;br /&gt;            &lt;span class="pl-smi"&gt;clientCount&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;socket&lt;/span&gt;.&lt;span class="pl-smi"&gt;Server&lt;/span&gt;.&lt;span class="pl-en"&gt;BroadcastAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;leased&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;);&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;Console&lt;/span&gt;.&lt;span class="pl-smi"&gt;Out&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteLineAsync&lt;/span&gt;(&lt;br /&gt;            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$"&lt;/span&gt;Broadcast {&lt;span class="pl-smi"&gt;len&lt;/span&gt;} bytes to {&lt;span class="pl-smi"&gt;clientCount&lt;/span&gt;} clients&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works much like the client, except any input other than &lt;code&gt;"q"&lt;/code&gt; is &lt;em&gt;broadcast&lt;/em&gt; to all the clients.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-now-race-your-horses" class="anchor" aria-hidden="true" href="#now-race-your-horses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Now race your horses&lt;/h2&gt;&lt;p&gt;We're not just doing this for fun! The key obective of things like pipelines and the array-pool is that it makes it &lt;strong&gt;much&lt;/strong&gt; simpler to write IO code that makes efficient use of memory; reducing allocations (and &lt;em&gt;especially&lt;/em&gt; reducing large object allocations) &lt;em&gt;signficantly&lt;/em&gt; reduces garbage collection overhead, allowing our code to be much more scalable (useful for both servers, and high-throughput client scenarios). Our use of &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; makes it &lt;strong&gt;much&lt;/strong&gt; simpler to make effective use of the CPU: instead of blocking for a while, we can make the thread available to do other &lt;em&gt;useful work&lt;/em&gt; - increasing throughput, and once again: reducing memory usage (having lots of threads is &lt;em&gt;not&lt;/em&gt; cheap - each thread has a quite significant stack space reserved for it).&lt;/p&gt;&lt;p&gt;Note that this isn't entirely free; fetching arrays from the pool (and remembering to return them) &lt;em&gt;by itself&lt;/em&gt; has some overhead - but the general expectation is that the cost of checking the pool is, &lt;em&gt;overall&lt;/em&gt;, lower than the cost associated from constant allocations and collections. Similarly, &lt;code&gt;async&lt;/code&gt;: the hope is that the increased scalability afforded by freeing up threads more-than-offsets the cost of the additional work required by the plumbing involved.&lt;/p&gt;&lt;p&gt;But: there's only one way to find out. &lt;a href="https://ericlippert.com/2012/12/17/performance-rant/" rel="nofollow"&gt;As Eric Lippert puts it&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;If you have two horses and you want to know which of the two is the faster then &lt;strong&gt;race your horses&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Setting up a good race-track for code can be awkward, because we need to try to reproduce a &lt;em&gt;meaningful scenario&lt;/em&gt;. And it is &lt;em&gt;amazingly&lt;/em&gt; easy to write bad performnce tests. Rather than reinvent bad code, it is &lt;em&gt;hugely&lt;/em&gt; adviseable to lean on tools like &lt;a href="https://benchmarkdotnet.org/" rel="nofollow"&gt;&lt;code&gt;BenchmarkDotNet&lt;/code&gt;&lt;/a&gt;. If you are &lt;em&gt;even remotely&lt;/em&gt; performance minded, and you haven't used &lt;code&gt;BenchmarkDotNet&lt;/code&gt;: sorry, but &lt;em&gt;you're doing it wrong&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;There are 4 combinations we can check here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;SimplSocketClient&lt;/code&gt; against &lt;code&gt;SimplSocketServer&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;SimplSocketClient&lt;/code&gt; against &lt;code&gt;SimplPipelineServer&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;SimplPipelineClient&lt;/code&gt; against &lt;code&gt;SimplSocketServer&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;SimplPipelineClient&lt;/code&gt; against &lt;code&gt;SimplPipelineServer&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I won't list all of these, but for these tests I'll use a &lt;code&gt;[GlobalSetup]&lt;/code&gt; method (a &lt;code&gt;BenchmarkDotNet&lt;/code&gt; concept) to spin up both servers (on different ports), then we can test clients against each. Here's our "&lt;code&gt;SimplSocketClient&lt;/code&gt; against &lt;code&gt;SimplSocketServer&lt;/code&gt;" test (remembering that &lt;code&gt;SimplSocketClient&lt;/code&gt; is synchronous):&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;Benchmark&lt;/span&gt;(&lt;span class="pl-en"&gt;OperationsPerInvoke&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Ops&lt;/span&gt;)]&lt;br /&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;long&lt;/span&gt; &lt;span class="pl-en"&gt;c1_s1&lt;/span&gt;()&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;long&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;SimplSocketClient&lt;/span&gt;(&lt;span class="pl-smi"&gt;CreateSocket&lt;/span&gt;))&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;client&lt;/span&gt;.&lt;span class="pl-en"&gt;Connect&lt;/span&gt;(&lt;span class="pl-smi"&gt;s1&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;for&lt;/span&gt; (&lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;i&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;; &lt;span class="pl-smi"&gt;i&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-smi"&gt;Ops&lt;/span&gt;; &lt;span class="pl-smi"&gt;i&lt;/span&gt;&lt;span class="pl-k"&gt;++&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;response&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt;.&lt;span class="pl-en"&gt;SendReceive&lt;/span&gt;(&lt;span class="pl-smi"&gt;_data&lt;/span&gt;);&lt;br /&gt;            &lt;span class="pl-smi"&gt;x&lt;/span&gt; &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-smi"&gt;response&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;;&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;AssertResult&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and here's our "&lt;code&gt;SimplPipelineClient&lt;/code&gt; against &lt;code&gt;SimplPipelineServer&lt;/code&gt;" test (using a &lt;code&gt;Task&lt;/code&gt; this time, as &lt;code&gt;SimplPipelineClient&lt;/code&gt; uses an &lt;code&gt;async&lt;/code&gt; API):&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;Benchmark&lt;/span&gt;(&lt;span class="pl-en"&gt;OperationsPerInvoke&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Ops&lt;/span&gt;)]&lt;br /&gt;&lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;Task&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;long&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;c2_s2&lt;/span&gt;()&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;long&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;SimplPipelineClient&lt;/span&gt;.&lt;span class="pl-en"&gt;ConnectAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;s2&lt;/span&gt;))&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;for&lt;/span&gt; (&lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;i&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;; &lt;span class="pl-smi"&gt;i&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-smi"&gt;Ops&lt;/span&gt;; &lt;span class="pl-smi"&gt;i&lt;/span&gt;&lt;span class="pl-k"&gt;++&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;response&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;br /&gt;                &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;client&lt;/span&gt;.&lt;span class="pl-en"&gt;SendReceiveAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;_data&lt;/span&gt;))&lt;br /&gt;            {&lt;br /&gt;                &lt;span class="pl-smi"&gt;x&lt;/span&gt; &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-smi"&gt;response&lt;/span&gt;.&lt;span class="pl-smi"&gt;Memory&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;;&lt;br /&gt;            }&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;AssertResult&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that we're performing multiple operations (&lt;code&gt;Ops&lt;/code&gt;) per run here, so we're not just measing overheads like connect. Other than that, we'll just let &lt;code&gt;BenchmarkDotNet&lt;/code&gt; do the hard work. We run our tests, and we get (after some time; benchmarking isn't always fast, although you can make suggestions on the iterations etc to speed it up if you want):&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Method&lt;/th&gt;&lt;th&gt;Runtime&lt;/th&gt;&lt;th align="right"&gt;Mean&lt;/th&gt;&lt;th align="right"&gt;Error&lt;/th&gt;&lt;th align="right"&gt;StdDev&lt;/th&gt;&lt;th align="right"&gt;Gen 0&lt;/th&gt;&lt;th align="right"&gt;Gen 1&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;c1_s1&lt;/td&gt;&lt;td&gt;Clr&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c1_s2&lt;/td&gt;&lt;td&gt;Clr&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c2_s1&lt;/td&gt;&lt;td&gt;Clr&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c2_s2&lt;/td&gt;&lt;td&gt;Clr&lt;/td&gt;&lt;td align="right"&gt;45.99us&lt;/td&gt;&lt;td align="right"&gt;0.4275us&lt;/td&gt;&lt;td align="right"&gt;0.2544us&lt;/td&gt;&lt;td align="right"&gt;0.3636&lt;/td&gt;&lt;td align="right"&gt;0.0909&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c1_s1&lt;/td&gt;&lt;td&gt;Core&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c1_s2&lt;/td&gt;&lt;td&gt;Core&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c2_s1&lt;/td&gt;&lt;td&gt;Core&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;NA&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;td align="right"&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;c2_s2&lt;/td&gt;&lt;td&gt;Core&lt;/td&gt;&lt;td align="right"&gt;29.87us&lt;/td&gt;&lt;td align="right"&gt;0.2294us&lt;/td&gt;&lt;td align="right"&gt;0.1518us&lt;/td&gt;&lt;td align="right"&gt;0.1250&lt;/td&gt;&lt;td align="right"&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Now, you're probaly looking at that table and thinking "huh? most of the data is missing - how can interpret that?" - and: you wouldn't be wrong! It turns out that the &lt;code&gt;c1&lt;/code&gt; (&lt;code&gt;SimplSocketClient&lt;/code&gt;) and &lt;code&gt;s1&lt;/code&gt; (&lt;code&gt;SimplSocketServer&lt;/code&gt;) implementations are &lt;em&gt;simply unreliable&lt;/em&gt;. Ultimately, it was &lt;strong&gt;painfully hard&lt;/strong&gt; to write reliable socket code before pipelines, and it looks like the legacy implementation simply has bugs and race conditions that &lt;em&gt;don't show up in casual usage&lt;/em&gt; (it works fine in the REPL client), but which manifest pretty quickly when &lt;code&gt;BenchmarkDotNet&lt;/code&gt; runs it &lt;em&gt;aggressively&lt;/em&gt;. Our "pipelines" implementation simply used the "obvious" thing, and &lt;em&gt;it works reliably first time&lt;/em&gt;. All of the complex pieces that IO authors previously had to worry about have now moved to the framework code, which enables programmers to focus on the interesting thing &lt;em&gt;that they're trying to do&lt;/em&gt; (rather than spending most of their time fighting with IO intrinsics), &lt;em&gt;and&lt;/em&gt; benefit from a reliable well-tested implementation of the ugly IO code.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;A major advantage of moving to pipelines is getting rid of the gnarly IO bugs that &lt;em&gt;you didn't even know you had&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;I will be more than happy to update this table with updated numbers if &lt;code&gt;SimplSockets&lt;/code&gt; can find the things that are stalling it.&lt;/p&gt;&lt;p&gt;Of the numbers that we &lt;em&gt;do&lt;/em&gt; have, we can see that it behaves &lt;em&gt;well&lt;/em&gt; on &lt;code&gt;Clr&lt;/code&gt; (.NET Framework) but works &lt;em&gt;much better&lt;/em&gt; on &lt;code&gt;Core&lt;/code&gt; (.NET Core). .NET Core 2.1 is frankly &lt;em&gt;amazing&lt;/em&gt; (and 3.0 looks even better) - with &lt;em&gt;lots&lt;/em&gt; of advantages. If you're serious about performance, migrating to .NET Core should definitely be on your roadmap.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;&lt;p&gt;This has been a long read, but I hope I've conveyed some useful practical advice and tips for working with pipelines in real systems, in a way that is directly translatable to your &lt;em&gt;own&lt;/em&gt; requirements. If you want to play with the code in more depth, or see it in action, you can &lt;a href="https://github.com/mgravell/simplsockets/"&gt;see my fork here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Update: please also see &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-31.html"&gt;part 3.1&lt;/a&gt; for further clarifications on this post&lt;/p&gt;&lt;p&gt;Enjoy!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/B5z4GmNruMI" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6007133552883736143" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6007133552883736143" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/B5z4GmNruMI/pipe-dreams-part-3.html" title="Pipe Dreams, part 3" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/07/pipe-dreams-part-3.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-5187891784616191510</id><published>2018-07-03T08:42:00.003-07:00</published><updated>2018-08-01T07:51:33.039-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="pipelines" /><title type="text">Pipe Dreams, part 2</title><content type="html">&lt;h1&gt;&lt;a href="#pipelines---a-guided-tour-of-the-new-io-api-in-net-part-2" aria-hidden="true" class="anchor" id="user-content-pipelines---a-guided-tour-of-the-new-io-api-in-net-part-2"&gt;&lt;/a&gt;Pipelines - a guided tour of the new IO API in .NET, part 2&lt;/h1&gt;&lt;p&gt;In &lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html" rel="nofollow"&gt;part 1&lt;/a&gt;, we discussed some of the problems that exist in the familiar &lt;code&gt;Stream&lt;/code&gt; API, and we had an introduction to the &lt;code&gt;Pipe&lt;/code&gt;, &lt;code&gt;PipeWriter&lt;/code&gt; and &lt;code&gt;PipeReader&lt;/code&gt; APIs, looking at how to write to a single &lt;code&gt;Pipe&lt;/code&gt; and then consume the data from that &lt;code&gt;Pipe&lt;/code&gt;; we also discussed how &lt;code&gt;FlushAsync()&lt;/code&gt; and &lt;code&gt;ReadAsync()&lt;/code&gt; work together to keep both sides of the machinery working, dealing with "empty" and "full" scenarios - suspending the reader when there is nothing to do, and resuming it when data arrives; and suspending the writer when it is out-pacing the reader (over-filling the pipe), and resuming when the reader has caught up; and we discussed what it &lt;em&gt;means&lt;/em&gt; to "resume" here, in terms of the threading model.&lt;/p&gt;&lt;p&gt;In this part, we're going to discuss the &lt;em&gt;memory model&lt;/em&gt; of pipelines: where does the data actually &lt;em&gt;live?&lt;/em&gt;. We'll also start looking at how we can use pipelines in realistic scenarios to fulfil real needs.&lt;/p&gt;&lt;h2&gt;&lt;a href="#the-memory-model-where-are-all-my-datas" aria-hidden="true" class="anchor" id="user-content-the-memory-model-where-are-all-my-datas"&gt;&lt;/a&gt;The memory model: where are all my datas?&lt;/h2&gt;&lt;p&gt;In part 1, we spoke about how the pipe owns all the buffers, allowing the writer to request a buffer via &lt;code&gt;GetMemory()&lt;/code&gt; and &lt;code&gt;GetSpan()&lt;/code&gt;, with the committed data later being exposed to the reader via the &lt;code&gt;.Buffer&lt;/code&gt; on &lt;code&gt;ReadAsync()&lt;/code&gt; - which is a &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt;, i.e some number of &lt;em&gt;segments&lt;/em&gt; of data.&lt;/p&gt;&lt;p&gt;So what &lt;em&gt;actually happens&lt;/em&gt;?&lt;/p&gt;&lt;p&gt;Each &lt;code&gt;Pipe&lt;/code&gt; instance has a reference to a &lt;code&gt;MemoryPool&amp;lt;byte&amp;gt;&lt;/code&gt; - a new device in &lt;a href="https://www.nuget.org/packages/System.Memory/" rel="nofollow"&gt;&lt;code&gt;System.Memory&lt;/code&gt;&lt;/a&gt; for, unsurprisingly, creating a memory pool. You can specify a specific &lt;code&gt;MemoryPool&amp;lt;byte&amp;gt;&lt;/code&gt; in the options when creating a &lt;code&gt;Pipe&lt;/code&gt;, but by default (and, I imagine, almost always) - a shared application-wide pool (&lt;code&gt;MemoryPool&amp;lt;byte&amp;gt;.Shared&lt;/code&gt;) is used.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;MemoryPool&amp;lt;byte&amp;gt;&lt;/code&gt; concept is very open-ended. The &lt;em&gt;default&lt;/em&gt; implementation simply makes use of &lt;code&gt;ArrayPool&amp;lt;byte&amp;gt;.Shared&lt;/code&gt; (the application wide array-pool), renting arrays as needed, and returning them when done. This &lt;code&gt;ArrayPool&amp;lt;T&amp;gt;&lt;/code&gt; is implemented using &lt;code&gt;WeakReference&lt;/code&gt;, so pooled arrays are collectible if memory pressure demands it. However, when you ask &lt;code&gt;GetMemory(someSize)&lt;/code&gt; or &lt;code&gt;GetSpan(someSize)&lt;/code&gt;, it doesn't simply ask the memory pool for &lt;em&gt;that amount&lt;/em&gt;; instead, it tracks a "segment" internally. A new "segment" will be (by default, configurable) the larger of &lt;code&gt;someSize&lt;/code&gt; or 2048 bytes. Requesting a non-trivial amount of memory means that we aren't filling the system with tiny arrays, which would significantly impact garbage collection. When you &lt;code&gt;Advance(bytesWritten)&lt;/code&gt; in the writer, it:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;moves an internal counter that is how much of the current segment has been used&lt;/li&gt;&lt;li&gt;updates the end of the "available to be read" chain for the reader; if we've just written the first bytes of an empty segment, this will mean adding a new segment to the chain, otherwise it'll mean increasing the end marker of the final segment of the existing chain&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It is this "available to be read" chain that we fetch in &lt;code&gt;ReadAsync()&lt;/code&gt;; and as we &lt;code&gt;AdvanceTo&lt;/code&gt; in the reader - when entire segments are consumed, the pipe hands those segments back to the memory pool. From there, they can be reused many times. And as a direct consequence of the two points above, we can see that &lt;em&gt;most of the time&lt;/em&gt;, even with multiple calls to &lt;code&gt;Advance&lt;/code&gt; in the writer, we may end up with a single segment in the reader, with multiple segments happening either at segment boundaries, or where the reader is falling behind the writer, and data is starting to accumulate.&lt;/p&gt;&lt;p&gt;What this achieves &lt;em&gt;just using the default pool&lt;/em&gt; is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we don't need to keep allocating every time we call &lt;code&gt;GetMemory()&lt;/code&gt; / &lt;code&gt;GetSpan()&lt;/code&gt;&lt;/li&gt;&lt;li&gt;we don't need a separate array per &lt;code&gt;GetMemory()&lt;/code&gt; / &lt;code&gt;GetSpan()&lt;/code&gt; - we'll often just get a different range of the same "segment"&lt;/li&gt;&lt;li&gt;a relatively small number of non-trivial buffer arrays are used&lt;/li&gt;&lt;li&gt;they are automatically recycled without needing lots of library code&lt;/li&gt;&lt;li&gt;when not being used, they are available for garbage collection&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This also explains why the approach of requesting a very small amount in &lt;code&gt;GetMemory()&lt;/code&gt; / &lt;code&gt;GetSpan()&lt;/code&gt; and then checking the size can be so successful: we have access to the &lt;em&gt;rest of the unused part of the current segment&lt;/em&gt;. Meaning: with a segment size of 2048, of which 200 bytes were already used by previous writes - even if we only ask for 5 bytes, we'll probably find we have 1848 bytes available to play with. Or possibly more - remember that obtaining an array from &lt;code&gt;ArrayPool&amp;lt;T&amp;gt;.Shared&lt;/code&gt; is &lt;em&gt;also&lt;/em&gt; an "at least this big" operation.&lt;/p&gt;&lt;h2&gt;&lt;a href="#zero-copy-buffers" aria-hidden="true" class="anchor" id="user-content-zero-copy-buffers"&gt;&lt;/a&gt;Zero copy buffers&lt;/h2&gt;&lt;p&gt;Something else to notice in this setup is that we get data buffering without &lt;em&gt;any&lt;/em&gt; data copying. The writer asked for a buffer, and wrote the data to &lt;em&gt;where it needed to be&lt;/em&gt; the first time, on the way in. This then acted as a buffer between the writer and the reader without any need to copy data around. And if the reader couldn't process all the data yet, it was able to push data back into the pipe simply by saying explicitly what it &lt;em&gt;did&lt;/em&gt; consume. There was no need to maintain a separate backlog of data for the reader, something that is &lt;em&gt;very&lt;/em&gt; common in protocol processing code using &lt;code&gt;Stream&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;It is this combination of features that makes the &lt;em&gt;memory&lt;/em&gt; aspect of pipeline code so friendly. You could &lt;em&gt;do&lt;/em&gt; all of this with &lt;code&gt;Stream&lt;/code&gt;, but it is an excruciating amount of error-prone code to do it, and even more if you want to do it &lt;em&gt;well&lt;/em&gt; - and you'd pretty much have to implement it separately for each scenario. Pipelines makes good memory handling the default simple path - &lt;a href="https://blog.codinghorror.com/falling-into-the-pit-of-success/" rel="nofollow"&gt;the pit of success&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#more-exotic-memory-pools" aria-hidden="true" class="anchor" id="user-content-more-exotic-memory-pools"&gt;&lt;/a&gt;More exotic memory pools&lt;/h2&gt;&lt;p&gt;You aren't limited to the memory model discussed; you can implement your own custom memory pool! The advantage of the default pool is that it is simple. In particular, it &lt;em&gt;doesn't really matter&lt;/em&gt; if we aren't 100% perfect about returning every segment - if we somehow drop a pipe on the floor, the worst that can happen is that the garbage collector collects the abandoned segments at some point. They won't go back into the pool, but that's fine.&lt;/p&gt;&lt;p&gt;You &lt;em&gt;can&lt;/em&gt;, however, do much more interesting things. Imagine, for example, a &lt;code&gt;MemoryPool&amp;lt;byte&amp;gt;&lt;/code&gt; that takes huge &lt;em&gt;slabs&lt;/em&gt; of memory - either managed memory via a number of very large arrays, or unmanaged memory via &lt;code&gt;Marshal.AllocHGlobal&lt;/code&gt; (note that &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt; and &lt;code&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; &lt;em&gt;are not limited to arrays&lt;/em&gt; - all they require is some kind of contiguous memory), leasing blocks of this larger chunk as required. This has great potential, but it becomes increasingly important to ensure that segments are reliably returned. Most systems shouldn't need this, but it is good that the flexibility is offered.&lt;/p&gt;&lt;h1&gt;&lt;a href="#useful-pipes-in-real-systems" aria-hidden="true" class="anchor" id="user-content-useful-pipes-in-real-systems"&gt;&lt;/a&gt;Useful pipes in real systems&lt;/h1&gt;&lt;p&gt;The example that we used in part 1 was of a single &lt;code&gt;Pipe&lt;/code&gt; that was written and read by the same code. That's clearly not a realistic scenario (unless we're trying to mock an "echo" server), so what can we do for more realistic scenarios? First, we need to connect our pipelines to something. We don't usually want a &lt;code&gt;Pipe&lt;/code&gt; in isolation; we want a pipe that &lt;em&gt;integrates with a common system or API&lt;/em&gt;. So; let's start by seeng what this would look like.&lt;/p&gt;&lt;p&gt;Here we need a bit of caveat and disclaimer: the pipelines released in .NET Core 2.1 &lt;em&gt;do not include any endpoint implementations&lt;/em&gt;. Meaning: the &lt;code&gt;Pipe&lt;/code&gt; machinery is there, but nothing is shipped &lt;em&gt;inside the box&lt;/em&gt; that actually connects pipes with any other existing systems - like shipping the abstract &lt;code&gt;Stream&lt;/code&gt; base-type, but without shipping &lt;code&gt;FileStream&lt;/code&gt;, &lt;code&gt;NetworkStream&lt;/code&gt;, etc. Yes, that sounds frustrating, but it was a pragmatic reality of time constraints. Don't panic! There are... "lively" conversations going on right now about which bindings to implement with which priority; and there are few community offerings to bridge the most obvious gaps for today.&lt;/p&gt;&lt;p&gt;Since we find ourselves in that position, we might naturally ask: "what does it take to connect pipelines to another data backend?".&lt;/p&gt;&lt;p&gt;Perhaps a good place to start would be connecting a pipe to a &lt;code&gt;Stream&lt;/code&gt;. I know what you're thinking: "Marc, but in part 1 you went out of your way to say how terrible &lt;code&gt;Stream&lt;/code&gt; is!". I haven't changed my mind; it isn't necessarily &lt;em&gt;ideal&lt;/em&gt; - for any scenario-specific &lt;code&gt;Stream&lt;/code&gt; implementation (such as &lt;code&gt;NetworkStream&lt;/code&gt; or &lt;code&gt;FileStream&lt;/code&gt;) we &lt;em&gt;could&lt;/em&gt; have a dedicated pipelines-based endpoint that talked &lt;em&gt;directly&lt;/em&gt; to that service with minimal indirection; but it is a &lt;em&gt;useful&lt;/em&gt; first step:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it gives us immediate access to a &lt;em&gt;huge&lt;/em&gt; range of API surfaces - anything that can expose data via &lt;code&gt;Stream&lt;/code&gt;, and anything that can act as a middle-layer via wrapped streams (encryption, compression, etc)&lt;/li&gt;&lt;li&gt;it hides all the wrinkly bits of the &lt;code&gt;Stream&lt;/code&gt; API behind a clear unambiguous surface&lt;/li&gt;&lt;li&gt;it gives us &lt;em&gt;almost all&lt;/em&gt; of the advantages that we have mentioned so far&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, let's get started! The first thing we need to think about is: what is the &lt;em&gt;direction&lt;/em&gt; here? As previously mentioned, a &lt;code&gt;Stream&lt;/code&gt; is ambiguous - and could be read-only, write-only, or read-write. Let's assume we want to deal with the most general case: a read-write stream that acts in a duplex manner - this will give us access to things like sockets (via &lt;code&gt;NetworkStream&lt;/code&gt;). This means we're actually going to want &lt;em&gt;two&lt;/em&gt; pipes - one for the input, one for the output. Pipelines helps clear this up for us, by declaring an interface expressly for this: &lt;code&gt;IDuplexPipe&lt;/code&gt;. This is a very simple interface, and being handed an &lt;code&gt;IDuplexPipe&lt;/code&gt; is analogous to being handed the ends of two pipes - one marked "in", one marked "out":&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;interface&lt;/span&gt; &lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-en"&gt;PipeReader&lt;/span&gt; &lt;span class="pl-smi"&gt;Input&lt;/span&gt; { &lt;span class="pl-k"&gt;get&lt;/span&gt;; }&lt;br /&gt;    &lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;Output&lt;/span&gt; { &lt;span class="pl-k"&gt;get&lt;/span&gt;; }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What we want to do, then, is create a type that &lt;em&gt;implements&lt;/em&gt; &lt;code&gt;IDuplexPipe&lt;/code&gt;, but using 2 &lt;code&gt;Pipe&lt;/code&gt; instances internally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;one &lt;code&gt;Pipe&lt;/code&gt; will be the output buffer (from the consumer's perspective), which will be filled by caller-code writing to &lt;code&gt;Output&lt;/code&gt; - and we'll have a loop that consumes this &lt;code&gt;Pipe&lt;/code&gt; and pushes the data into the underlying &lt;code&gt;Stream&lt;/code&gt; (to be written to the network, or whatever the stream does)&lt;/li&gt;&lt;li&gt;one &lt;code&gt;Pipe&lt;/code&gt; will be the input buffer (from the consumer's perspective); we'll have a loop that &lt;em&gt;reads&lt;/em&gt; data from the underlying &lt;code&gt;Stream&lt;/code&gt; (from the network, etc) and pushes it into the &lt;code&gt;Pipe&lt;/code&gt;, where it will be drained by caller-code reading from &lt;code&gt;Input&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This approach immediately solves a &lt;em&gt;wide range&lt;/em&gt; of problems that commonly affect people using &lt;code&gt;Stream&lt;/code&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we now have input/output buffers that decouple stream access from the read/write caller-code, without having to add &lt;code&gt;BufferedStream&lt;/code&gt; or similar to prevent packet fragmentation (for the writing code), and to make it very easy to continue receiving more data while we process it (for the reading code especially, so we don't have to keep pausing while we ask for more data)&lt;/li&gt;&lt;li&gt;if the caller-code is writing faster than the stream &lt;code&gt;Write&lt;/code&gt; can process, the back-pressure feature will kick in, throttling the caller-code so we don't end up with a huge buffer of unsent data&lt;/li&gt;&lt;li&gt;if the stream &lt;code&gt;Read&lt;/code&gt; is out-pacing the caller-code that is &lt;em&gt;consuming&lt;/em&gt; the data, the back-pressure will kick in here too, throttling our stream read loop so we don't end up with a huge buffer of unprocessed data&lt;/li&gt;&lt;li&gt;both the read and write implementations benefit from all the memory pool goodness that we discussed above&lt;/li&gt;&lt;li&gt;the caller-code doesn't ever need to worry about backlog of data (incomplete frames), etc - the pipe deals with it&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;&lt;a href="#so-what-might-that-look-like" aria-hidden="true" class="anchor" id="user-content-so-what-might-that-look-like"&gt;&lt;/a&gt;So what might that look like?&lt;/h1&gt;&lt;p&gt;Essentially, all we need to do, is something like:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;StreamDuplexPipe&lt;/span&gt; : &lt;span class="pl-en"&gt;IDuplexPipe&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-en"&gt;Stream&lt;/span&gt; &lt;span class="pl-smi"&gt;_stream&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-en"&gt;Pipe&lt;/span&gt; &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;, &lt;span class="pl-smi"&gt;_writePipe&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;PipeReader&lt;/span&gt; &lt;span class="pl-smi"&gt;Input&lt;/span&gt; &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Reader&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;public&lt;/span&gt; &lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;Output&lt;/span&gt; &lt;span class="pl-k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;_writePipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;;&lt;br /&gt;    &lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; ... more here&lt;/span&gt;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that we have two different pipes; the caller gets one end of each pipe - and our code will act on the &lt;em&gt;other&lt;/em&gt; end of each pipe.&lt;/p&gt;&lt;h2&gt;&lt;a href="#pumping-the-pipe" aria-hidden="true" class="anchor" id="user-content-pumping-the-pipe"&gt;&lt;/a&gt;Pumping the pipe&lt;/h2&gt;&lt;p&gt;So what does the code look like to interact with the stream? We need two methods, as disccused above. The first - and simplest - has a loop that reads data from the &lt;code&gt;_stream&lt;/code&gt; and pushes it to &lt;code&gt;_readPipe&lt;/code&gt;, to be consumed by the calling code; the core of this method could be &lt;em&gt;something like&lt;/em&gt;&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-c1"&gt;true&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; note we'll usually get *much* more than we ask for&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;.&lt;span class="pl-en"&gt;GetMemory&lt;/span&gt;(&lt;span class="pl-c1"&gt;1&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;bytes&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_stream&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Advance&lt;/span&gt;(&lt;span class="pl-smi"&gt;bytes&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;bytes&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; source EOF&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;.&lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt; &lt;span class="pl-k"&gt;||&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This loop asks the pipe for a buffer, then uses the new &lt;code&gt;netcoreapp2.1&lt;/code&gt; overload of &lt;code&gt;Stream.ReadAsync&lt;/code&gt; that accepts a &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; to populate that buffer - we'll discuss what to do if you don't have an API that takes &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; shortly. When the read is complete, it commits that-many bytes to the pipe using &lt;code&gt;Advance&lt;/code&gt;, then it invokes &lt;code&gt;FlushAsync()&lt;/code&gt; on the &lt;em&gt;pipe&lt;/em&gt; to (if needed) awaken the reader, or pause the write loop while the back-pressure eases. Note we should also check the outcome of the &lt;code&gt;Pipe&lt;/code&gt;'s &lt;code&gt;FlushAsync()&lt;/code&gt; - it could tell us that the pipe's &lt;em&gt;consumer&lt;/em&gt; has signalled that they've finished reading the data they want (&lt;code&gt;IsCompleted&lt;/code&gt;), or that the pipe itself was shut down (&lt;code&gt;IsCanceled&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;Note that in both cases, we want to ensure that we tell the pipe when this loop has exited - &lt;em&gt;however it exits&lt;/em&gt; - so that we don't end up with the calling code awaiting forever on data that will never come. Accidents happen, and sometimes the call to &lt;code&gt;_stream.ReadAsync&lt;/code&gt; (or any other method) might throw an exception, so a good way to do this is with a &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;finally&lt;/code&gt; block:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;Exception&lt;/span&gt; &lt;span class="pl-smi"&gt;error&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;null&lt;/span&gt;;&lt;br /&gt;&lt;span class="pl-k"&gt;try&lt;/span&gt;&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; our loop from the previous sample&lt;/span&gt;&lt;br /&gt;}&lt;br /&gt;&lt;span class="pl-k"&gt;catch&lt;/span&gt;(&lt;span class="pl-en"&gt;Exception&lt;/span&gt; &lt;span class="pl-smi"&gt;ex&lt;/span&gt;) { &lt;span class="pl-smi"&gt;error&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;ex&lt;/span&gt;; }&lt;br /&gt;&lt;span class="pl-k"&gt;finally&lt;/span&gt; { &lt;span class="pl-smi"&gt;_readPipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Complete&lt;/span&gt;(&lt;span class="pl-smi"&gt;error&lt;/span&gt;); }&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you prefer, you could also use two calls to &lt;code&gt;Complete&lt;/code&gt; - one at the end of the &lt;code&gt;try&lt;/code&gt; (for success) and one inside the &lt;code&gt;catch&lt;/code&gt; (for failure).&lt;/p&gt;&lt;p&gt;The second method we need is a bit more complex; we need a loop that consumes data from &lt;code&gt;_writePipe&lt;/code&gt; and pushes it to &lt;code&gt;_stream&lt;/code&gt;. The core of this could be something like:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-c1"&gt;true&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_writePipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Reader&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadAsync&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;Buffer&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsEmpty&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;) &lt;span class="pl-k"&gt;break&lt;/span&gt;;&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; write everything we got to the stream&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;foreach&lt;/span&gt; (&lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;segment&lt;/span&gt; &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_stream&lt;/span&gt;.&lt;span class="pl-en"&gt;WriteAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;segment&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;    &lt;span class="pl-smi"&gt;_writePipe&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;_stream&lt;/span&gt;.&lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();    &lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This awaits &lt;em&gt;some&lt;/em&gt; data (which could be in multiple buffers), and checks some exit conditions; as before, we can give up if &lt;code&gt;IsCanceled&lt;/code&gt;, but the next check is more subtle: we don't want to stop writing just because the &lt;em&gt;producer&lt;/em&gt; indicated that they've written everything they wanted to (&lt;code&gt;IsCompleted&lt;/code&gt;), or we might not write the last few segments of their data - we need to continue until &lt;em&gt;we've written all their data&lt;/em&gt;, so &lt;code&gt;buffer.IsEmpty&lt;/code&gt;. This is simplified in this case because we're always writing everything - we'll see a more complex example shortly. Once we have data, we write each of the non-contiguous buffers to the stream sequentially - because &lt;code&gt;Stream&lt;/code&gt; can only write one buffer at a time (again, I'm using the &lt;code&gt;netcoreapp2.1&lt;/code&gt; overload here that accepts &lt;code&gt;ReadOnlyMemory&amp;lt;byte&amp;gt;&lt;/code&gt;, but we aren't restricted to this). Once it has written the buffers, it tells the pipe that we have consumed the data, and flushes the underlying &lt;code&gt;Stream&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In "real" code we &lt;em&gt;might&lt;/em&gt; want to be a bit more aggressive about optimizing to reduce flushing the underlying stream until we know there is no more data readily available, perhaps using the &lt;code&gt;_writePipe.Reader.TryRead(...)&lt;/code&gt; method in addition to &lt;code&gt;_writePipe.Reader.ReadAsync()&lt;/code&gt; method; this method works similarly to &lt;code&gt;ReadAsync()&lt;/code&gt; but is guaranteed to always return synchronously - useful for testing "did the writer append something while I was busy?". But the above illustrates the point.&lt;/p&gt;&lt;p&gt;Additionally, like before we would want to add a &lt;code&gt;try&lt;/code&gt;/&lt;code&gt;finally&lt;/code&gt;, so that we always call &lt;code&gt;_writePipe.Reader.Complete();&lt;/code&gt; when we exit.&lt;/p&gt;&lt;p&gt;We can use the &lt;code&gt;PipeScheduler&lt;/code&gt; to start these two pumps, which will ensure that they run in the intended context, and our loops start pumping data. We'd have a &lt;em&gt;little&lt;/em&gt; more house-keeping to add (we'd probably want a mechanism to &lt;code&gt;Close()&lt;/code&gt;/&lt;code&gt;Dispose()&lt;/code&gt; the underlying stream, etc) - but as you can see, it doesn't have to be a &lt;em&gt;huge&lt;/em&gt; task to connect an &lt;code&gt;IDuplexPipe&lt;/code&gt; to a source that wasn't designed with pipelines in mind.&lt;/p&gt;&lt;h1&gt;&lt;a href="#heres-one-i-made-earlier" aria-hidden="true" class="anchor" id="user-content-heres-one-i-made-earlier"&gt;&lt;/a&gt;Here's one I made earlier...&lt;/h1&gt;&lt;p&gt;I've simplified the above a little (not too much, honest) to make it consise for discussion, but you still probably don't want to start copying/pasting chunks from here to try and get it to work. I'm not claiming they are the perfect solution for all situations, but as part of the &lt;a href="https://github.com/StackExchange/StackExchange.Redis/issues/871" rel="nofollow"&gt;2.0 work for &lt;code&gt;StackExchange.Redis&lt;/code&gt;&lt;/a&gt;, we have implemented a range of bindings for pipelines that we are making available on nuget - unimaginatively titled &lt;code&gt;Pipelines.Sockets.Unofficial&lt;/code&gt; (&lt;a href="https://www.nuget.org/packages/Pipelines.Sockets.Unofficial/" rel="nofollow"&gt;nuget&lt;/a&gt;, &lt;a href="https://github.com/mgravell/Pipelines.Sockets.Unofficial" rel="nofollow"&gt;github&lt;/a&gt;); this includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;converting a duplex &lt;code&gt;Stream&lt;/code&gt; to an &lt;code&gt;IDuplexPipe&lt;/code&gt; (like the above)&lt;/li&gt;&lt;li&gt;converting a read-only &lt;code&gt;Stream&lt;/code&gt; to a &lt;code&gt;PipeReader&lt;/code&gt;&lt;/li&gt;&lt;li&gt;converting a write-only &lt;code&gt;Stream&lt;/code&gt; to a &lt;code&gt;PipeWriter&lt;/code&gt;&lt;/li&gt;&lt;li&gt;converting an &lt;code&gt;IDuplexPipe&lt;/code&gt; to a duplex &lt;code&gt;Stream&lt;/code&gt;&lt;/li&gt;&lt;li&gt;converting a &lt;code&gt;PipeReader&lt;/code&gt; to a read-only &lt;code&gt;Stream&lt;/code&gt;&lt;/li&gt;&lt;li&gt;converting a &lt;code&gt;PipeWriter&lt;/code&gt; to a writer-only &lt;code&gt;Stream&lt;/code&gt;&lt;/li&gt;&lt;li&gt;converting a &lt;code&gt;Socket&lt;/code&gt; to an &lt;code&gt;IDuplexPipe&lt;/code&gt; directly (without going via &lt;code&gt;NetworkStream&lt;/code&gt;)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The first six are all available via static methods on &lt;code&gt;StreamConnection&lt;/code&gt;; the last is available via &lt;code&gt;SocketConnection&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;StackExchange.Redis&lt;/code&gt; is very involved in &lt;code&gt;Socket&lt;/code&gt; work, so we are very interested in how to connect pipelines to sockets; for redis connections without TLS, we can connect our &lt;code&gt;Socket&lt;/code&gt; direct to the pipeline:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;Socket&lt;/code&gt; ⇔ &lt;code&gt;SocketConnection&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For redis connections &lt;em&gt;with&lt;/em&gt; TLS (in particular: cloud redis providers), we can connect the pieces thusly:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;Socket&lt;/code&gt; ⇔ &lt;code&gt;NetworkStream&lt;/code&gt; ⇔ &lt;code&gt;SslStream&lt;/code&gt; ⇔ &lt;code&gt;StreamConnection&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both of these configurations give us a &lt;code&gt;Socket&lt;/code&gt; at one end, and an &lt;code&gt;IDuplexPipe&lt;/code&gt; at the other, and it begins to show how we can orchcestrate pipelines as part of a more complex system. Perhaps more importantly, it gives us room in the future to &lt;em&gt;change&lt;/em&gt; the implementation. As examples of future possibilities:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Tim Seaward has been working on &lt;a href="https://github.com/Drawaes/Leto" rel="nofollow"&gt;&lt;code&gt;Leto&lt;/code&gt;&lt;/a&gt;, which provides TLS capability as an &lt;code&gt;IDuplexPipe&lt;/code&gt; directly, without requiring &lt;code&gt;SslStream&lt;/code&gt; (and thus: no stream inverters)&lt;/li&gt;&lt;li&gt;between Tim Seaward, David Fowler and Ben Adams, there are a &lt;em&gt;range&lt;/em&gt; of experimental or in-progress network layers directly implementing pipelines without using managed sockets, including "libuv", "RIO" (Registered IO), and most recently, "magma" - which pushes the entire TCP stack into user code to reduce syscalls.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It'll be interesting to see how this space develops!&lt;/p&gt;&lt;h2&gt;&lt;a href="#but-my-existing-api-doesnt-talk-in-spanbyte-or-memorybyte" aria-hidden="true" class="anchor" id="user-content-but-my-existing-api-doesnt-talk-in-spanbyte-or-memorybyte"&gt;&lt;/a&gt;But my existing API doesn't talk in &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; or &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt;!&lt;/h2&gt;&lt;p&gt;When writing code to pump data from a pipe to another system (such as a &lt;code&gt;Socket&lt;/code&gt;), it is very likely you'll bump into APIs that don't take &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; or &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt;. Don't panic, all is not lost! You still have multiple ways of breaking out of that world into something more ... traditional.&lt;/p&gt;&lt;p&gt;The first trick, for when you have a &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt; or &lt;code&gt;ReadOnlyMemory&amp;lt;T&amp;gt;&lt;/code&gt;, is &lt;code&gt;MemoryMarshal.TryGetArray(...)&lt;/code&gt;. This takes in a &lt;em&gt;memory&lt;/em&gt; and attempts to get an &lt;code&gt;ArraySegment&amp;lt;T&amp;gt;&lt;/code&gt; that describes the same data in terms of a &lt;code&gt;T[]&lt;/code&gt; vector and an &lt;code&gt;int&lt;/code&gt; offset/count pair. Obviously this can only work if the memory &lt;em&gt;was based on&lt;/em&gt; a vector, which is not always the case. So this can fail &lt;em&gt;on exotic memory pools&lt;/em&gt;. Our second escape hatch is &lt;code&gt;MemoryMarshal.GetReference(...)&lt;/code&gt;. This takes in a &lt;em&gt;span&lt;/em&gt; and returns a reference (actually a "managed pointer", aka &lt;code&gt;ref T&lt;/code&gt;) to the start of the data. Once we have a &lt;code&gt;ref T&lt;/code&gt;, we can use &lt;code&gt;unsafe&lt;/code&gt; C# to get an unmanaged pointer to the data, useful for APIs that talk in such:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;Span&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;span&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; ...&lt;br /&gt;&lt;span class="pl-en"&gt;fixed&lt;/span&gt;(&lt;span class="pl-smi"&gt;byte&lt;/span&gt;&lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-smi"&gt;ptr&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt;&lt;span class="pl-smi"&gt;MemoryMarshal&lt;/span&gt;.&lt;span class="pl-en"&gt;GetReference&lt;/span&gt;(&lt;span class="pl-smi"&gt;span&lt;/span&gt;))&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; ...&lt;/span&gt;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It can still do this if the length of the span is zero, returning a reference to where the zeroth item &lt;em&gt;would have been&lt;/em&gt;, and it &lt;em&gt;even works&lt;/em&gt; for a &lt;code&gt;default&lt;/code&gt; span where there never was any backing memory. This last one requires a slight word of caution because a &lt;code&gt;ref T&lt;/code&gt; is &lt;em&gt;not usually expected to be null&lt;/em&gt;, but that's exactly what you get here. Essentially, as long as you don't ever try to dereference this kind of null reference: you'll be fine. If you use &lt;code&gt;fixed&lt;/code&gt; to convert it to an unmanaged pointer, you get back a null (zero) pointer, which &lt;em&gt;is&lt;/em&gt; more expected (and can be useful in some P/Invoke scenarios). &lt;code&gt;MemoryMarshal&lt;/code&gt; is &lt;em&gt;essentially&lt;/em&gt; synonymous with &lt;code&gt;unsafe&lt;/code&gt; code, even if the method you're calling doesn't require the &lt;code&gt;unsafe&lt;/code&gt; keyword. It is perfectly valid to use it, but if you use it incorrectly, it reserves the right to hurt you - so just be careful.&lt;/p&gt;&lt;h1&gt;&lt;a href="#what-about-the-app-code-end-of-the-pipe" aria-hidden="true" class="anchor" id="user-content-what-about-the-app-code-end-of-the-pipe"&gt;&lt;/a&gt;What about the app-code end of the pipe?&lt;/h1&gt;&lt;p&gt;OK, we've got our &lt;code&gt;IDuplexPipe&lt;/code&gt;, and we've seen how to connect the "business end" of both pipes to your backend data service of choice. Now; how do we use it in our app code?&lt;/p&gt;&lt;p&gt;As in our example from part 1, we're going to hand the &lt;code&gt;PipeWriter&lt;/code&gt; from &lt;code&gt;IDuplexPipe.Output&lt;/code&gt; to our outbound code, and the &lt;code&gt;PipeReader&lt;/code&gt; from &lt;code&gt;IDuplexPipe.Input&lt;/code&gt; to our inbound code.&lt;/p&gt;&lt;p&gt;The &lt;em&gt;outbound&lt;/em&gt; code is typically very simple, and is usually a very direct port to get from &lt;code&gt;Stream&lt;/code&gt;-based code to &lt;code&gt;PipeWriter&lt;/code&gt;-based. The key difference, once again, is that &lt;em&gt;you don't control the buffers&lt;/em&gt;. A typical implementation might look something like:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;bool&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;Write&lt;/span&gt;(&lt;span class="pl-en"&gt;SomeMessageType&lt;/span&gt; &lt;span class="pl-smi"&gt;message&lt;/span&gt;, &lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; (this may be multiple GetSpan/Advance calls, or a loop,&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; depending on what makes sense for the message/protocol)&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;span&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;GetSpan&lt;/span&gt;(...);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; TODO: ... actually write the message&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;bytesWritten&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; ... &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; from writing&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Advance&lt;/span&gt;(&lt;span class="pl-smi"&gt;bytesWritten&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;writer&lt;/span&gt;);&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;bool&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; apply back-pressure etc&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; tell the calling code whether any more messages&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; should be written&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;!&lt;/span&gt;(&lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt; &lt;span class="pl-k"&gt;||&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first part of &lt;code&gt;Write&lt;/code&gt; is our business code - we do whatever we need to write the data to the buffers from &lt;code&gt;writer&lt;/code&gt;; typically this will include multiple calls to &lt;code&gt;GetSpan(...)&lt;/code&gt; and &lt;code&gt;Advance()&lt;/code&gt;. When we've written our message, we can flush it to ensure the pump is active, and apply back-pressure. For very large messages we &lt;em&gt;could&lt;/em&gt; also flush at intermediate points, but for most simple scenarios: flushing once per message is fine.&lt;/p&gt;&lt;p&gt;If you're wondering why I split the &lt;code&gt;FlushAsync&lt;/code&gt; code into a separate method: that's because I want to &lt;code&gt;await&lt;/code&gt; the result of &lt;code&gt;FlushAsync&lt;/code&gt; to check the exit conditions, so it needs to be in an &lt;code&gt;async&lt;/code&gt; method. The most efficient way to access memory here is via the &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; API, and &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; is a &lt;code&gt;ref struct&lt;/code&gt; type; as a consequence &lt;a href="https://github.com/dotnet/csharplang/blob/master/proposals/csharp-7.2/span-safety.md" rel="nofollow"&gt;we &lt;strong&gt;cannot&lt;/strong&gt; use a &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; local variable in an &lt;code&gt;async&lt;/code&gt; method&lt;/a&gt;. A pragmatic solution is to simply split the methods, so one method deals with the &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; work, and another method deals with the &lt;code&gt;async&lt;/code&gt; aspect.&lt;/p&gt;&lt;h2&gt;&lt;a href="#random-aside-async-code-hot-synchronous-paths-and-async-machinery-overhead" aria-hidden="true" class="anchor" id="user-content-random-aside-async-code-hot-synchronous-paths-and-async-machinery-overhead"&gt;&lt;/a&gt;Random aside: async code, hot synchronous paths, and async machinery overhead&lt;/h2&gt;&lt;p&gt;The machinery involved in &lt;code&gt;async&lt;/code&gt; / &lt;code&gt;await&lt;/code&gt; is pretty good, but it can still be a surprising amount stack work - you can see this on &lt;a href="https://sharplab.io/#v2:D4AQDABCCMCsDcBYAUCkBmKAmCB5ArgE4DCA9gCYCmKA3ihAxAA6ECWAbgIYAulU0ANigAOCADVOAG3yUQAgDwAjUqUkA+CADFpAZwAWAQR0BPAHYBjABQAFVk0oB1Nr0IQA7s8qEAlPUZ1kRiCIAHoQiE4mJkljCEVOcwBrAFoWSh0dIj5KbnM/YIguVwAzXT0IAF4oAE53T0IAOm18fSMzK28kQILQ8N5JSQhuPT5zKUlWUwBzCHMKPjcR4a8I01iAW1JCPnX0nU4p9PzgsIh9UnxJcjiF515TY6CQAHYIAEJLUpa9BoBJHWInAslEklGuwGAEC++j+ANI62iOTBnWOAF8UOjUFjTjomCpimCoVshiMIJQAB6cBGglCTFymKTYCC2exOVguWjHDDiKQyOTyZr6ABK6Uu3A0gsMJgslm8lQ0VGKnDFXUxOm4hHw5m4WjKIsykm4nO6DG5ylUEH+gOBoOuFQVlCVKq5mHNgyt8MRvDtDqdhtVQA=" rel="nofollow"&gt;sharplab.io&lt;/a&gt; - take a look at the generated machinery for the &lt;code&gt;OurCode.FlushAsync&lt;/code&gt; method - and the entirety of &lt;code&gt;struct &amp;lt;FlushAsync&amp;gt;d__0&lt;/code&gt;. Now, this code is &lt;em&gt;not terrible&lt;/em&gt; - it tries hard to avoid allocations in the synchronous path - but it is &lt;em&gt;unnecessary&lt;/em&gt;. There are two ways to signficantly improve this; one is to not &lt;code&gt;await&lt;/code&gt; &lt;em&gt;at all&lt;/em&gt;, which is often possible if the &lt;code&gt;await&lt;/code&gt; is the last line in a method &lt;strong&gt;and we don't need to process the results&lt;/strong&gt;: don't &lt;code&gt;await&lt;/code&gt; - just remove the &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;return&lt;/code&gt; the task - complete or incomplete. We can't do that here, because we need to check the state of the result, but we can optimize for success by checking whether the task &lt;em&gt;is already complete&lt;/em&gt; (via &lt;code&gt;.IsCompletedSuccessfully&lt;/code&gt; - if it has completed but faulted, we still want to use the &lt;code&gt;await&lt;/code&gt; to make sure the exception behaves correctly). If it &lt;em&gt;is&lt;/em&gt; successfully completed, we're allowed to access the &lt;code&gt;.Result&lt;/code&gt;; so we could &lt;em&gt;also&lt;/em&gt; write our &lt;code&gt;FlushAsync&lt;/code&gt; method as:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;bool&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;Flush&lt;/span&gt;(&lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-smi"&gt;bool&lt;/span&gt; &lt;span class="pl-en"&gt;GetResult&lt;/span&gt;(&lt;span class="pl-en"&gt;FlushResult&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; tell the calling code whether any more messages&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; should be written&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;!&lt;/span&gt;(&lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt; &lt;span class="pl-k"&gt;||&lt;/span&gt; &lt;span class="pl-smi"&gt;flush&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;async&lt;/span&gt; &lt;span class="pl-smi"&gt;ValueTask&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;&lt;span class="pl-smi"&gt;bool&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;Awaited&lt;/span&gt;(&lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;FlushResult&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;incomplete&lt;/span&gt;)&lt;br /&gt;        &lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-en"&gt;GetResult&lt;/span&gt;(&lt;span class="pl-en"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;incomplete&lt;/span&gt;);&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; apply back-pressure etc&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;flushTask&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;flushTask&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompletedSuccessfully&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;?&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;bool&lt;/span&gt;&amp;gt;(&lt;span class="pl-en"&gt;GetResult&lt;/span&gt;(&lt;span class="pl-smi"&gt;flushTask&lt;/span&gt;.&lt;span class="pl-smi"&gt;Result&lt;/span&gt;))&lt;br /&gt;        &lt;span class="pl-k"&gt;:&lt;/span&gt; &lt;span class="pl-en"&gt;Awaited&lt;/span&gt;(&lt;span class="pl-smi"&gt;flushTask&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;em&gt;completely avoids&lt;/em&gt; the &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; machinery in the most common case: synchronous completion - as we can see again on &lt;a href="https://sharplab.io/#v2:D4AQDABCCMCsDcBYAUCkBmKAmCB5ArgE4DCA9gCYCmKA3ihAxAA6ECWAbgIYAulU0ANggA1TgBt8lEAIA8AI1KkxAPggAxCQGcAFgAoACqyaUA6m16EIAd3OVCASnqM6yRm4gKlEAOKVuAJUpNfDFuXQ18HUDg0IgAMy1tR1d3VIB6NIheMTEs7T4AY3ExVgA7AHMIAoo+K3zufMtOUoBPCABbUkI+dqDNTnKgp1S3DIgdUhDyD1rzXlLhkYYAXlUAQl0EyO0AOgBJTWJmgsoxSmngYHjE/cPSdqYz3nJ7JFQU1JAADhFxSWl5IoVBAAIJWTisZ66UQSKSyCJRIIhbiqMrVB5PSjJJZuVY+PzRZG6EAATggaPujz8WLei1GmU4TEebTknAKAGsALQsPpEPh+Ap0xhcSxbHTSCDLay2Qg7BHaEGaFqlAq6V4oIUMEAAdmu22ktzIGOp5AAyvgCidNJo4iExC1Ne4APwQUqUKy/WEAzwqXS+AJI0KbRIGwmhezYnEMABcoPBkPOwf1AnVHwgAF8UJn3igxpomIo4ud4l08vyAB6cY0oMoWUribAQQzGMwJwi0RYYT3/eGJMMo9SJRXK1X2SWqKhxTjIt7ZzTcQgW7iD7b9jtprs+iAHI4q07FvGT6ehN5uTdA7d3Y3PccQI8zrNAA=" rel="nofollow"&gt;sharplab.io&lt;/a&gt;. I should emphasize: there's absolutely no point doing this if the code is usually (or exclusively) going to &lt;em&gt;actually be asynchronous&lt;/em&gt;; it &lt;em&gt;only&lt;/em&gt; helps when the result is usually (or exclusively) going to be available synchronously.&lt;/p&gt;&lt;h2&gt;&lt;a href="#and-what-about-the-reader" aria-hidden="true" class="anchor" id="user-content-and-what-about-the-reader"&gt;&lt;/a&gt;And what about the reader?&lt;/h2&gt;&lt;p&gt;As we've seen many times, the reader is often slightly more complicated - we can't know that a single "read" operation will contain exactly one inbound message. We may need to loop until we have all the data we need, and we may have &lt;em&gt;additional&lt;/em&gt; data that we need to push back. So let's assume we want to consume a single message of some kind:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt;&amp;lt;&lt;span class="pl-en"&gt;SomeMessageType&lt;/span&gt;&amp;gt; &lt;span class="pl-en"&gt;GetNextMessage&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;PipeReader&lt;/span&gt; &lt;span class="pl-smi"&gt;reader&lt;/span&gt;,&lt;br /&gt;    &lt;span class="pl-en"&gt;CancellationToken&lt;/span&gt; &lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-c1"&gt;true&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadAsync&lt;/span&gt;(&lt;span class="pl-smi"&gt;cancellationToken&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCanceled&lt;/span&gt;) &lt;span class="pl-en"&gt;ThrowCanceled&lt;/span&gt;();&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; can we find a complete frame?&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;Buffer&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-en"&gt;TryParseFrame&lt;/span&gt;(&lt;br /&gt;            &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;,&lt;br /&gt;            &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-en"&gt;SomeMessageType&lt;/span&gt; &lt;span class="pl-smi"&gt;nextMessage&lt;/span&gt;,&lt;br /&gt;            &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-en"&gt;SequencePosition&lt;/span&gt; &lt;span class="pl-smi"&gt;consumedTo&lt;/span&gt;))&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;consumedTo&lt;/span&gt;);&lt;br /&gt;            &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-smi"&gt;nextMessage&lt;/span&gt;;&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;Start&lt;/span&gt;, &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;) &lt;span class="pl-en"&gt;ThrowEOF&lt;/span&gt;();        &lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here we obtain &lt;em&gt;some&lt;/em&gt; data from the pipe, checking exit conditions like cancelation. Next, we &lt;em&gt;try to find a message&lt;/em&gt;; what this means depends on your exact code - this could mean:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;looking through the buffer for some sentinel value such as an ASCII line-ending, then treating everything up to that point as a message (discarding the line ending)&lt;/li&gt;&lt;li&gt;parsing a well-defined binary frame header, obtaining the payload length, checking that we have that much data, and processing it&lt;/li&gt;&lt;li&gt;or anything else you want!&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If we &lt;em&gt;do&lt;/em&gt; manage to find a message, we can tell the pipe to discard the data that we've consumed - by &lt;code&gt;AdvanceTo(consumedTo)&lt;/code&gt;, which uses whatever our own frame-parsing code told us that we consumed. If we &lt;em&gt;don't&lt;/em&gt; manage to find a message, the first thing to do is tell the pipe that we consumed nothing despite trying to read everything - by &lt;code&gt;reader.AdvanceTo(buffer.Start, buffer.End)&lt;/code&gt;. At this point, there are two possibilities:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;we haven't got enough data &lt;em&gt;yet&lt;/em&gt;&lt;/li&gt;&lt;li&gt;the pipe is dead and there will &lt;em&gt;never&lt;/em&gt; be enough data&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Our check on &lt;code&gt;read.IsCompleted&lt;/code&gt; tests this, reporting failure in the latter case; otherwise we continue the loop, and await more data. What is left, then, is our frame parsing - we've reduced complex IO management down to simple operations; for example, if our messages are separated by line-feed sentinels:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;private&lt;/span&gt; &lt;span class="pl-k"&gt;static&lt;/span&gt; &lt;span class="pl-k"&gt;bool&lt;/span&gt; &lt;span class="pl-en"&gt;TryParseFrame&lt;/span&gt;(&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;,&lt;br /&gt;    &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-en"&gt;SomeMessageType&lt;/span&gt; &lt;span class="pl-smi"&gt;nextMessage&lt;/span&gt;,&lt;br /&gt;    &lt;span class="pl-k"&gt;out&lt;/span&gt; &lt;span class="pl-en"&gt;SequencePosition&lt;/span&gt; &lt;span class="pl-smi"&gt;consumedTo&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; find the end-of-line marker&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;eol&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-en"&gt;PositionOf&lt;/span&gt;((&lt;span class="pl-smi"&gt;byte&lt;/span&gt;)&lt;span class="pl-s"&gt;'&lt;span class="pl-cce"&gt;\n&lt;/span&gt;'&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;eol&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;null&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;nextMessage&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-smi"&gt;consumedTo&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;default&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; read past the line-ending&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;consumedTo&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-en"&gt;GetPosition&lt;/span&gt;(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-smi"&gt;eol&lt;/span&gt;.&lt;span class="pl-smi"&gt;Value&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; consume the data&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;var&lt;/span&gt; &lt;span class="pl-smi"&gt;payload&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-en"&gt;Slice&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;eol&lt;/span&gt;.&lt;span class="pl-smi"&gt;Value&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;nextMessage&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;ReadSomeMessageType&lt;/span&gt;(&lt;span class="pl-smi"&gt;payload&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here &lt;code&gt;PositionOf&lt;/code&gt; tries to find the first location of a line-feed. If it can't find one, we give up. Otherwise, we set &lt;code&gt;consumedTo&lt;/code&gt; to be "the line-feed plus one" (so we consume the line-feed), and we slice our buffer to create a sub-range that represents the payload &lt;em&gt;without&lt;/em&gt; the line-feed, which we can then parse (however). Finally, we report success, and can rejoice at the simplicity of parsing linux-style line-endings.&lt;/p&gt;&lt;h2&gt;&lt;a href="#whats-the-point-here" aria-hidden="true" class="anchor" id="user-content-whats-the-point-here"&gt;&lt;/a&gt;What's the point here?&lt;/h2&gt;&lt;p&gt;With minimal code that is &lt;em&gt;very similar to the most naïve and simple &lt;code&gt;Stream&lt;/code&gt; version&lt;/em&gt; (without any nice features) our app code now has a reader and writer chain that &lt;em&gt;automatically&lt;/em&gt; exploits a wide range of capabilities to ensure efficient and effective processing. Again, you &lt;em&gt;can do all these things&lt;/em&gt; with &lt;code&gt;Stream&lt;/code&gt;, but it is &lt;em&gt;really, really hard&lt;/em&gt; to do well and reliably. By pushing all theses features into the framework, multiple code-bases can benefit from a single implementation. It also gives future scope for interesting custom pipeline endpoints and decorators that work directly on the pipeline API.&lt;/p&gt;&lt;h1&gt;&lt;a href="#summary" aria-hidden="true" class="anchor" id="user-content-summary"&gt;&lt;/a&gt;Summary&lt;/h1&gt;&lt;p&gt;In this section, we looked at the memory model used by pipelines, and how it helps us avoid allocations. Then we looked at how we might integrate pipelines into existing APIs and systems such a &lt;code&gt;Stream&lt;/code&gt; - and we introduced &lt;code&gt;Pipelines.Sockets.Unofficial&lt;/code&gt; as an available utility library. We looked at the options available for integrating span/memory code with APIs that don't offer those options, and finally we looked at what the &lt;em&gt;actual calling code&lt;/em&gt; might look like when talking to pipelines (taking a brief side step into how to optimize &lt;code&gt;async&lt;/code&gt; code that is usually synchronous) - showing what our &lt;em&gt;application&lt;/em&gt; code might look like. In the third and final part, we'll look at how we combine all these learning points when looking at a real-world library such at &lt;code&gt;StackExchange.Redis&lt;/code&gt; - discussing what complications the code needed to solve, and how pipelines made it simple to do so.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/FRb1uPFQUNY" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/5187891784616191510" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/5187891784616191510" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/FRb1uPFQUNY/pipe-dreams-part-2.html" title="Pipe Dreams, part 2" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-292482151459961908</id><published>2018-07-02T08:27:00.003-07:00</published><updated>2018-07-30T04:25:16.984-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="pipelines" /><title type="text">Pipe Dreams, part 1</title><content type="html">&lt;h1&gt;&lt;a href="#pipelines---a-guided-tour-of-the-new-io-api-in-net-part-1" aria-hidden="true" class="anchor" id="user-content-pipelines---a-guided-tour-of-the-new-io-api-in-net-part-1"&gt;&lt;/a&gt;Pipelines - a guided tour of the new IO API in .NET, part 1&lt;/h1&gt;&lt;p&gt;&lt;a href="https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html"&gt;(part 2 here)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;About two years ago &lt;a href="https://blog.marcgravell.com/2016/09/channelling-my-inner-geek.html" rel="nofollow"&gt;I blogged about an upcoming experimental IO API in the .NET world&lt;/a&gt; - at the time provisionally called "Channels"; at the end of May 2018, this finally shipped - under the name &lt;a href="https://www.nuget.org/packages/System.IO.Pipelines/" rel="nofollow"&gt;&lt;code&gt;System.IO.Pipelines&lt;/code&gt;&lt;/a&gt;. I am hugely interested in the API, and over the last few weeks I'm been &lt;em&gt;consumed&lt;/em&gt; with converting &lt;code&gt;StackExchange.Redis&lt;/code&gt; to use "pipelines", &lt;a href="https://github.com/StackExchange/StackExchange.Redis/issues/871" rel="nofollow"&gt;as part of our 2.0 library update&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;My hope in this series, then, is to discuss:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;what "pipelines" &lt;em&gt;are&lt;/em&gt;&lt;/li&gt;&lt;li&gt;how to use them in terms of code&lt;/li&gt;&lt;li&gt;&lt;em&gt;when&lt;/em&gt; you might want to use them&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To help put this in concrete terms, after introducing "pipelines" I intend to draw heavily on the &lt;code&gt;StackExchange.Redis&lt;/code&gt; conversion - and in particular by discussing which problems it solves for us in each scenario. Spoiler: in virtually all cases, the answer can be summarized as:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;It perfectly fits a complex but common stumbling point in IO code; allowing us to replace an ugly kludge, workaround or compromise in &lt;em&gt;our&lt;/em&gt; code - with a purpose-designed elegant solution that is in framework code.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;I'm pretty sure that the pain points I'm going to cover below will be familiar to anyone who works at "data protocol" levels,  and I'm equally sure that the hacks and messes that we'll be replacing with pipelines will be duplicated in a lot of code-bases.&lt;/p&gt;&lt;h2&gt;&lt;a href="#what-do-pipelines-replace--complement" aria-hidden="true" class="anchor" id="user-content-what-do-pipelines-replace--complement"&gt;&lt;/a&gt;What do pipelines replace / complement?&lt;/h2&gt;&lt;p&gt;The starting point here has to be: &lt;em&gt;what is the closest analogue in existing framework code?&lt;/em&gt; And that is simple: &lt;code&gt;Stream&lt;/code&gt;. The &lt;code&gt;Stream&lt;/code&gt; API will be familiar to anyone who has worked with  serialization or data protocols. As an aside: &lt;code&gt;Stream&lt;/code&gt; is actually a very ambiguous API - it works &lt;em&gt;very&lt;/em&gt; differently in different scenarios:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;some streams are read-only, some are write-only, some are read-write&lt;/li&gt;&lt;li&gt;the same &lt;em&gt;concrete type&lt;/em&gt; can sometimes be read-only, and sometimes write-only (&lt;code&gt;DeflateStream&lt;/code&gt;, for example)&lt;/li&gt;&lt;li&gt;when a stream is read-write, sometimes it works like a &lt;a href="https://en.wikipedia.org/wiki/Compact_Cassette"&gt;cassette tape&lt;/a&gt;, where read and write are operating on the same underlying data (&lt;code&gt;FileStream&lt;/code&gt;, &lt;code&gt;MemoryStream&lt;/code&gt;); and sometimes it works like two separate streams, where read and write are essentially completely separate streams (&lt;code&gt;NetworkStream&lt;/code&gt;, &lt;code&gt;SslStream&lt;/code&gt;) - a &lt;em&gt;duplex stream&lt;/em&gt;&lt;/li&gt;&lt;li&gt;in many of the duplex cases, it is hard or impossible to express "no more data will be arriving, but you should continue to read the data to the end" - there's just &lt;code&gt;Close()&lt;/code&gt;, which usually kills both halves of the duplex&lt;/li&gt;&lt;li&gt;sometimes streams are seekable and support concepts like &lt;code&gt;Position&lt;/code&gt; and &lt;code&gt;Length&lt;/code&gt;; often they're not&lt;/li&gt;&lt;li&gt;because of the progression of APIs over time, there are often multiple ways of performing the same operation - for example, we could use &lt;code&gt;Read&lt;/code&gt; (synchronous), &lt;code&gt;BeginRead&lt;/code&gt;/&lt;code&gt;EndRead&lt;/code&gt; (asynchronous using the &lt;code&gt;IAsyncResult&lt;/code&gt; pattern), or &lt;code&gt;ReadAsync&lt;/code&gt; (asynchronous using the &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; pattern); calling code has no way &lt;em&gt;in the general case&lt;/em&gt; of knowing which of these is the "intended" (optimal) API&lt;/li&gt;&lt;li&gt;if you use either of the asynchronous APIs, it is often unclear what the threading model is; will it always actually be synchronous? if not, what thread will be calling me back? does it use sync-context? thread-pool? IO completion-port threads?&lt;/li&gt;&lt;li&gt;and more recently, there are also extensions to allow &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; / &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; to be used in place of &lt;code&gt;byte[]&lt;/code&gt; - again, the caller has no way of knowing which is the "preferred" API&lt;/li&gt;&lt;li&gt;the nature of the API &lt;em&gt;encourages&lt;/em&gt; copying data; need a buffer? that's a block-copy into another chunk of memory; need a backlog of data you haven't processed yet? block-copy into another chunk of memory; etc&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So even before we start talking about real-world &lt;code&gt;Stream&lt;/code&gt; examples and the problems that happen &lt;em&gt;when using it&lt;/em&gt;, it is clear that there are a &lt;em&gt;lot&lt;/em&gt; of problems in the &lt;code&gt;Stream&lt;/code&gt; API &lt;em&gt;itself&lt;/em&gt;. The first unsurprising news, then, is that pipelines sorts this mess out!&lt;/p&gt;&lt;h2&gt;&lt;a href="#what-are-pipelines" aria-hidden="true" class="anchor" id="user-content-what-are-pipelines"&gt;&lt;/a&gt;What are pipelines?&lt;/h2&gt;&lt;p&gt;By "pipelines", I mean a set of 4 key APIs that between them implement decoupled and overlapped reader/writer access to a binary stream (not &lt;code&gt;Stream&lt;/code&gt;), including buffer management (pooling, recycling), threading awareness, rich backlog control, and over-fill protection via back-pressure - all based around an API designed around non-contiguous memory. That's a &lt;em&gt;heck&lt;/em&gt; of a word salad - but don't worry, I'll be talking about each element to explain what I mean.&lt;/p&gt;&lt;h2&gt;&lt;a href="#starting-out-simple-writing-to-and-reading-from-a-single-pipe" aria-hidden="true" class="anchor" id="user-content-starting-out-simple-writing-to-and-reading-from-a-single-pipe"&gt;&lt;/a&gt;Starting out simple: writing to, and reading from, a single pipe&lt;/h2&gt;&lt;p&gt;Let's start with a &lt;code&gt;Stream&lt;/code&gt; analogue, and write sometthing simple to a stream, and read it back - sticking to just the &lt;code&gt;Stream&lt;/code&gt; API. We'll use ASCII text so we don't need to worry about any complex encoding concerns, and our read/write code shouldn't assume anything about the underlying stream. We'll just write the data, and then read to the end of the stream to consume it.&lt;/p&gt;&lt;p&gt;We'll do this with &lt;code&gt;Stream&lt;/code&gt; first - familiar territory. Then we'll re-implement it with pipelines, to see where the similarities and differences lie. After that, we'll investigate what is actually happening under the hood, so we understand &lt;em&gt;why&lt;/em&gt; this is interesting to us!&lt;/p&gt;&lt;p&gt;Also, before you say it: yes, I'm aware of &lt;code&gt;TextReader&lt;/code&gt;/&lt;code&gt;TextWriter&lt;/code&gt;; I'm not using them intentionally - because I'm trying to talk about the &lt;code&gt;Stream&lt;/code&gt; API here, so that the example extends to a wide range of data protocols and scenarios.&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;using&lt;/span&gt; (&lt;span class="pl-en"&gt;MemoryStream&lt;/span&gt; &lt;span class="pl-en"&gt;ms&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-en"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;MemoryStream&lt;/span&gt;())&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; write something&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-en"&gt;WriteSomeData&lt;/span&gt;(&lt;span class="pl-en"&gt;ms&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; rewind - MemoryStream works like a tape&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;ms&lt;/span&gt;.&lt;span class="pl-smi"&gt;Position&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; consume it&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-en"&gt;ReadSomeData&lt;/span&gt;(&lt;span class="pl-smi"&gt;ms&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, to write to a &lt;code&gt;Stream&lt;/code&gt; the caller needs to obtain and populate a buffer which they then pass to the &lt;code&gt;Stream&lt;/code&gt;. We'll keep it simple for now by using the synchronous API and simply allocating a &lt;code&gt;byte[]&lt;/code&gt;:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;WriteSomeData&lt;/span&gt;(&lt;span class="pl-en"&gt;Stream&lt;/span&gt; &lt;span class="pl-smi"&gt;stream&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;byte&lt;/span&gt;[] &lt;span class="pl-smi"&gt;bytes&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Encoding&lt;/span&gt;.&lt;span class="pl-smi"&gt;ASCII&lt;/span&gt;.&lt;span class="pl-en"&gt;GetBytes&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello, world!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;stream&lt;/span&gt;.&lt;span class="pl-en"&gt;Write&lt;/span&gt;(&lt;span class="pl-smi"&gt;bytes&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;bytes&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-smi"&gt;stream&lt;/span&gt;.&lt;span class="pl-en"&gt;Flush&lt;/span&gt;();&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: there are &lt;em&gt;tons&lt;/em&gt; of things in the above I could do for efficiency; but that isn't the point yet. So if you're familiar with this type of code and are twitching at the above... don't panic; we'll make it uglier - er, I mean &lt;em&gt;more efficient&lt;/em&gt; - later.&lt;/p&gt;&lt;p&gt;The &lt;em&gt;reading&lt;/em&gt; code is typically more complex than the writing code, because the reading code can't assume that it will get everything in a single call to &lt;code&gt;Read&lt;/code&gt;. A read operation on a &lt;code&gt;Stream&lt;/code&gt; can return nothing (which indicates the end of the data), or it could fill our buffer, or it could return a single byte despite being offered a huge buffer. So read code on a &lt;code&gt;Stream&lt;/code&gt; is &lt;em&gt;almost always&lt;/em&gt; a loop:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;void&lt;/span&gt; &lt;span class="pl-en"&gt;ReadSomeData&lt;/span&gt;(&lt;span class="pl-en"&gt;Stream&lt;/span&gt; &lt;span class="pl-smi"&gt;stream&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;bytesRead&lt;/span&gt;;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; note that the caller usually can't know much about&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; the size; .Length is not usually usable&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;byte&lt;/span&gt;[] &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-k"&gt;byte&lt;/span&gt;[&lt;span class="pl-c1"&gt;256&lt;/span&gt;];&lt;br /&gt;    &lt;span class="pl-k"&gt;do&lt;/span&gt;&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-smi"&gt;bytesRead&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;stream&lt;/span&gt;.&lt;span class="pl-en"&gt;Read&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;Length&lt;/span&gt;);&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;bytesRead&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;br /&gt;        {   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; note this only works for single-byte encodings&lt;/span&gt;&lt;br /&gt;            &lt;span class="pl-k"&gt;string&lt;/span&gt; &lt;span class="pl-smi"&gt;s&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Encoding&lt;/span&gt;.&lt;span class="pl-smi"&gt;ASCII&lt;/span&gt;.&lt;span class="pl-en"&gt;GetString&lt;/span&gt;(&lt;br /&gt;                &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-smi"&gt;bytesRead&lt;/span&gt;);&lt;br /&gt;            &lt;span class="pl-smi"&gt;Console&lt;/span&gt;.&lt;span class="pl-en"&gt;Write&lt;/span&gt;(&lt;span class="pl-smi"&gt;s&lt;/span&gt;);&lt;br /&gt;        }&lt;br /&gt;    } &lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-smi"&gt;bytesRead&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;);&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now let's translate that to pipelines. A &lt;code&gt;Pipe&lt;/code&gt; is broadly comparable to a &lt;code&gt;MemoryStream&lt;/code&gt;, except instead of being able to rewind it many times, the data is more simply a "first in first out" queue. We have a &lt;em&gt;writer&lt;/em&gt; API that can push data in at one end, and a &lt;em&gt;reader&lt;/em&gt; API that can pull the data out at the other. The &lt;code&gt;Pipe&lt;/code&gt; is the buffer that sits between the two. Let's reproduce our previous scenario, but using a single &lt;code&gt;Pipe&lt;/code&gt; instead of the &lt;code&gt;MemoryStream&lt;/code&gt; (again not something we'd usually do in practice, but it is simple to illustrate):&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;Pipe&lt;/span&gt; &lt;span class="pl-smi"&gt;pipe&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;new&lt;/span&gt; &lt;span class="pl-en"&gt;Pipe&lt;/span&gt;();&lt;br /&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; write something&lt;/span&gt;&lt;br /&gt;&lt;span class="pl-en"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;WriteSomeDataAsync&lt;/span&gt;(pipe.Writer);&lt;br /&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; signal that there won't be anything else written&lt;/span&gt;&lt;br /&gt;&lt;span class="pl-smi"&gt;pipe&lt;/span&gt;.&lt;span class="pl-smi"&gt;Writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Complete&lt;/span&gt;();&lt;br /&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; consume it&lt;/span&gt;&lt;br /&gt;&lt;span class="pl-en"&gt;await&lt;/span&gt; &lt;span class="pl-en"&gt;ReadSomeDataAsync&lt;/span&gt;(pipe.Reader);&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First we create a pipe using the default options, then we write to it. Note that IO operations on pipes are usually asynchronous, so we'll need to &lt;code&gt;await&lt;/code&gt; our two helper methods. Note also that we don't pass the &lt;code&gt;Pipe&lt;/code&gt; to them - unlike &lt;code&gt;Stream&lt;/code&gt;, pipelines have separate API surfaces for read and write operations, so we pass a &lt;code&gt;PipeWriter&lt;/code&gt; to the helper method that does our writing, and a &lt;code&gt;PipeReader&lt;/code&gt; to the helper method that does our reading. After writing the data, we call &lt;code&gt;Complete()&lt;/code&gt; on the &lt;code&gt;PipeWriter&lt;/code&gt;. We didn't have to do this with the &lt;code&gt;MemoryStream&lt;/code&gt; because it automatically &lt;a href="https://en.wikipedia.org/wiki/End-of-file" rel="nofollow"&gt;EOFs&lt;/a&gt; when it reaches the end of the buffered data - but on some other &lt;code&gt;Stream&lt;/code&gt; implementations - especially one-way streams - we might have had to call &lt;code&gt;Close&lt;/code&gt; after writing the data.&lt;/p&gt;&lt;p&gt;OK, so what does &lt;code&gt;WriteSomeDataAsync&lt;/code&gt; look like? Note, I've deliberately over-annotated here:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;WriteSomeDataAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;PipeWriter&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; use an oversized size guess&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;workspace&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;GetMemory&lt;/span&gt;(&lt;span class="pl-c1"&gt;20&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; write the data to the workspace&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;int&lt;/span&gt; &lt;span class="pl-smi"&gt;bytes&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Encoding&lt;/span&gt;.&lt;span class="pl-smi"&gt;ASCII&lt;/span&gt;.&lt;span class="pl-en"&gt;GetBytes&lt;/span&gt;(&lt;br /&gt;        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello, world!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;workspace&lt;/span&gt;.&lt;span class="pl-smi"&gt;Span&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; tell the pipe how much of the workspace&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; we actually want to commit&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;Advance&lt;/span&gt;(&lt;span class="pl-smi"&gt;bytes&lt;/span&gt;);&lt;br /&gt;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; this is **not** the same as Stream.Flush!&lt;/span&gt;&lt;br /&gt;    &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;writer&lt;/span&gt;.&lt;span class="pl-en"&gt;FlushAsync&lt;/span&gt;();&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first thing to note is that when dealing with pipelines: &lt;em&gt;you don't control the buffers&lt;/em&gt;: the &lt;code&gt;Pipe&lt;/code&gt; does. Recall how in our &lt;code&gt;Stream&lt;/code&gt; code, both the read and write code created a local &lt;code&gt;byte[]&lt;/code&gt;, but we don't have that here. Instead, we ask the &lt;code&gt;Pipe&lt;/code&gt; for a buffer (&lt;code&gt;workspace&lt;/code&gt;), via the &lt;code&gt;GetMemory&lt;/code&gt; method (or it's twin - &lt;code&gt;GetSpan&lt;/code&gt;). As you might expect from the name, this gives us either a &lt;code&gt;Memory&amp;lt;byte&amp;gt;&lt;/code&gt; or a &lt;code&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; - of size &lt;em&gt;at least&lt;/em&gt; twenty bytes.&lt;/p&gt;&lt;p&gt;Having obtained this buffer, we encode our &lt;code&gt;string&lt;/code&gt; into it. This means that we're writing directly into the pipe's memory, and keep track of how many bytes we &lt;em&gt;actually used&lt;/em&gt;, so we can tell it in &lt;code&gt;Advance&lt;/code&gt;. We are under no obligation to use the twenty that we asked for: we could write zero, one, twenty, or even fifty bytes. The last one may seem surprising, but it is actually actively encouraged! The emphasis previously was on "&lt;em&gt;at least&lt;/em&gt;" - the writer can actually give us a &lt;em&gt;much bigger&lt;/em&gt; buffer than we ask for. When dealing with larger data, it is common to make modest requests but expect greatness: ask for the &lt;em&gt;minumum we can usefully utilize&lt;/em&gt;, but then check the size of the memory/span that it gives us before deciding how much to actually write.&lt;/p&gt;&lt;p&gt;The call to &lt;code&gt;Advance&lt;/code&gt; is important; this completes a single write operation, making the data available in the pipe to be consumed by a reader. The call to &lt;code&gt;FlushAsync&lt;/code&gt; is &lt;em&gt;equally important&lt;/em&gt;, but much more nuanced. However, before we can adequately describe what it does, we need to take a look at the reader. So; here's our &lt;code&gt;ReadSomeDataAsync&lt;/code&gt; method:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;async&lt;/span&gt; &lt;span class="pl-en"&gt;ValueTask&lt;/span&gt; &lt;span class="pl-en"&gt;ReadSomeDataAsync&lt;/span&gt;(&lt;span class="pl-en"&gt;PipeReader&lt;/span&gt; &lt;span class="pl-smi"&gt;reader&lt;/span&gt;)&lt;br /&gt;{&lt;br /&gt;    &lt;span class="pl-k"&gt;while&lt;/span&gt; (&lt;span class="pl-c1"&gt;true&lt;/span&gt;)&lt;br /&gt;    {&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; await some data being available&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-en"&gt;ReadResult&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;await&lt;/span&gt; &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;ReadAsync&lt;/span&gt;();&lt;br /&gt;        &lt;span class="pl-en"&gt;ReadOnlySequence&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;Buffer&lt;/span&gt;;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; check whether we've reached the end&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; and processed everything&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsEmpty&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-smi"&gt;read&lt;/span&gt;.&lt;span class="pl-smi"&gt;IsCompleted&lt;/span&gt;)&lt;br /&gt;            &lt;span class="pl-k"&gt;break&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; exit loop&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; process what we received&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-k"&gt;foreach&lt;/span&gt; (&lt;span class="pl-en"&gt;Memory&lt;/span&gt;&amp;lt;&lt;span class="pl-k"&gt;byte&lt;/span&gt;&amp;gt; &lt;span class="pl-smi"&gt;segment&lt;/span&gt; &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;)&lt;br /&gt;        {&lt;br /&gt;            &lt;span class="pl-k"&gt;string&lt;/span&gt; &lt;span class="pl-smi"&gt;s&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;Encoding&lt;/span&gt;.&lt;span class="pl-smi"&gt;ASCII&lt;/span&gt;.&lt;span class="pl-en"&gt;GetString&lt;/span&gt;(&lt;br /&gt;                &lt;span class="pl-smi"&gt;segment&lt;/span&gt;.&lt;span class="pl-smi"&gt;Span&lt;/span&gt;);&lt;br /&gt;            &lt;span class="pl-smi"&gt;Console&lt;/span&gt;.&lt;span class="pl-en"&gt;Write&lt;/span&gt;(&lt;span class="pl-smi"&gt;s&lt;/span&gt;);&lt;br /&gt;        }&lt;br /&gt;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; tell the pipe that we used everything&lt;/span&gt;&lt;br /&gt;        &lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just like with the &lt;code&gt;Stream&lt;/code&gt; example, we have a loop that continues until we've reached the end of the data. With &lt;code&gt;Stream&lt;/code&gt;, that is defined as being when &lt;code&gt;Read&lt;/code&gt; returns a non-positive result, but with pipelines there are two things to check:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;read.IsCompleted&lt;/code&gt; tells us whether the write pipe has been signalled as completed and therefore no more data will be written (&lt;code&gt;pipe.Writer.Complete();&lt;/code&gt; in our earlier code did this)&lt;/li&gt;&lt;li&gt;&lt;code&gt;buffer.IsEmpty&lt;/code&gt; tells us whether there is any data left to proces &lt;em&gt;in this iteration&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If there's nothing in the pipe now &lt;em&gt;and&lt;/em&gt; the writer has been completed, then there will &lt;strong&gt;never&lt;/strong&gt; be anything in the pipe, and we can exit.&lt;/p&gt;&lt;p&gt;If we &lt;em&gt;do&lt;/em&gt; have data, then we can look at &lt;code&gt;buffer&lt;/code&gt;. So first - let's talk about &lt;code&gt;buffer&lt;/code&gt;; in the code it is a &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt;, which is a new type - this concept combines a few roles:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;describing non-contiguous memory, speficially a sequence of zero, one or many &lt;code&gt;ReadOnlyMemory&amp;lt;byte&amp;gt;&lt;/code&gt; chunks&lt;/li&gt;&lt;li&gt;describing a logical position (&lt;code&gt;SequencePosition&lt;/code&gt;) in such a data-stream - in particular via &lt;code&gt;buffer.Start&lt;/code&gt; and &lt;code&gt;buffer.End&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The &lt;em&gt;non-contiguous&lt;/em&gt; is very important here. We'll look at where the data is actually going shortly, but in terms of reading: we need to be prepared to handle data that &lt;em&gt;could&lt;/em&gt; be spread accross multiple segments. In this case, we do this by a simple &lt;code&gt;foreach&lt;/code&gt; over the &lt;code&gt;buffer&lt;/code&gt;, decoding each segment in turn. Note that even though the API is designed to be able to describe multiple non-contiguous buffers, it is frequently the case that the data received is contiguous in a single buffer; and in that case, it is often possible to write an optimized implementation for a single buffer. You can do that by checking &lt;code&gt;buffer.IsSingleSegment&lt;/code&gt; and accessing &lt;code&gt;buffer.First&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Finally, we call &lt;code&gt;AdvanceTo&lt;/code&gt;, which tells the pipe &lt;em&gt;how much data we actually used&lt;/em&gt;.&lt;/p&gt;&lt;h1&gt;&lt;a href="#key-point-you-dont-need-to-take-everything-you-are-given" aria-hidden="true" class="anchor" id="user-content-key-point-you-dont-need-to-take-everything-you-are-given"&gt;&lt;/a&gt;Key point: &lt;em&gt;you don't need to take everything you are given!&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;Contrast to &lt;code&gt;Stream&lt;/code&gt;: when you call &lt;code&gt;Read&lt;/code&gt; on a &lt;code&gt;Stream&lt;/code&gt;, it puts data into the buffer you gave it. In most real-world scenarios, it isn't always possible to consume all the data yet - maybe it only makes sense to consider "commands" as "entire text lines", and you haven't yet seen a &lt;code&gt;cr&lt;/code&gt;/&lt;code&gt;lf&lt;/code&gt; in the data. With &lt;code&gt;Stream&lt;/code&gt;: this is tough - once you've been given the data, it is your problem; if you can't use it yet, you need to store the backlog somewhere. However, with pipelines, &lt;em&gt;you can tell it&lt;/em&gt; what you've consumed. In our case, we're telling it that we consumed everything we were given, which we do by passing &lt;code&gt;buffer.End&lt;/code&gt; to &lt;code&gt;AdvanceTo&lt;/code&gt;. That means we'll never see that data again, just like with &lt;code&gt;Stream&lt;/code&gt;. However, we could also have passed &lt;code&gt;buffer.Start&lt;/code&gt;, which would mean "we didn't use anything" - and &lt;em&gt;even though we had chance to inspect the data&lt;/em&gt;, it would remain in the pipe for subsequent reads. We can also get arbitrary &lt;code&gt;SequencePosition&lt;/code&gt; values inside the buffer - if we read 20 bytes, for example - so we have full control over how much data is dropped from the pipe. There are two ways of getting a &lt;code&gt;SequencePosition&lt;/code&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;you can &lt;code&gt;Slice(...)&lt;/code&gt; a &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt; in the same way that you &lt;code&gt;Slice(...)&lt;/code&gt; a &lt;code&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; or &lt;code&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt; - and access the &lt;code&gt;.Start&lt;/code&gt; or &lt;code&gt;.End&lt;/code&gt; of the resulting sub-range&lt;/li&gt;&lt;li&gt;you can use the &lt;code&gt;.GetPosition(...)&lt;/code&gt; method of the &lt;code&gt;ReadOnlySequence&amp;lt;byte&amp;gt;&lt;/code&gt;, which returns a relative position &lt;em&gt;without&lt;/em&gt; actually slicing&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Even more subtle: we can tell it separetely that we &lt;em&gt;consumed&lt;/em&gt; some amount, but that we &lt;em&gt;inspected&lt;/em&gt; a different amount. The most common example here is to express "you can drop &lt;em&gt;this much&lt;/em&gt; - I'm done with that; but I looked at everything, I can't make any more progress at the moment - I need more data" - specifically:&lt;/p&gt;&lt;div class="highlight highlight-source-cs"&gt;&lt;pre&gt;&lt;span class="pl-smi"&gt;reader&lt;/span&gt;.&lt;span class="pl-en"&gt;AdvanceTo&lt;/span&gt;(&lt;span class="pl-smi"&gt;consumedToPosition&lt;/span&gt;, &lt;span class="pl-smi"&gt;buffer&lt;/span&gt;.&lt;span class="pl-smi"&gt;End&lt;/span&gt;);&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is where the subtle interplay of &lt;code&gt;PipeWriter.FlushAsync()&lt;/code&gt; and &lt;code&gt;PipeReader.ReadAsync()&lt;/code&gt; starts to come into play. I skipped over &lt;code&gt;FlushAsync&lt;/code&gt; earlier, but it actually serves two different functions in one call:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;if there is a &lt;code&gt;ReadAsync&lt;/code&gt; call that is outstanding because it needs data, then it &lt;em&gt;awakens&lt;/em&gt; the reader, allowing the read loop to continue&lt;/li&gt;&lt;li&gt;if the writer is out-pacing the reader, such that the pipe is filling up with data that isn't being cleared by the reader, it can &lt;em&gt;suspend&lt;/em&gt; the writer (by not completing synchronously) - to be reactivated when there is more space in the pipe (the thresholds for writer suspend/resume can be optionally specified when creating the &lt;code&gt;Pipe&lt;/code&gt; instance)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Obviously these concepts don't come into play in our example, but they are central ideas to how pipelines works. The ability to push data back into the pipe &lt;em&gt;hugely&lt;/em&gt; simplifies a vast range of IO scenarios. Virtually every piece of protocol handling code I've seen before pipelines has &lt;em&gt;masses&lt;/em&gt; of code related to handling the backlog of incomplete data - it is such a repeated piece of logic that I am &lt;em&gt;incredibly&lt;/em&gt; happy to see it handled well in a framework library instead.&lt;/p&gt;&lt;h2&gt;&lt;a href="#what-does-awaken-or-reactivate-mean-here" aria-hidden="true" class="anchor" id="user-content-what-does-awaken-or-reactivate-mean-here"&gt;&lt;/a&gt;What does "awaken" or "reactivate" mean here?&lt;/h2&gt;&lt;p&gt;You might have observed that I didn't really define what I meant here. At the &lt;em&gt;obvious&lt;/em&gt; level, I mean that: an &lt;code&gt;await&lt;/code&gt; operation of &lt;code&gt;ReadAsync&lt;/code&gt; or &lt;code&gt;FlushAsync&lt;/code&gt; had previously returned as incomplete, so now the asynchronous continuation gets invoked, allowing our &lt;code&gt;async&lt;/code&gt; method to resume execution. Yeah, OK, but that's just re-stating what &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; &lt;em&gt;mean&lt;/em&gt;. It is bug-bear of mine that I care &lt;em&gt;deeply&lt;/em&gt; (really, it is alarming how deep) about which threads code runs on - for reasons that I'll talk about later in this series. So saying "the asynchronous continuation gets invoked" &lt;em&gt;isn't enough for me&lt;/em&gt;. I want to understand &lt;em&gt;who is invoking it&lt;/em&gt;, in terms of threads. The most common answers to this are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it delegates via the &lt;code&gt;SynchronizationContext&lt;/code&gt; (note: many systems &lt;em&gt;do not have&lt;/em&gt; a &lt;code&gt;SynchronizationContext&lt;/code&gt;)&lt;/li&gt;&lt;li&gt;the thread that &lt;em&gt;triggered the state change&lt;/em&gt; gets used, at the point of the state change, to invoke the continuation&lt;/li&gt;&lt;li&gt;the global thread-pool is used to invoke the continuation&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All of these can be fine in some cases, and all of these can be terrible in some cases! Sync-context is a well-established mechanism for getting from worker threads back to primary application threads (epecially: the UI thread in desktop applications). However, it isn't necessarily the case that just because we've finished one IO operation, we're ready to jump back to an application thread; and doing so can effectively push a lot of IO code and data processing code &lt;em&gt;onto&lt;/em&gt; an application thread - usually the one thing we explicitly want to avoid. Additionally, it can be prone to deadlocks if the application code has used &lt;code&gt;Wait()&lt;/code&gt; or &lt;code&gt;.Result&lt;/code&gt; on an asynchronous call (which, to be fair, you're not meant to do). The second option (performing the callback "inline" on the thread that triggered it) can be problematic because it can steal a thread that you expected to be doing something else (and can lead to deadlocks as a consequence); and in some extreme cases it can lead to a stack-dive (and eventually a stack-overflow) when two asynchronous methods are essentially functioning as co-routines. The final option (global thread-pool) is immune to the problems of the other two - but can run into severe problems under some load conditions - something again that I'll discuss in a later part in this series.&lt;/p&gt;&lt;p&gt;However, the good news is that &lt;em&gt;pipelines gives you control here&lt;/em&gt;. When creating the &lt;code&gt;Pipe&lt;/code&gt; instance, we can supply &lt;code&gt;PipeScheduler&lt;/code&gt; instances to use for the reader and writer (separately). The &lt;code&gt;PipeScheduler&lt;/code&gt; is used to perform these activations. If not specified, then it defaults &lt;em&gt;first&lt;/em&gt; to checking for &lt;code&gt;SynchronizationContext&lt;/code&gt;, then using the global thread-pool, with "inline" continuations (i.e. intionally using the thread that caused the state change) as another option readily available. But: &lt;em&gt;you can provide your own implementation&lt;/em&gt; of a &lt;code&gt;PipeScheduler&lt;/code&gt;, giving you full control of the threading model.&lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" aria-hidden="true" class="anchor" id="user-content-summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;&lt;p&gt;So: we've looked at what a &lt;code&gt;Pipe&lt;/code&gt; is when considered individually, and how we can write to a pipe with a &lt;code&gt;PipeWriter&lt;/code&gt;, and read from a pipe with a &lt;code&gt;PipeReader&lt;/code&gt; - and how to "advance" both reader and writer. We've looked at the similarity and differences with &lt;code&gt;Stream&lt;/code&gt;, and we've discussed how &lt;code&gt;ReadAsync()&lt;/code&gt; and &lt;code&gt;FlushAsync()&lt;/code&gt; can interact to control how the writer and reader pieces execute. We looked at how responsibility for buffers is reversed, with the pipe providing all buffers - and how the pipe can simplify backlog management. Finally, we discussed the threading model that is active for continuations in the &lt;code&gt;await&lt;/code&gt; operations.&lt;/p&gt;&lt;p&gt;That's probably enough for step 1; next, we'll look at how the &lt;em&gt;memory model&lt;/em&gt; for pipelines works - i.e. where does the data live. We'll also look at &lt;em&gt;how we can use pipelines in real scenarios&lt;/em&gt; to start doing interesting things.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/Kt1vbeysSCw" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/292482151459961908" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/292482151459961908" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/Kt1vbeysSCw/pipe-dreams-part-1.html" title="Pipe Dreams, part 1" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/07/pipe-dreams-part-1.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-6368326545347137725</id><published>2018-04-12T06:42:00.001-07:00</published><updated>2018-04-17T01:10:55.914-07:00</updated><title type="text">Having a serious conversation about open source</title><content type="html">&lt;h2 id="havingaseriousconversationaboutopensource"&gt;Having a serious conversation about open source&lt;/h2&gt; &lt;p&gt;A certain topic has been surfacing a lot lately on places like twitter; one that I've quietly tried to ignore, but which has been gnawing away at me slowly. It is seemingly the most taboo, dirty and avoided topics.&lt;/p&gt; &lt;p&gt;Open source and money&lt;/p&gt; &lt;p&gt;See, I said it was taboo and dirty&lt;/p&gt; &lt;p&gt;Talking openly about money is always hard, but when you combine money and open source, it very quickly devolves into a metaphorical warzone, complete with entrenched camps, propaganda, etc.&lt;/p&gt; &lt;p&gt;This is a complex area, and if I mis-speak I hope that you'll afford me some generosity in interpreting my words as meant constructively and positively. This is largely a bit of a brain-dump; I don't intend it to be too ranty or preachy, but I'm probably not the best judge of that.&lt;/p&gt; &lt;p&gt;I absolutely love open source and the open source community. I love sharing ideas, being challenged by requirements and perspectives outside of my own horizon, benefitting from the contributions and wisdom of like-minded folks etc. I love that packages I've created (usually originally because I needed to solve a problem that vexed me) have been downloaded and used to help people tens of millions of times - that's a greate feeling! I love that I have benefitted indirectly from community recognition (including things like an MVP award), and professionally (I doubt I'd have got my job at Stack Overflow if the team hadn't been using protobuf-net from a very early date).&lt;/p&gt; &lt;p&gt;But: the consumers of open source (and I &lt;strong&gt;very much&lt;/strong&gt; include myself in this) have become... for want of a better word: entitled. We've essentially reinforced that software is free (edit: I mean in the "beer" sense, not in the "Stallman" sense). That our efforts - as an open source community: have no value beyond the occasional pat on a back. Perhaps worse, it undermines the efforts of those developers trying to earn an honest buck by offering professional products in the same area... or maybe it just forces them to offer very clear advantages and extra features, which perhaps is a good thing for them? Or is that just me trying to suppress a sense of guilt at cutting off someone else's customer base?&lt;/p&gt; &lt;p&gt;Yes it is true that some open source projects get great community backing from companies benefitting from the technology, but that seems to be the minority. Most open source projects... just don't.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;maybe this is because the project isn't popular&lt;/li&gt; &lt;li&gt;maybe it is popular, but not in a way that helps "enterprise" customers (the people most likely to pay)&lt;/li&gt; &lt;li&gt;maybe the project team simply haven't made a pay option available, which could be lack of confidence, or lack or know-how, or legal issues - or it could be the &lt;em&gt;expectation&lt;/em&gt; that the software is completely free&lt;/li&gt; &lt;li&gt;maybe people like it, but not enough to pay anything towards it&lt;/li&gt; &lt;li&gt;maybe anything other than the "free to use and open licensing" is massively disruptive to the dominant tool-chain for accessing libraries in a particular ecosytem (npm, nuget, etc)&lt;/li&gt; &lt;li&gt;maybe with multiple contributors of different-sized efforts, it would become massively confusing as to who &lt;em&gt;receives&lt;/em&gt; what money, if any was made&lt;/li&gt;&lt;li&gt;(added) does your daytime employment contract &lt;em&gt;prohibit&lt;/em&gt; taking payment for additional work&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;But whatever the reason; most open source libraries don't get financially supported. Sometimes this might not be a problem; maybe a library is sponsored internally by a company that has then &lt;em&gt;made&lt;/em&gt; that software available for other people to benefit from. But: the moment a library hits the public, it has to deal with all the scenarios and problems and edge cases that the originating company &lt;em&gt;didn't&lt;/em&gt; need to worry about. For example, you'd be &lt;em&gt;amazed&lt;/em&gt; at how much trouble SQLite causes dapper due to the data types, or how much complexity things like "sentinel", "cluster" and "modules" make for redis clients. But: the originating company (Stack Overflow in the case of dapper, etc) &lt;em&gt;doesn't use&lt;/em&gt; those features, so they don't get fixed on company time. This is a recurring theme in many such projects - and now you're in an even more complex place where the people maintaining and advancing something are doing a lot of that work on their own time, but it is now &lt;em&gt;even more awkward&lt;/em&gt; to ask the simple question: "this thing that I'm doing to benefit real users: am I getting paid for this? can I even accept contributions other than PRs?".&lt;/p&gt; &lt;h2 id="isthisaproblem"&gt;Is this a problem?&lt;/h2&gt; &lt;p&gt;Perhaps, perhaps not. I'm certainly not bitter; I love working on this stuff - I do it for hobby and pleasure reasons too (I love solving challenging problems), and it has &lt;em&gt;hugely&lt;/em&gt; advanced my knowledge, but I have to be honest and admit that there's a peice of me that thinks an opportunity has been missed. Take protobuf-net: if I sat down and added up the hours that I've spent on that, it would be horrifying. And I know people are succeding with it, and using it for commercial gain - I get the support emails from people using it in incredibly interesting ways.&lt;/p&gt; &lt;p&gt;Quite a while back I tried adding small and discreet contribution options for protobuf-net (think: "buy me a beer"). It wasn't entirely unsuccessful: to date I've received a little over (edit: incorrectly stated USD) GBP 100 in direct contributions; most of that was in one very much appreciated donation - that I can't find the details of because "pledgie" closed down. But overall, almost all of the work done has been completely for free. Again, I don't &lt;em&gt;resent this&lt;/em&gt;, but it feels that there's a huge imbalance in terms of who is doing the work, versus who is &lt;em&gt;benefiting&lt;/em&gt; from the work. There is very little motivation for companies benefiting from open source to contribute back to it - even when they're using it in commercial ways that are helping them create profits from successful products or services.&lt;/p&gt; &lt;p&gt;In my view, this is just as bad for the &lt;em&gt;consuming company&lt;/em&gt; as it is for the author: if the developer isn't motivated to improve and maintain a library that you depend on for your successful product, then: that sounds a lot like a supply-chain risk. But then, I guess you can just move onto the next competing  free tool if one author burns out.&lt;/p&gt; &lt;p&gt;I'm not sure I have a &lt;em&gt;solution&lt;/em&gt; here, but I &lt;em&gt;do&lt;/em&gt; think there's a very real conversation that we shouldn't be afraid of having, about how we - as an industry - avoid open source being treated simply as free contractors.&lt;/p&gt; &lt;h2 id="imtoyingwithideas"&gt;I'm toying with ideas&lt;/h2&gt; &lt;p&gt;For protobuf-net, I'm aware that a good number of my users are doing things like games, which tend to run on limited runtimes that don't tend to have runtime-emit support (they are "AOT" runtimes). This really hurts the performance of reflection-based libraries. I've been toying &lt;em&gt;for ages&lt;/em&gt; with new ideas to make protobuf-net work much better on those platforms by having much richer build tooling that does all of the emit code up-front as part of the build, and &lt;em&gt;one of the ideas I'm playing with&lt;/em&gt; is to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;keep the core protobuf-net runtime library "as is" (and continue to make it available for free)&lt;/li&gt; &lt;li&gt;add an additional separate package that adds the AOT features&lt;/li&gt; &lt;li&gt;but make this package dual licencesed: GPL or purchase non-GPL&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;But I'm very very unsure about this. Philosophically, I kinda hate the GPL. I just do. But the stickiness of the GPL might be the thing that actually gets &lt;em&gt;some&lt;/em&gt; customers - the ones who care about compliance - to pay a little for it. It doesn't have to be much; just enough to make it feel justified to spend such a vast amount of time developing these complex features. As for the people that don't care about compliance: they weren't going to pay anything &lt;em&gt;anyway&lt;/em&gt;, so frankly it isn't worth worrying about what they do.&lt;/p&gt; &lt;p&gt;Is this a terrible idea? Is this just me exploiting the fact that I know some users have AOT requirements? Am I just being greedy in my middle-age? Is this just going to make a nightmare of accounting and legal problems? Am I just being grumpy? Should I just accept that open source is free work? I genuinely don't know, and I haven't made my mind up on what to do. I'm genuinely interested in what people think though; &lt;strike&gt;comments should be open below&lt;/strike&gt; (edit: comments &lt;em&gt;were&lt;/em&gt; open, but... blog spam, so much blog span; maybe &lt;a href="https://twitter.com/marcgravell"&gt;tweet me @marcgravell&lt;/a&gt;).&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/enhTY5E9Oxc" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6368326545347137725" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6368326545347137725" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/enhTY5E9Oxc/having-serious-conversation-about-open.html" title="Having a serious conversation about open source" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/04/having-serious-conversation-about-open.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-935361126186292502</id><published>2018-01-30T05:07:00.001-08:00</published><updated>2018-01-31T02:24:18.187-08:00</updated><title type="text">Sorting myself out, extreme edition</title><content type="html">&lt;h3 id="sorting-myself-out-extreme-edition"&gt;...where I go silly with optimization&lt;/h3&gt; &lt;p&gt;Yes, I’m still talking about sorting... ish. I love going &lt;em&gt;deep&lt;/em&gt; on problems - it isn’t enough to just have &lt;em&gt;a solution&lt;/em&gt; - I like knowing that I have the &lt;em&gt;best solution that I am capable of&lt;/em&gt;. This isn’t always possible in a business setting - deadlines etc - but this is my own time.&lt;/p&gt; &lt;p&gt;&lt;a href="http://blog.marcgravell.com/2018/01/a-sort-of-problem.html"&gt;In part 1&lt;/a&gt;, we introduced the scenario - and discussed how to build a composite unsigned value that we could use to sort multiple properties as a single key.&lt;/p&gt; &lt;p&gt;&lt;a href="http://blog.marcgravell.com/2018/01/more-of-sort-of-problem.html"&gt;In part 2&lt;/a&gt;, we looked a little at radix sort, and found that it was a very compelling candidate.&lt;/p&gt; &lt;p&gt;In this episode, we’re going to look at some ways to signficantly improve what we’ve done so far. In particular, we’ll look at:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;using knowledge of how signed data works to avoid having to transform between them&lt;/li&gt;  &lt;li&gt;performing operations in blocks rather than per value to reduce calls&lt;/li&gt;  &lt;li&gt;using &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; as a replacemment for &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code and unmanaged pointers, allowing you to get very high performance even in 100% managed/safe code&lt;/li&gt;  &lt;li&gt;investigating branch removal as a performance optimization of critical loops&lt;/li&gt;  &lt;li&gt;vectorizing critical loops to do the same work with significantly fewer CPU operations&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Hopefully, as a follow up after this one, I’ll look at pratical guidance on &lt;em&gt;parallelizing&lt;/em&gt; this same work to spread the load over available cores.&lt;/p&gt; &lt;p&gt;Key point: the main purpose of these words is &lt;strong&gt;not&lt;/strong&gt; to discuss how to implement a radix sort - in fact, we don’t even do that. Instead, it uses &lt;em&gt;one small part&lt;/em&gt; of radix sort as an example problem with which to discuss &lt;strong&gt;much broader&lt;/strong&gt; concepts of performance optimization in C# / .NET.&lt;/p&gt; &lt;p&gt;Obviously I can’t cover the entire of radix sort for these, so I’m going to focus on one simple part: composing the radix for sorting. To recall, a naive implementation of radix sort requires unsigned keys, so that the data is naturally sortable in their binary representation. Signed integers and floating point numbers don’t follow this layout, so in part 1 we introduced some basic tools to change between them:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;uint Sortable(int value)&lt;br /&gt;{&lt;br /&gt;    // re-base eveything upwards, so anything&lt;br /&gt;    // that was the min-value is now 0, etc&lt;br /&gt;    var val = unchecked((uint)(value - int.MinValue));&lt;br /&gt;    return val;&lt;br /&gt;}&lt;br /&gt;unsafe uint Sortable (float value)&lt;br /&gt;{&lt;br /&gt;    const int MSB = 1 &amp;lt;&amp;lt; 31;&lt;br /&gt;    int raw = *(int*)(&amp;amp;value);&lt;br /&gt;    if ((raw &amp;amp; MSB) != 0) // IEEE first bit is the sign bit&lt;br /&gt;    {&lt;br /&gt;        // is negative; shoult interpret as -(the value without the MSB) - not the same as just&lt;br /&gt;        // dropping the bit, since integer math is twos-complement&lt;br /&gt;        raw = -(raw &amp;amp; ~MSB);&lt;br /&gt;    }&lt;br /&gt;    return Sortable(raw);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;These two simple transformation - applied to our target values - will form the central theme of this entry.&lt;/p&gt; &lt;p&gt;To measure performance, I’ll be using the inimitable &lt;a href="https://www.nuget.org/packages/BenchmarkDotNet/"&gt;BenchmarkDotNet&lt;/a&gt;, looking at multiple iterations of transforming 2 million random &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; values taken from a seeded &lt;code class="highlighter-rouge"&gt;Random()&lt;/code&gt;, with varying signs etc. The method above will be our baseline, and at each step we’ll add a new row to our table at the bottom:&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;SortablePerValue&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This gives us a good starting point.&lt;/p&gt; &lt;h2 id="a-negative-sign-of-the-times"&gt;A negative sign of the times&lt;a class="anchorjs-link " href="#a-negative-sign-of-the-times" aria-label="Anchor link for: a negative sign of the times" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;What’s faster than performing a fast operation? &lt;em&gt;Not&lt;/em&gt; performing a fast operation. The way radix sort works is by looking at the sort values &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; bits at a time (commonly 4, 8, 10, but any number is valid) and for that block of bits: counting how many candidates are in each of the &lt;code class="highlighter-rouge"&gt;1 &amp;lt;&amp;lt; r&lt;/code&gt; possible buckets. So if &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; is 3, we have 8 possible buckets. From that it computes target offsets for each group: if there are 27 values in bucket 0, 12 in bucket 1, 3 in bucket 2, etc - then &lt;em&gt;when sorted&lt;/em&gt; bucket 0 will start at offset 0, bucket 1 at offset 27, bucket 2 at offset 39, bucket 3 at offset 41, and so on - just by accumulating the counts. But this breaks if we have signed numbers.&lt;/p&gt; &lt;p&gt;Why?&lt;/p&gt; &lt;p&gt;First, let’s remind ourselves of the various ways that signed and unsigned data can be represented in binary, using a 4 bit number system and integer representations:&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th style="text-align: center"&gt;Binary&lt;/th&gt;      &lt;th style="text-align: center"&gt;Unsigned&lt;/th&gt;      &lt;th style="text-align: center"&gt;2s-complement&lt;/th&gt;      &lt;th style="text-align: center"&gt;1s-complement&lt;/th&gt;      &lt;th style="text-align: center"&gt;Sign bit&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0000&lt;/td&gt;      &lt;td style="text-align: center"&gt;0&lt;/td&gt;      &lt;td style="text-align: center"&gt;0&lt;/td&gt;      &lt;td style="text-align: center"&gt;+0&lt;/td&gt;      &lt;td style="text-align: center"&gt;+0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0001&lt;/td&gt;      &lt;td style="text-align: center"&gt;1&lt;/td&gt;      &lt;td style="text-align: center"&gt;1&lt;/td&gt;      &lt;td style="text-align: center"&gt;1&lt;/td&gt;      &lt;td style="text-align: center"&gt;1&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0010&lt;/td&gt;      &lt;td style="text-align: center"&gt;2&lt;/td&gt;      &lt;td style="text-align: center"&gt;2&lt;/td&gt;      &lt;td style="text-align: center"&gt;2&lt;/td&gt;      &lt;td style="text-align: center"&gt;2&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0011&lt;/td&gt;      &lt;td style="text-align: center"&gt;3&lt;/td&gt;      &lt;td style="text-align: center"&gt;3&lt;/td&gt;      &lt;td style="text-align: center"&gt;3&lt;/td&gt;      &lt;td style="text-align: center"&gt;3&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0100&lt;/td&gt;      &lt;td style="text-align: center"&gt;4&lt;/td&gt;      &lt;td style="text-align: center"&gt;4&lt;/td&gt;      &lt;td style="text-align: center"&gt;4&lt;/td&gt;      &lt;td style="text-align: center"&gt;4&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0101&lt;/td&gt;      &lt;td style="text-align: center"&gt;5&lt;/td&gt;      &lt;td style="text-align: center"&gt;5&lt;/td&gt;      &lt;td style="text-align: center"&gt;5&lt;/td&gt;      &lt;td style="text-align: center"&gt;5&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0110&lt;/td&gt;      &lt;td style="text-align: center"&gt;6&lt;/td&gt;      &lt;td style="text-align: center"&gt;6&lt;/td&gt;      &lt;td style="text-align: center"&gt;6&lt;/td&gt;      &lt;td style="text-align: center"&gt;6&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;0111&lt;/td&gt;      &lt;td style="text-align: center"&gt;7&lt;/td&gt;      &lt;td style="text-align: center"&gt;7&lt;/td&gt;      &lt;td style="text-align: center"&gt;7&lt;/td&gt;      &lt;td style="text-align: center"&gt;7&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1000&lt;/td&gt;      &lt;td style="text-align: center"&gt;8&lt;/td&gt;      &lt;td style="text-align: center"&gt;-8&lt;/td&gt;      &lt;td style="text-align: center"&gt;-7&lt;/td&gt;      &lt;td style="text-align: center"&gt;-0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1001&lt;/td&gt;      &lt;td style="text-align: center"&gt;9&lt;/td&gt;      &lt;td style="text-align: center"&gt;-7&lt;/td&gt;      &lt;td style="text-align: center"&gt;-6&lt;/td&gt;      &lt;td style="text-align: center"&gt;-1&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1010&lt;/td&gt;      &lt;td style="text-align: center"&gt;10&lt;/td&gt;      &lt;td style="text-align: center"&gt;-6&lt;/td&gt;      &lt;td style="text-align: center"&gt;-5&lt;/td&gt;      &lt;td style="text-align: center"&gt;-2&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1011&lt;/td&gt;      &lt;td style="text-align: center"&gt;11&lt;/td&gt;      &lt;td style="text-align: center"&gt;-5&lt;/td&gt;      &lt;td style="text-align: center"&gt;-4&lt;/td&gt;      &lt;td style="text-align: center"&gt;-3&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1100&lt;/td&gt;      &lt;td style="text-align: center"&gt;12&lt;/td&gt;      &lt;td style="text-align: center"&gt;-4&lt;/td&gt;      &lt;td style="text-align: center"&gt;-3&lt;/td&gt;      &lt;td style="text-align: center"&gt;-4&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1101&lt;/td&gt;      &lt;td style="text-align: center"&gt;13&lt;/td&gt;      &lt;td style="text-align: center"&gt;-3&lt;/td&gt;      &lt;td style="text-align: center"&gt;-2&lt;/td&gt;      &lt;td style="text-align: center"&gt;-5&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1110&lt;/td&gt;      &lt;td style="text-align: center"&gt;14&lt;/td&gt;      &lt;td style="text-align: center"&gt;-2&lt;/td&gt;      &lt;td style="text-align: center"&gt;-1&lt;/td&gt;      &lt;td style="text-align: center"&gt;-6&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td style="text-align: center"&gt;1111&lt;/td&gt;      &lt;td style="text-align: center"&gt;15&lt;/td&gt;      &lt;td style="text-align: center"&gt;-1&lt;/td&gt;      &lt;td style="text-align: center"&gt;-0&lt;/td&gt;      &lt;td style="text-align: center"&gt;-7&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We’re usually most familiar with unsigned and 2s-complement representations, because that is what most modern processors use to represent integers. 1s-complement is where &lt;code class="highlighter-rouge"&gt;-x ≡ ~x&lt;/code&gt; - i.e. to negate something we simply invert &lt;em&gt;all the bits&lt;/em&gt;. This works fine but has two zeros, which is one more than we usually need - hence we usually use 2s-complement which simply adds an off-by-one step; this makes zero unambiguous (very useful for &lt;code class="highlighter-rouge"&gt;false&lt;/code&gt; as we’ll see later) and (perhaps less important) gives us an extra negative value to play with.&lt;/p&gt; &lt;p&gt;The final option is to use a sign bit; to negate a number we flip the most significant bit, so &lt;code class="highlighter-rouge"&gt;-x ≡ x ^ 0b1000&lt;/code&gt;. IEEE754 floating point numbers (&lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;double&lt;/code&gt;) are implemented using a sign bit, which is why floating point numbers have +0 and -0. Due to clever construction, the rest of the value can be treated as naturally/bitwise sortable - even without needing to understand about the “mantissa”, “exponent” and “bias”. This means that to convert a &lt;strong&gt;negative&lt;/strong&gt; &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; (or any other sign-bit number) to a 1s-complement representation, we simply flip all the bits except the most significant bit. Or we flip &lt;em&gt;all&lt;/em&gt; the bits and put the most significant bit back again, since we know it should be a &lt;code class="highlighter-rouge"&gt;1&lt;/code&gt;.&lt;/p&gt; &lt;hr&gt; &lt;p&gt;So: armed with this knowledge, we can see that signed data in 1s-complement or 2s-complement is &lt;em&gt;almost&lt;/em&gt; “naturally sortable” in binary, but simply: the negative values are sorted in increasing numerical value, but come &lt;em&gt;after&lt;/em&gt; the positive values (we can happily assert that -0 &amp;lt; +0). This means that we can educate radix sort about 1s-complement and 2s-complement signed data &lt;em&gt;simply by being clever when processing the &lt;strong&gt;final left-most chunk&lt;/strong&gt;&lt;/em&gt;: based on &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; and the bit-width, calculate which bit is the most-signficant bit (which indicates sign), and simply &lt;em&gt;process the negative buckets first&lt;/em&gt; (still in the same order) when calculating the offsets; then calculate the offsets of the non-negative buckets. If we were using the 4-bit system above and &lt;code class="highlighter-rouge"&gt;r=4&lt;/code&gt;, we would have 16 buckets, and would calculate offsets in the order (of unsigned buckets) 8-15 then 0-7.&lt;/p&gt; &lt;p&gt;By doing this, we can &lt;strong&gt;completely remove&lt;/strong&gt; any need to do any pre-processing when dealing with values like &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt;. We could perhaps wish that the IEEE754 committee had preferred 1s-complement so we could skip all of this for &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; too, but a: I think it is fair to assume that there are good technical reasons for the choice (presumably relating to fast negation, and fast access to the mantissa/exponent), and b: it is moot: IEEE754 is implemented in CPU architectures and is here to stay.&lt;/p&gt; &lt;p&gt;So we’re still left with an issue for &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt;: we can’t use the same trick for values with a sign bit, because the sign bit changes the order &lt;em&gt;throughout&lt;/em&gt; the data - making grouping impossible. We can make our lives &lt;em&gt;easier&lt;/em&gt; though: since the algorithm can now cope with 1s-complement and 2s-complement data, we can switch to 1s-complement rather than to fully unsigned, which as discussed above: is pretty easy:&lt;/p&gt; &lt;p&gt;(Aside: actually, there &lt;em&gt;is&lt;/em&gt; a related trick we can do to avoid having to pre-process floating point data, but: it would make this entire blog redundant! So for the purposes of a problem to investigate, we’re going to assume that we need to do this transformation.)&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;unsafe int ToRadix (float value)&lt;br /&gt;{&lt;br /&gt;    const int MSB = 1 &amp;lt;&amp;lt; 31;&lt;br /&gt;    int raw = *(int*)(&amp;amp;value);&lt;br /&gt;    // if sign bit set: flip all bits except the MSB&lt;br /&gt;    return (raw &amp;amp; MSB) == 0 ? raw : ~raw | MSB;&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;A nice side-effect of this is that it is self-reversing: we can apply the exact same bit operations to convert from 1s-complement back to a sign bit.&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;SortablePerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;ToRadixPerValue&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,120.5 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.97&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We’ve made a slight but measurable improvment - nothing drastic, but the code is nicer.&lt;/p&gt; &lt;h2 id="blocking-ourselves-out"&gt;Blocking ourselves out&lt;a class="anchorjs-link " href="#blocking-ourselves-out" aria-label="Anchor link for: blocking ourselves out" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;We have a large chunk of data, and we want to perform a transformation on each value. So far, we’ve looked at a per-value transformation function (&lt;code class="highlighter-rouge"&gt;Sortable&lt;/code&gt;), but that means the overhead of a call per-value (which may or may not get inlined, depending on the complexity, and how we resolve the method - i.e. does it involve a &lt;code class="highlighter-rouge"&gt;virtual&lt;/code&gt; call to a type that isn’t reliably known). Additioanlly, it makes it very hard for us to apply more advanced optimizations! Blocks good.&lt;/p&gt; &lt;p&gt;So; let’s say we have our existing loop:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;float[] values = ...&lt;br /&gt;int[] radix = ...&lt;br /&gt;for(int i = 0 ; i &amp;lt; values.Length; i++)&lt;br /&gt;{&lt;br /&gt;    radix = someHelper.Sortable(values[i]);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;and we want to retain the ability to swap in per-type implementations of &lt;code class="highlighter-rouge"&gt;someHelper.Sortable&lt;/code&gt;; we can significantly reduce the call overhead by performing a block-based transformation. Consider:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;float[] values = ...&lt;br /&gt;int[] radix = ...&lt;br /&gt;someHelper.ToRadix(values, radix);&lt;br /&gt;...&lt;br /&gt;unsafe void ToRadix(float[] source, int[] destination)&lt;br /&gt;{&lt;br /&gt;    const int MSB = 1 &amp;lt;&amp;lt; 31;&lt;br /&gt;    for(int i = 0 ; i &amp;lt; source.Length; i++)&lt;br /&gt;    {&lt;br /&gt;        var val = source[i];&lt;br /&gt;        int raw = *(int*)(&amp;amp;val);&lt;br /&gt;        // if sign bit set: flip all bits except the MSB&lt;br /&gt;        destination[i] = (raw &amp;amp; MSB) == 0 ? raw : ~raw | MSB;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;How much of a speed improvement this makes depends a lot on whether the JIT managed to inline the IL from the original version. It is usually a good win by itself, but more importantly: it is a key stepping stone to further optimizations.&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;SortablePerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixPerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,120.5 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.97&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;ToRadixBlock&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,080.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.96&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Another small improvement; I was hoping for more, but I suspect that the JIT was &lt;em&gt;already&lt;/em&gt; doing a good job of inlining the method we're calling, making it &lt;em&gt;almost&lt;/em&gt; the same loop at runtime. This is not always the case, though - especially if you have multiple different transformations to apply through a single API.&lt;/p&gt; &lt;h2 id="safely-spanning-the-performance-chasm"&gt;Safely spanning the performance chasm&lt;a class="anchorjs-link " href="#safely-spanning-the-performance-chasm" aria-label="Anchor link for: safely spanning the performance chasm" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;You’ll notice that in the code above I’ve made use of &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code. There are a few things that make &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; appealing, but one of the things it does exceptionally well is allow us to reintrepret chunks of data as other types, which is what this line does:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;int raw = *(int*)(&amp;amp;val);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Actually, there are some methods on &lt;code class="highlighter-rouge"&gt;BitConverter&lt;/code&gt; &lt;a href="https://github.com/dotnet/coreclr/blob/master/src/mscorlib/shared/System/BitConverter.cs#L462-L472"&gt;to do exactly this&lt;/a&gt;, but only the 64-bit (&lt;code class="highlighter-rouge"&gt;double&lt;/code&gt;/&lt;code class="highlighter-rouge"&gt;long&lt;/code&gt;) versions exist in the “.NET Framework” (“.NET Core” has both 32-bit and 64-bit) - and that only helps us with this single example, rather than the general case.&lt;/p&gt; &lt;p&gt;For example, if we are talking in pointers, we can tell the compiler to treat a &lt;code class="highlighter-rouge"&gt;float*&lt;/code&gt; as though it were an &lt;code class="highlighter-rouge"&gt;int*&lt;/code&gt;. One way we &lt;em&gt;might&lt;/em&gt; be tempted to rewrite our &lt;code class="highlighter-rouge"&gt;ToRadix&lt;/code&gt; method could be to move this coercison earlier:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;unsafe void ToRadix(float[] source, int[] destination)&lt;br /&gt;{&lt;br /&gt;    const int MSB = 1 &amp;lt;&amp;lt; 31;&lt;br /&gt;    fixed(float* fPtr = source)&lt;br /&gt;    {&lt;br /&gt;        int* ptr = (int*)fPtr;&lt;br /&gt;        for(int i = 0 ; i &amp;lt; values.Length; i++)&lt;br /&gt;        {&lt;br /&gt;            int raw = *ptr++;&lt;br /&gt;            destination[i] = (raw &amp;amp; MSB) == 0 ? raw : ~raw | MSB;&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Now we’re naturally reading values out &lt;em&gt;as&lt;/em&gt; &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt;, rather than performing any reinterpretation per value. This is &lt;em&gt;useful&lt;/em&gt;, but it requires us to use &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code (always a great way to get hurt), and it doesn’t work with generics - you &lt;strong&gt;cannot&lt;/strong&gt; use &lt;code class="highlighter-rouge"&gt;T*&lt;/code&gt; for some &lt;code class="highlighter-rouge"&gt;&amp;lt;T&amp;gt;&lt;/code&gt;, even with the &lt;code class="highlighter-rouge"&gt;where T : struct&lt;/code&gt; constraint.&lt;/p&gt; &lt;p&gt;I’ve spoken more than a few times about &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;; quite simply: it rocks. To recap, &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; (and it’s heap-friendly cousin, &lt;code class="highlighter-rouge"&gt;Memory&amp;lt;T&amp;gt;&lt;/code&gt;) is a general purpose, efficient, and versatile representation of contiguous memory - which includes things like arrays (&lt;code class="highlighter-rouge"&gt;float[]&lt;/code&gt;), but more exotic things too.&lt;/p&gt; &lt;p&gt;One of the most powerful (but simple) features of &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is that it allows us to do type coercison &lt;em&gt;in fully safe managed code&lt;/em&gt;. For example, instead of a &lt;code class="highlighter-rouge"&gt;float[]&lt;/code&gt;, let’s say that we have a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;float&amp;gt;&lt;/code&gt;. We can reinterpet that as &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt; &lt;em&gt;very simply&lt;/em&gt;:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Span&amp;lt;float&amp;gt; values = ...&lt;br /&gt;var asIntegers = values.NonPortableCast&amp;lt;float, int&amp;gt;();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Note: this API is likely to change - by the time it hits general availablity, it’ll probably be:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;var asIntegers = values.Cast&amp;lt;int&amp;gt;();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;What this does is:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;look at the sizes of the original and target type&lt;/li&gt;  &lt;li&gt;calculate how many of the target type fit into the original data&lt;/li&gt;  &lt;li&gt;round down (so we never go out of range)&lt;/li&gt;  &lt;li&gt;hand us back a span of the target type, of that (possibly reduced) length&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Since &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt; are the same size, we’ll find that &lt;code class="highlighter-rouge"&gt;asIntegers&lt;/code&gt; has the same length as &lt;code class="highlighter-rouge"&gt;values&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;What is &lt;em&gt;especially&lt;/em&gt; powerful here is that this trick &lt;em&gt;works with generics&lt;/em&gt;. It does something that &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code &lt;strong&gt;will not do for us&lt;/strong&gt;. Note that a &lt;em&gt;lot of love&lt;/em&gt; has been shown to &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; in the runtime and JIT - essentially all of the same tricks that make &lt;code class="highlighter-rouge"&gt;T[]&lt;/code&gt; array performance largely indistinguishable from pointer &lt;code class="highlighter-rouge"&gt;T*&lt;/code&gt; performance.&lt;/p&gt; &lt;p&gt;This means we could simplify a lot of things by converting from our generic &lt;code class="highlighter-rouge"&gt;T&lt;/code&gt; &lt;em&gt;even earlier&lt;/em&gt; (so we only do it once for the entire scope of our radix code), and having our radix converter just talk in terms of the raw bits (usually: &lt;code class="highlighter-rouge"&gt;uint&lt;/code&gt;).&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// our original data&lt;br /&gt;float[] values = ...&lt;br /&gt;// recall that radix sort needs some extra space to work in&lt;br /&gt;float[] workspace = ... &lt;br /&gt;// implicit &amp;lt;float&amp;gt; and implicit conversion to Span&amp;lt;float&amp;gt;&lt;br /&gt;RadixSort32(values, workspace); &lt;br /&gt;&lt;br /&gt;...&lt;br /&gt;RadixSort32&amp;lt;TSource&amp;gt;(Span&amp;lt;T&amp;gt; typedSource, Span&amp;lt;T&amp;gt; typedWorkspace)&lt;br /&gt;    where T : struct&lt;br /&gt;{&lt;br /&gt;    // note that the JIT can inline this *completely* and remove impossible&lt;br /&gt;    //  code paths, because for struct T, the JIT is per-T&lt;br /&gt;    if (Unsafe.SizeOf&amp;lt;T&amp;gt;() != Unsafe.SizeOf&amp;lt;uint&amp;gt;()) .. throw an error&lt;br /&gt;&lt;br /&gt;    var source = typedSource.NonPortableCast&amp;lt;T, uint&amp;gt;();&lt;br /&gt;    var workspace = typedWorkspace.NonPortableCast&amp;lt;T, uint&amp;gt;();&lt;br /&gt;&lt;br /&gt;    // convert the radix if needed (into the workspace)&lt;br /&gt;    var converter = GetConverter&amp;lt;T&amp;gt;();&lt;br /&gt;    converter?.ToRadix(source, workspace);&lt;br /&gt;&lt;br /&gt;    // ... more radix sort details not shown&lt;br /&gt;}&lt;br /&gt;...&lt;br /&gt;// our float converter&lt;br /&gt;public void ToRadix(Span&amp;lt;uint&amp;gt; values, Span&amp;lt;uint&amp;gt; destination)&lt;br /&gt;{&lt;br /&gt;    const uint MSB = 1U &amp;lt;&amp;lt; 31;&lt;br /&gt;    for(int i = 0 ; i &amp;lt; values.Length; i++)&lt;br /&gt;    {&lt;br /&gt;        uint raw = values[i];&lt;br /&gt;        destination[i] = (raw &amp;amp; MSB) == 0 ? raw : ~raw | MSB;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The code is getting simpler, while retaining performance &lt;em&gt;and&lt;/em&gt; becoming more generic-friendly; and we haven’t needed to use a single &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;. You’ll have to excuse me an &lt;code class="highlighter-rouge"&gt;Unsafe.SizeOf&amp;lt;T&amp;gt;()&lt;/code&gt; - despite the name, this isn’t &lt;em&gt;really&lt;/em&gt; an “unsafe” operation in the usual sense - this is simply a wrapper to the &lt;code class="highlighter-rouge"&gt;sizeof&lt;/code&gt; IL instruction that is &lt;em&gt;perfectly well defined&lt;/em&gt; for all &lt;code class="highlighter-rouge"&gt;T&lt;/code&gt; that are usable in generics. It just isn’t directly available in safe C#.&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;SortablePerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixPerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,120.5 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.97&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixBlock&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,080.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.96&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;ToRadixSpan&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;7,976.3 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.76&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Now we’re starting to make decent improvements - &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is &lt;em&gt;really&lt;/em&gt; useful for large operations where type coercion is necessary.&lt;/p&gt; &lt;h2 id="taking-up-tree-surgery-prune-those-branches"&gt;Taking up tree surgery: prune those branches&lt;a class="anchorjs-link " href="#taking-up-tree-surgery-prune-those-branches" aria-label="Anchor link for: taking up tree surgery prune those branches" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Something that gnaws at my soul in where we’ve got to is that it includes a branch - an &lt;code class="highlighter-rouge"&gt;if&lt;/code&gt; test, essentially - in the inner part of the loop. Actually, there’s two and they’re both hidden. The first is in the &lt;code class="highlighter-rouge"&gt;for&lt;/code&gt; loop, but the one I’m talking about here is the one hidden in the ternary conditional operation, &lt;code class="highlighter-rouge"&gt;a ? b : c&lt;/code&gt;. CPUs are very clever about branching, with branch prediction and other fancy things - but it can still stall the instruction pipeline, especially if the prediction is wrong. If only there was a way to rewrite that operation to not need a branch. I’m sure you can see where this is going.&lt;/p&gt; &lt;p&gt;Branching: bad. Bit operations: good. A common trick we can use to remove branches is to obtain a bit-mask that is either all 0s (000…000) or all 1s (111…111) - so: 0 and -1 in 2s-complement terms. There are various ways we can do that (although it also depends on the actual value of &lt;code class="highlighter-rouge"&gt;true&lt;/code&gt; in your target system, which is a surprisingly complex question). Obviously one way to do that would be:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// -1 if negative, 0 otherwise&lt;br /&gt;var mask = (raw &amp;amp; MSB) == 0 ? 0 : ~0;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;but that just &lt;em&gt;adds&lt;/em&gt; another branch. If we were using C, we could use the knowledge that the equality test returns an &lt;em&gt;integer&lt;/em&gt; of either 0 or 1, and just negate that to get 0 or -1:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// -1 if negative, 0 otherwise&lt;br /&gt;var mask = -((raw &amp;amp; MSB) == 0);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;But no such trick is available in C#. What we &lt;em&gt;can&lt;/em&gt; do, though, is use knowledge of &lt;em&gt;arithmetic shift&lt;/em&gt;. Left-shift (&lt;code class="highlighter-rouge"&gt;&amp;lt;&amp;lt;&lt;/code&gt;) is simple; we shift our bits &lt;code class="highlighter-rouge"&gt;n&lt;/code&gt; places to the left, filling in with 0s on the right. So binary &lt;code class="highlighter-rouge"&gt;11001101 &amp;lt;&amp;lt; 3&lt;/code&gt; becomes &lt;code class="highlighter-rouge"&gt;01101000&lt;/code&gt; (we lose the &lt;code class="highlighter-rouge"&gt;110&lt;/code&gt; from the left).&lt;/p&gt; &lt;p&gt;Right-shift is more subtle, as there are two of them: logical and arithmetic, which are essentially unsigned and signed. The &lt;em&gt;logical&lt;/em&gt; shift (used with &lt;code class="highlighter-rouge"&gt;uint&lt;/code&gt; etc) moves our bits  &lt;code class="highlighter-rouge"&gt;n&lt;/code&gt; places to the right, filling in with 0s on the left, so &lt;code class="highlighter-rouge"&gt;11001101 &amp;gt;&amp;gt; 3&lt;/code&gt; gives &lt;code class="highlighter-rouge"&gt;00011001&lt;/code&gt; (we lose the &lt;code class="highlighter-rouge"&gt;101&lt;/code&gt; from the right). The &lt;em&gt;arithmetic&lt;/em&gt; shift behaves differently depending on whether the most significant bit (which tells us the sign of the value) is &lt;code class="highlighter-rouge"&gt;0&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;1&lt;/code&gt;. If it is &lt;code class="highlighter-rouge"&gt;0&lt;/code&gt;, it behaves exactly like the logical shift; however, if it is &lt;code class="highlighter-rouge"&gt;1&lt;/code&gt;, it fills in with 1s on the left; so &lt;code class="highlighter-rouge"&gt;11001101 &amp;gt;&amp;gt; 3&lt;/code&gt; gives &lt;code class="highlighter-rouge"&gt;11111001&lt;/code&gt;. Using this, we can use &lt;code class="highlighter-rouge"&gt;&amp;gt;&amp;gt; 31&lt;/code&gt; (or &lt;code class="highlighter-rouge"&gt;&amp;gt;&amp;gt; 63&lt;/code&gt; for 64-bit data) to create a mask that matches the sign of the original data:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// -1 if negative, 0 otherwise&lt;br /&gt;var mask = (uint)(((int)raw) &amp;gt;&amp;gt; 31);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Don’t worry about the extra conversions: as long as we’re in an &lt;code class="highlighter-rouge"&gt;unchecked&lt;/code&gt; context (which we are by default), they simply don’t exist. All they do is tell the compiler which shift operation to emit. If you’re curious, you can &lt;a href="https://sharplab.io/#v2:EYLgtghgzgLgpgJwDQxASwDYB8ACAmAAgGUALNAMxgFgAoAb1oKYJwEYA2AgVzQDsYCAcTgwAstADWACh78CANwgYucAJSNmDGsx04A7ARl8YqqWeOrFytQQB8tggGZWqgNwamAX1qegA==="&gt;see this here&lt;/a&gt;, but in IL terms, this is just:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;(push the value onto the stack)&lt;br /&gt;ldc.i4.s 31 // push 31 onto the stack&lt;br /&gt;shr         // arithmetic right shift&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;In IL terms, there’s really no difference between signed and unsigned integers, &lt;em&gt;other&lt;/em&gt; than whether the compiler emites the signed or unsigned opcodes for operations. In this case we’ve told it to emit &lt;a href="https://msdn.microsoft.com/en-us/library/system.reflection.emit.opcodes.shr(v=vs.110).aspx"&gt;&lt;code class="highlighter-rouge"&gt;shr&lt;/code&gt;&lt;/a&gt; - the signed/arithmetic opcode, instead of &lt;a href="https://msdn.microsoft.com/en-us/library/system.reflection.emit.opcodes.shr_un(v=vs.110).aspx"&gt;&lt;code class="highlighter-rouge"&gt;shr.un&lt;/code&gt;&lt;/a&gt; - the unsigned/logical opcode.&lt;/p&gt; &lt;p&gt;OK; so we’ve got a mask that is either all zeros or all ones. Now we need to use it to avoid a branch, but how? Consider:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;var condition = // all zeros or all ones&lt;br /&gt;var result = (condition &amp;amp; trueValue) | (~condition &amp;amp; falseValue);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;If &lt;code class="highlighter-rouge"&gt;condition&lt;/code&gt; is all zeros, then the &lt;code class="highlighter-rouge"&gt;condition &amp;amp; trueValue&lt;/code&gt; gives us zero; the &lt;code class="highlighter-rouge"&gt;~condition&lt;/code&gt; becomes all ones, and therefore &lt;code class="highlighter-rouge"&gt;~condition &amp;amp; falseValue&lt;/code&gt; gives us &lt;code class="highlighter-rouge"&gt;falseValue&lt;/code&gt;. When we “or” (&lt;code class="highlighter-rouge"&gt;|&lt;/code&gt;) those together, we get &lt;code class="highlighter-rouge"&gt;falseValue&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Likewise, if &lt;code class="highlighter-rouge"&gt;condition&lt;/code&gt; is all ones, then &lt;code class="highlighter-rouge"&gt;condition &amp;amp; trueValue&lt;/code&gt; gives us &lt;code class="highlighter-rouge"&gt;trueValue&lt;/code&gt;; the &lt;code class="highlighter-rouge"&gt;~condition&lt;/code&gt; becomes all zeros, and therefore &lt;code class="highlighter-rouge"&gt;~condition &amp;amp; falseValue&lt;/code&gt; gives us zero. When we “or” (&lt;code class="highlighter-rouge"&gt;|&lt;/code&gt;) those together, we get &lt;code class="highlighter-rouge"&gt;trueValue&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;So our branchless operation becomes:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public void ToRadix(Span&amp;lt;uint&amp;gt; values, Span&amp;lt;uint&amp;gt; destination)&lt;br /&gt;{&lt;br /&gt;    const uint MSB = 1U &amp;lt;&amp;lt; 31;&lt;br /&gt;    for(int i = 0 ; i &amp;lt; values.Length; i++)&lt;br /&gt;    {&lt;br /&gt;        uint raw = values[i];&lt;br /&gt;        var ifNeg = (uint)(((int)raw) &amp;gt;&amp;gt; 31);&lt;br /&gt;        destination[i] =&lt;br /&gt;            (ifNeg &amp;amp; (~raw | MSB)) // true&lt;br /&gt;            | (~ifNeg &amp;amp; raw);      // false&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This might look more complicated, but it is &lt;strong&gt;very&lt;/strong&gt; CPU-friendly: it pipelines very well, and doesn’t involve any branches for it to worry about. Doing a few extra bit operations is &lt;em&gt;nothing&lt;/em&gt; to a CPU - especially if they can be pipelined. Long instruction pipelines are actually a &lt;em&gt;good&lt;/em&gt; thing to a CPU - compared to a branch or something that might involve a cache miss, at least.&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;SortablePerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixPerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,120.5 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.97&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixBlock&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,080.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.96&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixSpan&lt;/td&gt;      &lt;td style="text-align: right"&gt;7,976.3 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.76&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;Branchless&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;2,507.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.24&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;By removing the branches, we’re down to &lt;em&gt;less than a quarter&lt;/em&gt; of the original run-time; that’s a huge win, even if the code is slightly more complex.&lt;/p&gt; &lt;h2 id="why-do-one-thing-at-a-time"&gt;Why do one thing at a time?&lt;a class="anchorjs-link " href="#why-do-one-thing-at-a-time" aria-label="Anchor link for: why do one thing at a time" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;OK, so we’ve got a nice branchless version, and the world looks great. We’ve made significant improvements. But we can still get &lt;em&gt;much better&lt;/em&gt;. At the moment we’re processing each value one at a time, but as it happens, this is a perfect scenario for &lt;em&gt;vectorization&lt;/em&gt; via &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;SIMD (“Single instruction, multiple data”)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We have a pipelineable operation without any branches; if we can execute it for &lt;em&gt;one&lt;/em&gt; value, we can probably execute it for &lt;em&gt;multiple&lt;/em&gt; values &lt;strong&gt;at the same time&lt;/strong&gt; - magic, right? Many modern CPUs include support for performing basic operations like the above on multiple values at a time, using super-wide registers. Right now we’re using 32-bit values, but most current CPUs will have support for AVX (mostly: 128-bit) or AVX2 (mostly: 256-bit) operations. If you’re on a very expensive server, you might have more (AVX512). But let’s assume AVX2: that means we can handle 8 32-bit values at a time. That means 1/8th of the main operations, and also 1/8th of the &lt;code class="highlighter-rouge"&gt;if&lt;/code&gt; branches hidden in the &lt;code class="highlighter-rouge"&gt;for&lt;/code&gt; loop.&lt;/p&gt; &lt;p&gt;Some languages have automatic vectorization during compilation; C# doesn’t have that, and neither does the JIT. But, we still have access to a range of vectorized operations (with support for the more exotic intrinsics being added soon). Until recently, one of the most awkward things about working with vectorization has been &lt;em&gt;loading the values&lt;/em&gt;. This might sound silly, but it is surprisingly difficult to pull the values in efficiently when you don’t know how wide the vector registers are on the target CPU. Fortunately, our amazing new friend &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; jumps to the rescue here - making it almost embarrassingly easy!&lt;/p&gt; &lt;p&gt;First, let’s look at what the shell loop might look like, without actually doing the real work in the middle:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public void ToRadix(Span&amp;lt;uint&amp;gt; values, Span&amp;lt;uint&amp;gt; destination)&lt;br /&gt;{&lt;br /&gt;    const uint MSB = 1U &amp;lt;&amp;lt; 31;&lt;br /&gt;&lt;br /&gt;    int i = 0;&lt;br /&gt;    if (Vector.IsHardwareAccelerated)&lt;br /&gt;    {                               &lt;br /&gt;        var vSource = values.NonPortableCast&amp;lt;uint, Vector&amp;lt;uint&amp;gt;&amp;gt;();&lt;br /&gt;        var vDest = destination.NonPortableCast&amp;lt;uint, Vector&amp;lt;uint&amp;gt;&amp;gt;();&lt;br /&gt;        &lt;br /&gt;        for (int j = 0; j &amp;lt; vSource.Length; j++)&lt;br /&gt;        {&lt;br /&gt;            var vec = vSource[j];&lt;br /&gt;            vDest[j] = // TODO&lt;br /&gt;        }&lt;br /&gt;        // change our root offset for the remainder of the values&lt;br /&gt;        i = vSource.Length * Vector&amp;lt;uint&amp;gt;.Count;&lt;br /&gt;    }&lt;br /&gt;    for( ; i &amp;lt; values.Length; i++)&lt;br /&gt;    {&lt;br /&gt;        uint raw = values[i];&lt;br /&gt;        var ifNeg = (uint)(((int)raw) &amp;gt;&amp;gt; 31);&lt;br /&gt;        destination[i] =&lt;br /&gt;            (ifNeg &amp;amp; (~raw | MSB)) // true&lt;br /&gt;            | (~ifNeg &amp;amp; raw);      // false&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;First, look at the bottom of the code; here we see that our &lt;em&gt;regular branchless&lt;/em&gt; code still persists. This is for two reasons:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;the target CPU &lt;em&gt;might not be capable of vectorization&lt;/em&gt;&lt;/li&gt;  &lt;li&gt;our input data might not be a nice multiple of the register-width, so we might need to process a final few items the old way&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Note that we’ve changed the &lt;code class="highlighter-rouge"&gt;for&lt;/code&gt; loop so that it doesn’t reset the position of &lt;code class="highlighter-rouge"&gt;i&lt;/code&gt; - we don’t &lt;em&gt;necessarily&lt;/em&gt; start at &lt;code class="highlighter-rouge"&gt;0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now look at the &lt;code class="highlighter-rouge"&gt;if (Vector.IsHardwareAccelerated)&lt;/code&gt;; this checks that suitable vectorization support is available. Note that the JIT can optimize this check away completely (and remove all of the inner code if it won’t be reached). If we &lt;em&gt;do&lt;/em&gt; have support, we cast the span from a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;uint&amp;gt;&lt;/code&gt; to a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;Vector&amp;lt;uint&amp;gt;&amp;gt;&lt;/code&gt;. Note that &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt; is recognized by the JIT, and will be &lt;em&gt;reshaped&lt;/em&gt; by the JIT to match the size of the available vectorization support on the running computer. That means that when using &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt; we don’t need to worry about whether the target computer has SSE vs AVX vs AVX2 etc - or what the available widths are; simply: “give me what you can”, and the JIT worries about the details.&lt;/p&gt; &lt;p&gt;We can now loop over the &lt;em&gt;vectors&lt;/em&gt; available in the cast spans - loading an entire vector at a time simply using our familiar: &lt;code class="highlighter-rouge"&gt;var vec = vSource[j];&lt;/code&gt;. This is a &lt;em&gt;huge&lt;/em&gt; difference to what loading vectors used to look like. We then do some operation (not shown) on &lt;code class="highlighter-rouge"&gt;vec&lt;/code&gt;, and assign the result &lt;em&gt;again as an entire vector&lt;/em&gt; to &lt;code class="highlighter-rouge"&gt;vDest[j]&lt;/code&gt;. On my machine with AVX2 support, &lt;code class="highlighter-rouge"&gt;vec&lt;/code&gt; is block of 8 32-bit values.&lt;/p&gt; &lt;p&gt;Next, we need to think about that &lt;code class="highlighter-rouge"&gt;// TODO&lt;/code&gt; - what are we actually going to &lt;em&gt;do&lt;/em&gt; here? If you’ve already re-written your inner logic to be branchless, there’s actually a very good chance that it will be a like-for-like translation of your branchless code. In fact, it turns out that the ternary conditional scenario we’re looking at here is &lt;em&gt;so common&lt;/em&gt; that there are vectorized operations &lt;em&gt;precisely to do it&lt;/em&gt;; the “conditional select” vectorized CPU instruction can essentially be stated as:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// result conditionalSelect(condition, left, right)&lt;br /&gt;result = (condition &amp;amp; left) | (~condition &amp;amp; right);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Where &lt;code class="highlighter-rouge"&gt;condition&lt;/code&gt; is &lt;em&gt;usually&lt;/em&gt; either all-zeros or all-ones (but it doesn’t have to be; if you want to pull different bits from each value, you can do that too).&lt;/p&gt; &lt;p&gt;This intrinsic is exposed directly on &lt;code class="highlighter-rouge"&gt;Vector&lt;/code&gt;, so our missing code becomes simply:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;var vMSB = new Vector&amp;lt;uint&amp;gt;(MSB);&lt;br /&gt;var vNOMSB = ~vMSB;&lt;br /&gt;for (int j = 0; j &amp;lt; vSource.Length; j++)&lt;br /&gt;{&lt;br /&gt;    var vec = vSource[j];&lt;br /&gt;    vDest[j] = Vector.ConditionalSelect(&lt;br /&gt;        condition: Vector.GreaterThan(vec, vNOMSB),&lt;br /&gt;        left: ~vec | vMSB, // when true&lt;br /&gt;        right: vec // when false&lt;br /&gt;    );&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Note that I’ve pre-loaded a vector with the MSB value (which creates a vector with that value in every cell), and I’ve switched to using a &lt;code class="highlighter-rouge"&gt;&amp;gt;&lt;/code&gt; test instead of a bit test and shift. Partly, this is because the vectorized equality / inequality operations &lt;em&gt;expect&lt;/em&gt; this kind of usage, and very kindly return &lt;code class="highlighter-rouge"&gt;-1&lt;/code&gt; as their true value - using the result to directly feed “conditional select”.&lt;/p&gt; &lt;table&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;Method&lt;/th&gt;      &lt;th style="text-align: right"&gt;Mean&lt;/th&gt;      &lt;th style="text-align: right"&gt;Scaled&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;SortablePerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,483.8 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;1.00&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixPerValue&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,120.5 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.97&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixBlock&lt;/td&gt;      &lt;td style="text-align: right"&gt;10,080.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.96&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;ToRadixSpan&lt;/td&gt;      &lt;td style="text-align: right"&gt;7,976.3 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.76&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;Branchless&lt;/td&gt;      &lt;td style="text-align: right"&gt;2,507.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.24&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;&lt;strong&gt;Vectorized&lt;/strong&gt;&lt;/td&gt;      &lt;td style="text-align: right"&gt;930.0 us&lt;/td&gt;      &lt;td style="text-align: right"&gt;0.09&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the effect of vectorization on this type of code is just amazing - with us now getting more than an order-of-magnitude improvement on the original data. That’s why I’m so excited about how easy (relatively speaking) &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; makes vectorization, and why I can’t wait for &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; to hit production.&lt;/p&gt; &lt;p&gt;A reasonable range of common operations are available on &lt;a href="https://msdn.microsoft.com/en-us/library/system.numerics.vector(v=vs.111).aspx"&gt;&lt;code class="highlighter-rouge"&gt;Vector&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://msdn.microsoft.com/en-us/library/dn858385(v=vs.111).aspx"&gt;&lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt;. If you need exotic operations like “gather”, you might need to wait until &lt;a href="https://github.com/dotnet/corefx/blob/master/src/System.Runtime.Intrinsics/ref/System.Runtime.Intrinsics.cs"&gt;&lt;code class="highlighter-rouge"&gt;System.Runtime.Intrinsics&lt;/code&gt;&lt;/a&gt; lands. One key difference here is that &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt; exposes the &lt;em&gt;common intersection&lt;/em&gt; of operations that might be available (with different widths) against &lt;em&gt;different&lt;/em&gt; CPU instruction sets, where as &lt;code class="highlighter-rouge"&gt;System.Runtime.Intrinsics&lt;/code&gt; aims to expose the &lt;em&gt;underlying&lt;/em&gt; intrinsics - giving access to the full range of instructions, but forcing you to code specifically to a chosen instruction set (or possibly having two implementations - one for AVX and one for AVX2). This is simply because there isn’t a uniform API surface between generatons and vendors - it isn’t simply that you get the same operations with different widths: you get different operations too. So you’d typically be checking &lt;code class="highlighter-rouge"&gt;Aes.IsSupported&lt;/code&gt;, &lt;code class="highlighter-rouge"&gt;Avx2.IsSupported&lt;/code&gt;, etc. Being realistic: &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt; is what we have &lt;em&gt;today&lt;/em&gt;, and it worked damned well.&lt;/p&gt; &lt;h2 id="summary"&gt;Summary&lt;a class="anchorjs-link " href="#summary" aria-label="Anchor link for: summary" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;We’ve looked at a range of advanced techniques for improving performance of critical loops of C# code, including (to repeat the list from the start):&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;using knowledge of how signed data works to avoid having to transform between them&lt;/li&gt;  &lt;li&gt;performing operations in blocks rather than per value to reduce calls&lt;/li&gt;  &lt;li&gt;using &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; as a replacemment for &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code and unmanaged pointers, allowing you to get very high performance even in 100% managed/safe code&lt;/li&gt;  &lt;li&gt;investigating branch removal as a performance optimization of critical loops&lt;/li&gt;  &lt;li&gt;vectorizing critical loops to do the same work with significantly fewer CPU operations&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;And we’ve seen &lt;em&gt;dramatic&lt;/em&gt; improvements to the performance. Hopefully, some or all of these techniques will be applicable to your own code. Either way, I hope it has been an interesting diversion.&lt;/p&gt; &lt;p&gt;Next time: practical parallelization&lt;/p&gt; &lt;h2 id="addendum"&gt;Addendum&lt;/h2&gt;&lt;p&gt;For completeness: yes I also tried a &lt;code class="highlighter-rouge"&gt;val ^ ~MSB&lt;/code&gt; approach for both branchless and vectorized; it wasn't an improvement.&lt;/p&gt;&lt;p&gt;And for the real implementation (the "aside" mentioned above): what the code &lt;em&gt;actually&lt;/em&gt; does for sign-bit data (IEEE754) is: sort &lt;em&gt;just&lt;/em&gt; on the sign bit &lt;em&gt;first&lt;/em&gt;, use the count data to find where the sign changes (without scanning over the data an extra time), and then sort the two halves separately &lt;em&gt;ignoring&lt;/em&gt; the MSB, with the first chunk in descending order and the second chunk in ascending order. By doing this, we avoid the need for the transform - again, by using knowledge of the bit layout of the data.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/eGmdZnFJYBk" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/935361126186292502" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/935361126186292502" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/eGmdZnFJYBk/sorting-myself-out-extreme-edition.html" title="Sorting myself out, extreme edition" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/01/sorting-myself-out-extreme-edition.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-9054553060030033587</id><published>2018-01-20T13:27:00.002-08:00</published><updated>2018-01-30T06:19:38.435-08:00</updated><title type="text">More Of A Sort Of Problem</title><content type="html">&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2018/01/a-sort-of-problem.html"&gt;part 1 here&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2018/01/sorting-myself-out-extreme-edition.html"&gt;part 3 here&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;So &lt;a href="http://blog.marcgravell.com/2018/01/a-sort-of-problem.html"&gt;last time&lt;/a&gt; I talked about a range of ways of performing a sort, ranging from the simple thru to hijacking .NET source code. Very quickly, some folks pointed out that I should have looked at “radix sort”, and they’re absoltely right - I should have. In fact, in the GPU version of this same code, we do exactly that &lt;a href="https://nvlabs.github.io/cub/structcub_1_1_device_radix_sort.html"&gt;via the CUB library&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The great thing is, radix sort is relatively simple, so:&lt;/p&gt; &lt;h2 id="attempt-9-radix-sort"&gt;Attempt 9: radix sort&lt;a class="anchorjs-link " href="#attempt-9-radix-sort" aria-label="Anchor link for: attempt 9 radix sort" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;The key point about radix sort is that it works by grouping the data by groups of bits in the data, using the same “bitwise sortable” idea that we used previously. We’ve already done the hard work to get a radix compliant representation of our sort data.&lt;/p&gt; &lt;p&gt;We can get a &lt;a href="https://en.wikibooks.org/wiki/Algorithm_Implementation/Sorting/Radix_sort"&gt;basic radix sort implementation from wikibooks&lt;/a&gt;, but this version has a few things we need to fix:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;this is a single-array version; we want a dual array&lt;/li&gt;  &lt;li&gt;we can use &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code to get rid of a lot of array range checks (and just: don’t be wrong!)&lt;/li&gt;  &lt;li&gt;radix sort needs a workspace the same same as the input values as a scratch area; in the version shown, it allocates this internally, but in “real” code we’ll want to manage that externally and pass it in&lt;/li&gt;  &lt;li&gt;we can make &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; (the number of bits to consider at a time) configurable&lt;/li&gt;  &lt;li&gt;the shown code copies the workspace over the real data each cycle, but we can avoid this by simply swapping what we consider “real” and “workspace” each cycle, and copying once at the end if required&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;I’m not going to try and describe how or why radix sort works (wikipedia covers much of that &lt;a href="https://en.wikipedia.org/wiki/Radix_sort"&gt;here&lt;/a&gt;); the key thing &lt;strong&gt;that will be relevant in a moment&lt;/strong&gt; is: for the group size &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt;, it loops through all the data looking &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; bits at a time, to see how many values there are with each possible value for those &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; bits. So if &lt;code class="highlighter-rouge"&gt;r=4&lt;/code&gt;, there are 16 possible values over each 4 bits. Once it has that, it iterates a second time, writing the values into the corresponding places for the group it is in.&lt;/p&gt; &lt;p&gt;Once we have an implementation, our code basically consists of preparing the bit-sortable keys just like we did before, then simply invoking the algoritm, passing in our reusable workspace:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Helpers.RadixSort(sortKeys, index, keysWorkspace, valuesWorkspace, r);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;(where &lt;code class="highlighter-rouge"&gt;keysWorkspace&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;valuesWorkspace&lt;/code&gt; are scratch areas of the required size, shared between sort cycles).&lt;/p&gt; &lt;p&gt;One consideration here is: what value of &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; (the number of bits to consider at a time) to choose. &lt;code class="highlighter-rouge"&gt;4&lt;/code&gt; is a reasonably safe default, but you can experiment with different values for your data to see what works well.&lt;/p&gt; &lt;p&gt;I get:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;r=2: 3800ms&lt;/li&gt;  &lt;li&gt;r=4: 1900ms&lt;/li&gt;  &lt;li&gt;r=8: 1200ms&lt;/li&gt;  &lt;li&gt;r=16: 2113ms&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;This r=8 is very tempting that is a significant improvement on our previous best.&lt;/p&gt; &lt;h2 id="attempt-10-radix-sort-with-parellelization"&gt;Attempt 10: radix sort with parellelization&lt;a class="anchorjs-link " href="#attempt-10-radix-sort-with-parellelization" aria-label="Anchor link for: attempt 10 radix sort with parellelization" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Remember the “&lt;strong&gt;that will be relevant in a moment&lt;/strong&gt;” from a few paragraphs ago? Recall: a key point of radix sort is that for each group of bits (of size &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt;), it needs to iterate the entire key-set to count the frequencies of each possible group value. This count operation is something that is embarrasingly parallelizable, since counting chunks can be done independently over the entire data.&lt;/p&gt; &lt;p&gt;To do that, we can create a number of workers, divide the key-space into that many chunks, and tell each worker to perform the counts &lt;em&gt;for that chunk&lt;/em&gt;. Fire these workers in parallel via &lt;code class="highlighter-rouge"&gt;Parallel.Invoke&lt;/code&gt; or similar, and reap the rewards. This creates a slight complexity that we need to &lt;em&gt;combine&lt;/em&gt; the counts, and there will be thread races. A naive but thread-safe implementation would be to use &lt;code class="highlighter-rouge"&gt;Interlocked.Increment&lt;/code&gt; to do all the counts, but that would have severe collision penalties - it is far preferable to count each chunk in complete isolation, and only worry about the combination at the end. At that point, either &lt;code class="highlighter-rouge"&gt;lock&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;Interlocked&lt;/code&gt; would be fine, as it is going to happen very minimally. We should also be careful to hoist everything we want into a local, to avoid a lot of &lt;code class="highlighter-rouge"&gt;ldarg.0&lt;/code&gt;, &lt;code class="highlighter-rouge"&gt;ldfld&lt;/code&gt; overhead:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public void Invoke()&lt;br /&gt;{&lt;br /&gt;    var len = Length;&lt;br /&gt;    var mask = Mask;&lt;br /&gt;    var keys = Keys + Offset;&lt;br /&gt;    var shift = Shift;&lt;br /&gt;    int* count = stackalloc int[CountLength];&lt;br /&gt;&lt;br /&gt;    // count into a local buffer&lt;br /&gt;    for (int i = 0; i &amp;lt; len; i++)&lt;br /&gt;        count[(*keys++ &amp;gt;&amp;gt; shift) &amp;amp; mask]++;&lt;br /&gt;&lt;br /&gt;    // now update the origin data, synchronized&lt;br /&gt;    lock (SyncLock)&lt;br /&gt;    {&lt;br /&gt;        for (int i = 0; i &amp;lt; CountLength; i++)&lt;br /&gt;            Counts[i] += count[i];&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Here we’re also using &lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; to do all our counting in the stack space, rather than allocating a count buffer per worker. This is fine, since we’ll typically be dealing with values like &lt;code class="highlighter-rouge"&gt;r=4&lt;/code&gt; (&lt;code class="highlighter-rouge"&gt;CountLength=16&lt;/code&gt;). Even for larger &lt;em&gt;reasonable&lt;/em&gt; &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt;, the stack space is fine. We could very reasonably put an upper bound on &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; of &lt;code class="highlighter-rouge"&gt;16&lt;/code&gt; if we wanted to be sure.&lt;/p&gt; &lt;p&gt;Our calling code is virtually idental - all we’re doing is changing the internal implementation:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Helpers.RadixSortParallel(sortKeys, index, keysWorkspace, valuesWorkspace, r);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;So what does this do for performance? Note: I’m using &lt;code class="highlighter-rouge"&gt;Environment.ProcessorCount * 2&lt;/code&gt; workers, but we could play with other values.&lt;/p&gt; &lt;p&gt;I get&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;r=2: 3600ms&lt;/li&gt;  &lt;li&gt;r=4: 1800ms&lt;/li&gt;  &lt;li&gt;r=8: 1200ms&lt;/li&gt;  &lt;li&gt;r=16: 2000ms&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;So; we don’t get a &lt;em&gt;vast&lt;/em&gt; improvement really - our key benefit comes from simply choosing a suitable &lt;code class="highlighter-rouge"&gt;r&lt;/code&gt; for our data, like &lt;code class="highlighter-rouge"&gt;r=8&lt;/code&gt;.&lt;/p&gt; &lt;h2 id="throws-down-gauntlet"&gt;Throws down gauntlet&lt;a class="anchorjs-link " href="#throws-down-gauntlet" aria-label="Anchor link for: throws down gauntlet" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;So; so far we’ve gone from 17s (LINQ) to 1.2s (radix sort, single-threaded or parallel). What more can we do? Can we parallelize the second half of radix sort? Can we try a completely different sort? Can we combine our index and keys so we are performing a single array sort? Can we make use of some obscure CPU instructions to perform 128-bit (or wider) operations to combine our existing 64-bit key and 32-bit value? Vectorize a key part of one of the existing algorithms with SIMD?&lt;/p&gt; &lt;p&gt;If you have more ideas, please feel free to fork and PR &lt;a href="https://github.com/mgravell/SortOfProblem"&gt;from here&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/U2TiQP6eK9U" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/9054553060030033587" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/9054553060030033587" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/U2TiQP6eK9U/more-of-sort-of-problem.html" title="More Of A Sort Of Problem" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/01/more-of-sort-of-problem.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-1179063524440025033</id><published>2018-01-19T16:50:00.002-08:00</published><updated>2018-01-30T06:18:56.119-08:00</updated><title type="text">A Sort Of Problem</title><content type="html">&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2018/01/more-of-sort-of-problem.html"&gt;part 2 here&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2018/01/sorting-myself-out-extreme-edition.html"&gt;part 3 here&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I &lt;em&gt;love&lt;/em&gt; interesting questions, especially when they directly relate to things I need to do. A great question came up on Stack Overflow today about &lt;a href="https://stackoverflow.com/q/48345753/23354"&gt;how to efficiently sort large data&lt;/a&gt;. I gave an answer, but there’s &lt;em&gt;so much more&lt;/em&gt; we can say on the topic, so I thought I’d turn it into a blog entry, exploring pragmatic ways to improve sort performance when dealing with non-trivial amounts of data. In particular, this is remarkably similar to time I’ve spent trying to make our “tag engine” faster.&lt;/p&gt; &lt;h2 id="the-problem"&gt;The problem&lt;a class="anchorjs-link " href="#the-problem" aria-label="Anchor link for: the problem" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;So, the premise is this:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;we have a complex entity, &lt;code class="highlighter-rouge"&gt;SomeType&lt;/code&gt;, with multiple properties&lt;/li&gt;  &lt;li&gt;we have a large number of these entities - lets say 16M+&lt;/li&gt;  &lt;li&gt;we want to sort this data using a sort that considers multiple properties - “this, then that”&lt;/li&gt;  &lt;li&gt;and we want it to be fast&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Note that sorting data when it is already sorted or nearly-sorted is &lt;em&gt;usually&lt;/em&gt; cheap under most common algorithms, so I’m going to be focusing only on the initial painful sort when the data is not at all sorted.&lt;/p&gt; &lt;p&gt;Because we’re going to have so many of them, and they are going to be basic storage types only, this is a good scenario to consider a &lt;code class="highlighter-rouge"&gt;struct&lt;/code&gt;, and I was delighted to see that the OP in the question had already done this. We’ll play with a few of the properties (for sorting, etc), but to simulate the usual context, there will be extra stuff that isn’t relevant to the question, so we’ll pad the size of the struct with some dummy fields up to 64 bytes. So, something like:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;readonly partial struct SomeType&lt;br /&gt;{&lt;br /&gt;    public int Id { get; }&lt;br /&gt;    public DateTime ReleaseDate { get; }&lt;br /&gt;    public double Price { get; }&lt;br /&gt;&lt;br /&gt;    public SomeType(int id, DateTime releaseDate, double price)&lt;br /&gt;    {&lt;br /&gt;        Id = id;&lt;br /&gt;        ReleaseDate = releaseDate;&lt;br /&gt;        Price = price;&lt;br /&gt;        _some = _other = _stuff = _not = _shown = 0;&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;#pragma warning disable CS0414 // suppress "assigned, never used"&lt;br /&gt;    private readonly long _some, _other, _stuff, _not, _shown;&lt;br /&gt;#pragma warning restore CS0414&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Note: yes, I know that &lt;code class="highlighter-rouge"&gt;double&lt;/code&gt; is a terrible choice for something that describes money.&lt;/p&gt; &lt;p&gt;Note: &lt;code class="highlighter-rouge"&gt;readonly struct&lt;/code&gt; is a new C# feature, &lt;a href="https://blogs.msdn.microsoft.com/mazhou/2017/11/21/c-7-series-part-6-read-only-structs/"&gt;described in more detail here&lt;/a&gt; - this is a good fit, and might help us avoid some large “load” costs.&lt;/p&gt; &lt;p&gt;For something interesting to do, we’ll try sorting things “most recent, then cheapest”.&lt;/p&gt; &lt;h2 id="inventing-some-data"&gt;Inventing some data&lt;a class="anchorjs-link " href="#inventing-some-data" aria-label="Anchor link for: inventing some data" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;The first thing we need is some data; a very basic seeded random data script might be something like:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;var rand = new Random(data.Length);&lt;br /&gt;for (int i = 0; i &amp;lt; data.Length; i++)&lt;br /&gt;{&lt;br /&gt;    int id = rand.Next();&lt;br /&gt;    var releaseDate = Epoch&lt;br /&gt;        .AddYears(rand.Next(50))&lt;br /&gt;        .AddDays(rand.Next(365))&lt;br /&gt;        .AddSeconds(rand.Next(24 * 60 * 60));&lt;br /&gt;    var price = rand.NextDouble() * 50000;&lt;br /&gt;    data[i] = new SomeType(&lt;br /&gt;        id, releaseDate, price);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;h2 id="attempt-1-linq"&gt;Attempt 1: LINQ&lt;a class="anchorjs-link " href="#attempt-1-linq" aria-label="Anchor link for: attempt 1 linq" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;LINQ is great; I love LINQ, and it makes some code very expressive. So let’s try the most obvious thing first:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;sorted = (from item in data&lt;br /&gt;        orderby item.ReleaseDate descending,&lt;br /&gt;                item.Price&lt;br /&gt;        select item).ToArray();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This LINQ expression performs the sort we want, creating a copy of the data - but on my machine this takes about 17 seconds to run - not ideal. So that’s the target to beat. The key thing about LINQ is that it is designed for &lt;em&gt;your&lt;/em&gt; efficiency, i.e. the size and complexity of the code that you need to write, on the assumption that you’ll only use it on reasonable data. We do not have reasonable data here.&lt;/p&gt; &lt;h2 id="attempt-2-icomparablet"&gt;Attempt 2: &lt;code class="highlighter-rouge"&gt;IComparable&amp;lt;T&amp;gt;&lt;/code&gt;&lt;a class="anchorjs-link " href="#attempt-2-icomparablet" aria-label="Anchor link for: attempt 2 icomparablet" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Since we’re talking about arrays, another obvious thing to do is &lt;code class="highlighter-rouge"&gt;Array.Sort&lt;/code&gt;; for the simplest version of that, we need to implement &lt;code class="highlighter-rouge"&gt;IComparable&amp;lt;T&amp;gt;&lt;/code&gt; on our type:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;partial struct SomeType : IComparable&amp;lt;SomeType&amp;gt;&lt;br /&gt;{&lt;br /&gt;    int IComparable&amp;lt;SomeType&amp;gt;.CompareTo(SomeType other)&lt;br /&gt;    {&lt;br /&gt;        var delta = other.ReleaseDate&lt;br /&gt;            .CompareTo(this.ReleaseDate);&lt;br /&gt;        if (delta == 0) // second property&lt;br /&gt;            delta = this.Price.CompareTo(other.Price);&lt;br /&gt;        return delta;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;And then we can use:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort&amp;lt;SomeType&amp;gt;(data);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;to perform an in-place sort. The generic &lt;code class="highlighter-rouge"&gt;&amp;lt;SomeType&amp;gt;&lt;/code&gt; here is actually redundant, but I’ve included it to make it obious that I am using the generic API.&lt;/p&gt; &lt;p&gt;This takes just over 6 seconds for me, so: a huge improvement! Note that for the purpose of our tests, we will re-populate the data after this, to oensure that all tests start with randomized data. For brevity, assume we’re doing this whenever necessary - I won’t keep calling it out.&lt;/p&gt; &lt;h2 id="attempt-3-icomparert"&gt;Attempt 3: &lt;code class="highlighter-rouge"&gt;IComparer&amp;lt;T&amp;gt;&lt;/code&gt;&lt;a class="anchorjs-link " href="#attempt-3-icomparert" aria-label="Anchor link for: attempt 3 icomparert" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;There’s a second common sort API: &lt;code class="highlighter-rouge"&gt;IComparer&amp;lt;T&amp;gt;&lt;/code&gt; custom comparers. This has the advantages that a: you don’t need to edit the target type, and b: you can support multiple different sorts against the same type via different custom comparers. For this, we add or own comparer:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;sealed class SomeTypeComparer : IComparer&amp;lt;SomeType&amp;gt;&lt;br /&gt;{&lt;br /&gt;    private SomeTypeComparer() { }&lt;br /&gt;    public static SomeTypeComparer Default { get; } = new SomeTypeComparer();&lt;br /&gt;    int IComparer&amp;lt;SomeType&amp;gt;.Compare(SomeType x, SomeType y)&lt;br /&gt;    {&lt;br /&gt;        var delta = y.ReleaseDate&lt;br /&gt;                .CompareTo(x.ReleaseDate);&lt;br /&gt;        if (delta == 0) // second property&lt;br /&gt;            delta = x.Price.CompareTo(y.Price);&lt;br /&gt;        return delta;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;using it via:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort&amp;lt;SomeType&amp;gt;(data, SomeTypeComparer.Default);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This takes around 8 seconds; we’re not going in the right direction here!&lt;/p&gt; &lt;h2 id="attempt-4-comparisont"&gt;Attempt 4: &lt;code class="highlighter-rouge"&gt;Comparison&amp;lt;T&amp;gt;&lt;/code&gt;&lt;a class="anchorjs-link " href="#attempt-4-comparisont" aria-label="Anchor link for: attempt 4 comparisont" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Why stop at two ways to do the same thing, when we can have 3? For completeness, there’s yet another primary &lt;code class="highlighter-rouge"&gt;Array.Sort&amp;lt;T&amp;gt;&lt;/code&gt; variant that takes a delegate, for example:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort&amp;lt;SomeType&amp;gt;(data, (x, y) =&amp;gt;&lt;br /&gt;{&lt;br /&gt;    var delta = y.ReleaseDate&lt;br /&gt;            .CompareTo(x.ReleaseDate);&lt;br /&gt;    if (delta == 0) // second property&lt;br /&gt;        delta = x.Price.CompareTo(y.Price);&lt;br /&gt;    return delta;&lt;br /&gt;});&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This keeps the “do a sort” and “like this” code all in the same place, which is nice; but: it performs virtually identically to the previous attempt, at around 8 seconds.&lt;/p&gt; &lt;h1 id="first-intermission-what-is-going-wrong"&gt;First intermission: what is going wrong?&lt;/h1&gt; &lt;p&gt;We’re doing a lot of work here, that much is true; but there are things that are exacerbating the situation:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;we have a large struct, which means we need to copy that data on the stack whenever we do anything&lt;/li&gt;  &lt;li&gt;because it needs to compare values to their neighbours, there are a &lt;em&gt;lot&lt;/em&gt; of virtual calls going on&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;These costs are fine for reasonable data, but for larger volumes the costs start building up.&lt;/p&gt; &lt;p&gt;We need an alternative.&lt;/p&gt; &lt;p&gt;It happens that &lt;code class="highlighter-rouge"&gt;Array.Sort&lt;/code&gt; also has overloads that accept &lt;em&gt;two&lt;/em&gt; arrays - the keys and the values. What this does is: perform the sort logic on the &lt;em&gt;first&lt;/em&gt; array, but whenever it swaps data around: it swaps the corresponding items in &lt;strong&gt;both&lt;/strong&gt; arrays. This has the effect of sorting the &lt;em&gt;second&lt;/em&gt; array by the values of the &lt;em&gt;first&lt;/em&gt;. In visual terms, it is like selecting two columns of a spreadsheet and clicking sort.&lt;/p&gt; &lt;p&gt;If we only had a single value, this would be great! For example…&lt;/p&gt; &lt;h2 id="attempt-5-dual-arrays-single-property"&gt;Attempt 5: dual arrays, single property&lt;a class="anchorjs-link " href="#attempt-5-dual-arrays-single-property" aria-label="Anchor link for: attempt 5 dual arrays single property" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Let’s pretend for a moment that we only want to sort by the date, in ascending order. Which isn’t what we want, but: humour me.&lt;/p&gt; &lt;p&gt;What we &lt;em&gt;could&lt;/em&gt; do is keep a &lt;code class="highlighter-rouge"&gt;CreationDate[]&lt;/code&gt; hanging around (reuse it between operations), and when we want to sort: populate the data we want to sort by into this array:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;for (int i = 0; i &amp;lt; data.Length; i++)&lt;br /&gt;    releaseDates[i] = data[i].ReleaseDate;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;and then to sort:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort(releaseDates, data);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;For me, this takes about 150ms to prepare the keys, and 4.5s to execute the sort. Promising, although hard to tell if that is useful until we can handle the complex dual sort.&lt;/p&gt; &lt;h1 id="second-intermission-how-can-we-compose-the-sort"&gt;Second intermission: how can we compose the sort?&lt;/h1&gt; &lt;p&gt;We have two properties that we want to sort by, and a &lt;code class="highlighter-rouge"&gt;Sort&lt;/code&gt; method that only takes a single value. We could start looking at tuple types, but that is just making things even more complex. What we want is a way to &lt;em&gt;simplify&lt;/em&gt; the complex sort into a single value. What if we could use something simple like an integer to represent our combined sort? Well, we can!&lt;/p&gt; &lt;p&gt;Many basic values can - either directly, or via a hack - be treated as a bitwise-sortable value. By bitwise sortable, I essentially mean: “sorts like the samme bits expressed as an unsigned integer would sort”. Consider a 32-bit integer: obviously an unsigned integer sorts &lt;em&gt;just like&lt;/em&gt; an unsigned integer, but a &lt;em&gt;signed&lt;/em&gt; integer does not - negative numbers are problematic. What would be great is if &lt;code class="highlighter-rouge"&gt;int.MinValue&lt;/code&gt; was treated as 0, with &lt;code class="highlighter-rouge"&gt;int.MinValue + 1&lt;/code&gt; treated as 1, etc; we can do that by subtraction:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;protected static ulong Sortable(int value)&lt;br /&gt;{&lt;br /&gt;    // re-base eveything upwards, so anything&lt;br /&gt;    // that was the min-value is now 0, etc&lt;br /&gt;    var val = unchecked((uint)(value - int.MinValue));&lt;br /&gt;    return val;&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The result of this is that &lt;code class="highlighter-rouge"&gt;Sortable&lt;/code&gt; will return 32-bits worth of data (the same as the input), but with &lt;code class="highlighter-rouge"&gt;000...000&lt;/code&gt; as the minimum expected value, and &lt;code class="highlighter-rouge"&gt;111...111&lt;/code&gt; as the maximum expected value.&lt;/p&gt; &lt;p&gt;Now; notice that we’re only talking about 32 bits here, but we’ve returne a &lt;code class="highlighter-rouge"&gt;ulong&lt;/code&gt;; that’s because we’re going to pack &lt;em&gt;2&lt;/em&gt; values into a single token.&lt;/p&gt; &lt;p&gt;For our &lt;em&gt;actual&lt;/em&gt; data, we hae two pieces:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;a &lt;code class="highlighter-rouge"&gt;DateTime&lt;/code&gt;&lt;/li&gt;  &lt;li&gt;a &lt;code class="highlighter-rouge"&gt;Double&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Now, that’s 16 bytes worth of data, and we only have 8 to play with. This sounds like a dilemma, but &lt;em&gt;usually&lt;/em&gt;: we can cheat by fudging the precision.&lt;/p&gt; &lt;p&gt;For many common applications - and especially things like a &lt;code class="highlighter-rouge"&gt;ReleaseDate&lt;/code&gt;, most of the bits in a &lt;code class="highlighter-rouge"&gt;DateTime&lt;/code&gt; are not useful. We probably don’t need to handle every tick in a 10,000-year range. We can &lt;em&gt;almost certainly&lt;/em&gt; use per-second precision - perhaps even per-&lt;em&gt;day&lt;/em&gt; for a release date. Unix time in seconds using 32 bits has us covered until January 19, 2038. If we need &lt;em&gt;less precision than seconds&lt;/em&gt;, we can extend that hugely; and we can often use a different epoch that fits our minimum expected data. Heck, starting at the year 2000 instead of 1970 buys 30 years even in per-second precision. Time in an epoch is bitwise-sortable.&lt;/p&gt; &lt;p&gt;Likewise, an obvious way of approximating a &lt;code class="highlighter-rouge"&gt;double&lt;/code&gt; in 32 bits would be to cast it as a &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt;. This doesn’t have the same range or precision, but &lt;em&gt;will usually be just fine&lt;/em&gt; for sorting purposes. Floating point data in .NET &lt;a href="https://en.wikipedia.org/wiki/IEEE_754"&gt;has a complex internal structure&lt;/a&gt;, but fortunately making it bitwise-sortable can be achieved through some simple well-known bit hacks:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;protected static unsafe ulong Sortable (float value)&lt;br /&gt;{&lt;br /&gt;    const int MSB = 1 &amp;lt;&amp;lt; 31;&lt;br /&gt;    int raw = *(int*)(&amp;amp;value);&lt;br /&gt;    if ((raw &amp;amp; MSB) != 0) // IEEE first bit is the sign bit&lt;br /&gt;    {&lt;br /&gt;        // is negative; shoult interpret as -(the value without the MSB) - not the same as just&lt;br /&gt;        // dropping the bit, since integer math is twos-complement&lt;br /&gt;        raw = -(raw &amp;amp; ~MSB);&lt;br /&gt;    }&lt;br /&gt;    return Sortable(raw);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Putting these together, we havev all the tools we need to create a single composite value that is &lt;strong&gt;totally meanningless&lt;/strong&gt; for all ordinary purposes, but which represents our sort perfectly.&lt;/p&gt; &lt;h2 id="attempt-6-dual-arrays-dual-property"&gt;Attempt 6: dual arrays, dual property&lt;a class="anchorjs-link " href="#attempt-6-dual-arrays-dual-property" aria-label="Anchor link for: attempt 6 dual arrays dual property" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;We can create a method that composes our two properties:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;static ulong Sortable(in SomeType item)&lt;br /&gt;{&lt;br /&gt;    return (~(ulong)item.ReleaseDate.ToMillenialTime()) &amp;lt;&amp;lt; 32&lt;br /&gt;                | Sortable((float)item.Price);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This might look complex, but what it does is:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;compute the time in seconds since 2000 as a 32-bit unsigned integer&lt;/li&gt;  &lt;li&gt;extends it to 64-bits&lt;/li&gt;  &lt;li&gt;&lt;em&gt;inverts it&lt;/em&gt;; this has the same effect as “descending”, since it reverses the order&lt;/li&gt;  &lt;li&gt;left shifts it by 32 bits, to place those 32 bits in the &lt;strong&gt;upper&lt;/strong&gt; half of our 64 bits (padding on the right with zero)&lt;/li&gt;  &lt;li&gt;compute the bitwise-sortable representation of the price as a 32-bit unsigned integer&lt;/li&gt;  &lt;li&gt;throws that value into the lower 32 bits&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;We can prepare our data into a &lt;code class="highlighter-rouge"&gt;ulong[]&lt;/code&gt; that we keep around between sort operations:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;for (int i = 0; i &amp;lt; data.Length; i++)&lt;br /&gt;    sortKeys[i] = Sortable(in data[i]);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;and finally sort:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort(sortKeys, data);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The prepare operation is more complex now - and has gone up to 300ms, but the sort is &lt;em&gt;faster&lt;/em&gt; at just over 4 seconds. We’re moving in the right direction. Note that the prepare operation is &lt;a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel"&gt;embarrassingly parallelizable&lt;/a&gt;, so we can trivially divide that over a number of cores (say: 16 blocks of 1M records per block) - and can often be further reduced by storing the data in the struct in similar terms to the sortable version (so: the same representation of time, and the same floating-point scale) - thus I’m not going to worry about the prepare cost here.&lt;/p&gt; &lt;p&gt;But we’re still paying a lot of overhead from having to move around those big structs. We can avoid that by… just not doing that!&lt;/p&gt; &lt;h2 id="attempt-7-indexed"&gt;Attempt 7: indexed&lt;a class="anchorjs-link " href="#attempt-7-indexed" aria-label="Anchor link for: attempt 7 indexed" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Rather than sorting our &lt;code class="highlighter-rouge"&gt;SomeType[]&lt;/code&gt; array, we could instead &lt;em&gt;leave that data alone&lt;/em&gt;, forever. Never move the items around (although it is usually fine to replace them with updates). This has multiple advantages, but the one we’re keen on is the reduction of cost copying the data.&lt;/p&gt; &lt;p&gt;So; we can declare an &lt;code class="highlighter-rouge"&gt;int[] index&lt;/code&gt; that is our &lt;em&gt;index&lt;/em&gt; - it just tells us the offsets to look in the &lt;em&gt;actual&lt;/em&gt; data. We can sort that index &lt;em&gt;as though&lt;/em&gt; it were the actual data, and just make sure we go through the index. We need to initialize the index as well as the composite sortable value (although we can re-use the positions if we are re-sorting the data, as usually the data doesn’t move much between cycles - we’ll get another huge boost on re-sorts when the data hasn’t drifted much; we &lt;em&gt;do not&lt;/em&gt; need to reset the index when re-sorting the same data):&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;for (int i = 0; i &amp;lt; data.Length; i++)&lt;br /&gt;{&lt;br /&gt;    index[i] = i;&lt;br /&gt;    sortKeys[i] = Sortable(in data[i]);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;and sort:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Array.Sort(sortKeys, index);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;The only complication is that now, to access the sorted data - instead of looking at &lt;code class="highlighter-rouge"&gt;data[i]&lt;/code&gt; we need to look at &lt;code class="highlighter-rouge"&gt;data[index[i]]&lt;/code&gt;, i.e. find the i’th item in the index, and use &lt;em&gt;that value&lt;/em&gt; as the offset in the actual data.&lt;/p&gt; &lt;p&gt;This takes the time down to 3 seconds - we’re getting there.&lt;/p&gt; &lt;h2 id="attempt-8-indexed-direct-compare-no-range-checks"&gt;Attempt 8: indexed, direct compare, no range checks&lt;a class="anchorjs-link " href="#attempt-8-indexed-direct-compare-no-range-checks" aria-label="Anchor link for: attempt 8 indexed direct compare no range checks" data-anchorjs-icon="" style="font-style: normal; font-variant: normal; font-weight: normal; font-stretch: normal; font-size: 1em; line-height: 1; font-family: anchorjs-icons; padding-left: 0.375em;"&gt;&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;The introspective sort that &lt;code class="highlighter-rouge"&gt;Array.Sort&lt;/code&gt; does is great, but it is still going to be talking via the general &lt;code class="highlighter-rouge"&gt;CompareTo&lt;/code&gt; API on our key type (&lt;code class="highlighter-rouge"&gt;ulong&lt;/code&gt;), and using the array indexers extensively. The JIT in .NET is good, but we can help it out a &lt;em&gt;little bit&lt;/em&gt; more by … “borrowing” (ahem) the &lt;a href="https://github.com/dotnet/coreclr/blob/775003a4c72f0acc37eab84628fcef541533ba4e/src/mscorlib/src/System/Collections/Generic/ArraySortHelper.cs"&gt;&lt;code class="highlighter-rouge"&gt;IntroSort&lt;/code&gt; code&lt;/a&gt;, and:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;replacing the &lt;code class="highlighter-rouge"&gt;CompareTo&lt;/code&gt; usage on the keys with direct  integer operations&lt;/li&gt;  &lt;li&gt;replacing the array access with &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code that uses  &lt;code class="highlighter-rouge"&gt;ulong*&lt;/code&gt; (for the keys) and &lt;code class="highlighter-rouge"&gt;int*&lt;/code&gt; (for the index)&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;(as an aside, it’ll be interesting to see how this behaves with &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;, but that’s not complete yet).&lt;/p&gt; &lt;p&gt;I’m not going to show the implementation here, but it is available in the source project. Our code for consuming this doesn’t change much, except to call our butchered sort API:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Helpers.Sort(sortKeys, index);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;For me this now takes just over 2.5 seconds.&lt;/p&gt; &lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt; &lt;p&gt;So there we go; I’ve explored some common approaches to improrving sort performance; we’ve looked at LINQ; we’ve looked at basic sorts using comparables, comparers and comparisons (which are all different, &lt;em&gt;obviously&lt;/em&gt;); we’ve looked at keyed dual-array sorts; we’ve looked at &lt;em&gt;indexed&lt;/em&gt; sorts (where the source data remains unsorted); and finally we’ve hacked the introspective sort to squeeze a tiny bit more from it.&lt;/p&gt; &lt;p&gt;We’ve seen performance range from 17 seconds for LINQ, 8 seconds for the 3 basic sort APIs, then 4 seconds for our dual array sorts, 3 seconds for the indexed sort, and finally 2.5 seconds with our hacked and debased version.&lt;/p&gt; &lt;p&gt;Not bad for a night’s work!&lt;/p&gt; &lt;p&gt;All the code discussed here is &lt;a href="https://github.com/mgravell/SortOfProblem"&gt;available on github&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2018/01/more-of-sort-of-problem.html"&gt;part 2 here&lt;/a&gt;)&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/reqrLFaQCSo" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/1179063524440025033" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/1179063524440025033" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/reqrLFaQCSo/a-sort-of-problem.html" title="A Sort Of Problem" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2018/01/a-sort-of-problem.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-5061648232667716643</id><published>2017-12-06T02:47:00.004-08:00</published><updated>2017-12-06T08:24:38.392-08:00</updated><title type="text">Dapper, Prepared Statements, and Car Tyres</title><content type="html">&lt;h2&gt;&lt;a id="why-doesnt-dapper-use-prepared-statements" class="anchor" href="#why-doesnt-dapper-use-prepared-statements" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Doesn't Dapper Use Prepared Statements?&lt;/h2&gt;&lt;p&gt;I had a very interesting email in my inbox this week from a &lt;a href="https://www.nuget.org/packages/Dapper/"&gt;Dapper&lt;/a&gt; user; I'm not going to duplicate the email here, but it can be boiled down to:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;My external security consultant is telling me that Dapper is insecure because it doesn't use prepared statements, and is therefore susceptible to SQL injection. What are your thoughts on this?&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;with a Dapper-specific example of something comparable to:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;List&amp;lt;Order&amp;gt; GetOpenOrders(int customerId) =&amp;gt; _connection.Query&amp;lt;Order&amp;gt;(&lt;br /&gt;        "select * from Orders where CustomerId=@customerId and Status=@Open",&lt;br /&gt;        new { customerId, OrderStatus.Open }).AsList();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now this is a fun topic for me, because in my head I'm reading it in the same way that I would read:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;My car mechanic is telling me my car is dangerous because it doesn't use anti-smear formula screen-wash, and is therefore susceptible to skidding in icy weather. What are your thoughts on this?&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Basically, these are two completely unrelated topics. You can have a perfectly good and constructive conversation about either in isolation. There are merits to both discussions. But when you smash them together, it might suggest that the person raising the issue (the "security consultant" in this case, not the person sending me the email) has misunderstood something fundamental.&lt;/p&gt;&lt;p&gt;My initial response - while in my opinion valid - probably needs to be expanded upon:&lt;/p&gt;&lt;p&gt;&lt;a href="https://twitter.com/marcgravell/status/938128960474492928"&gt;&lt;img src="https://pbs.twimg.com/media/DQTnTo4W4AEA7Gz.jpg" alt="Hi! No problem at all. If your security consultant is telling you that a correctly parameterized SQL query is prone to SQL injection, then your security consultant is a fucking idiot with no clue what they're talking about, and you can quote me on that." style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;So; let's take this moment to discuss the two topics and try to put this beast to bed!&lt;/p&gt;&lt;h2&gt;&lt;a id="part-the-first-what-is-sql-injection" class="anchor" href="#part-the-first-what-is-sql-injection" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part The First: What is SQL injection?&lt;/h2&gt;&lt;p&gt;Most folks will be very familiar with this, so I'm not going to cover every nuance, but: SQL injection is the &lt;strong&gt;major&lt;/strong&gt; error made by concatenating inputs into SQL strings. It could be typified by the bad example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;string customerName = customerNameTextBox.Value; // or a http request input; whatever&lt;br /&gt;var badOptionOne = connection.Query&amp;lt;Customer&amp;gt;(&lt;br /&gt;    "select * from Customers where Name='" + customerName + "'");&lt;br /&gt;var badOptionTwo = connection.Query&amp;lt;Customer&amp;gt;(&lt;br /&gt;    string.Format("select * from Customers where Name='{0}'", customerName));&lt;br /&gt;var badOptionThree = connection.Query&amp;lt;Customer&amp;gt;(&lt;br /&gt;    $"select * from Customers where Name='{customerName}'");&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As an aside on &lt;code&gt;badOptionThree&lt;/code&gt;, it &lt;em&gt;really&lt;/em&gt; frustrates me that C# overloading prefers &lt;code&gt;string&lt;/code&gt; to &lt;code&gt;FormattableString&lt;/code&gt; (interpolated &lt;code&gt;$"..."&lt;/code&gt; strings can be assigned to either, but only &lt;code&gt;FormattableString&lt;/code&gt; retains the semantics). I would really have loved to be able to add a method to Dapper like:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[Obsolete("No! Bad developer! Bobby Tables will find you in the dark", error: true)]&lt;br /&gt;public static IEnumerable&amp;lt;T&amp;gt; Query&amp;lt;T&amp;gt;(FormattableString query, /* other args not shown */)&lt;br /&gt;    =&amp;gt; throw new InvalidOperation(...);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This category of coding error is perpetually  on the &lt;a href="https://www.owasp.org/index.php/Top_10_2017-A1-Injection"&gt;OWASP "top 10" list&lt;/a&gt;, and is now infamously associated with &lt;a href="https://xkcd.com/327/"&gt;xkcd's "Bobby Tables"&lt;/a&gt;:&lt;/p&gt;&lt;p&gt;&lt;a href="https://xkcd.com/327/"&gt;&lt;img src="https://imgs.xkcd.com/comics/exploits_of_a_mom.png" alt="Did you really name your son Robert'); DROP TABLE Students;-- ?" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The problem, as the cartoon shows us, is that this allows malicious input to do unexpected and dangerous things. &lt;em&gt;In this case&lt;/em&gt; the hack was to use a quote to end a SQL literal (&lt;code&gt;');...&lt;/code&gt; - in this case with the attacker guessing that the clause was inside parentheses), then issue a separate command (&lt;code&gt;DROP TABLE ...&lt;/code&gt;), then discard anything at the end of the original query using a comment (&lt;code&gt;-- ...&lt;/code&gt;). But the issue is not limited to quotes, and frankly any attempt to play "replace/escape the risky tokens" is an arms race where you need to win every time, but the attacker only needs to win once. Don't play that game.&lt;/p&gt;&lt;p&gt;It can also be a huge internationalization problem, familiar to every developer who has received bug reports about the search not working for some people of Irish or Scottish descent. This (SQL injection - not Irish or Scottish people) is such an exploitable problem that readily available tools exist that can &lt;em&gt;trivially&lt;/em&gt; seach a site for exploitable inputs and give free access to the database with a friendly UI. So... yeah, you really don't want to have SQL injection bugs. No argument there.&lt;/p&gt;&lt;h2&gt;&lt;a id="so-how-do-we-prevent-sql-injection" class="anchor" href="#so-how-do-we-prevent-sql-injection" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;So how do we prevent SQL injection?&lt;/h2&gt;&lt;p&gt;The solution to SQL injection is &lt;em&gt;parameters&lt;/em&gt;. One complaint I have about the xkcd comic - and a range of other discussions on the topic - is the suggestion that you should "sanitize" your inputs to prevent SQL injection. Nope! Wrong. You "sanitize" your inputs to check that they are &lt;em&gt;within what your logic allows&lt;/em&gt; - for example, if the only permitted options from a drop-down are 1, 2 and 3 - then you might want to check that they haven't entered 42. Sanitizing the inputs &lt;em&gt;is not&lt;/em&gt; the right solution to SQL injection: parameters are. We already showed an example of parameters in my SQL example at the top, but to take our search example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;string customerName = customerNameTextBox.Value; // or a http request input; whatever&lt;br /&gt;var customers = connection.Query&amp;lt;Customer&amp;gt;(&lt;br /&gt;    "select * from Customers where Name=@customerName",&lt;br /&gt;    new { customerName });&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;What &lt;em&gt;this&lt;/em&gt; does is add a &lt;em&gt;parameter&lt;/em&gt; called "customerName" with the chosen value, passing that &lt;strong&gt;alongside and separate to&lt;/strong&gt; the command text, in a raw form that doesn't need it to be encoded to work inside a SQL string. At no point does the parameter value get written into the SQL as a literal. Well, except perhaps on some DB backends that don't support parameters at all, in which case frankly it is up to the DB provider to get the handling right (and: virtually all RDBMS have first-class support for parameters).&lt;/p&gt;&lt;p&gt;Note that parameters solve other problems too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the formatting of things like dates and numbers: if you use injection you need to know the format that the RDBMS expects, which is usually not the format that the "current culture" is going to specify, making it awkward; but by using a parameter, the value &lt;em&gt;doesn't need to be formatted as text at all&lt;/em&gt; - with things like numbers and dates usually being sent in a raw binary format - some-defined-endian-fixed-width for integers (including dates), or something like IEEE754 for floating point.&lt;/li&gt;&lt;li&gt;query-plan re-use: the RDBMS can cache our &lt;code&gt;...Name=@customerName&lt;/code&gt; query and re-use the same plan automatically and trivially (without saturating the cache with a different plan for every unique name searched), with different values of the parameter &lt;code&gt;@customerName&lt;/code&gt; - this can provide a great performance boost (side note: this can be double-edged, so you should &lt;em&gt;also&lt;/em&gt; probably learn about &lt;a href="https://blogs.msdn.microsoft.com/sqlprogrammability/2008/11/26/optimize-for-unknown-a-little-known-sql-server-2008-feature/"&gt;&lt;code&gt;OPTIMIZE FOR ... UNKNOWN&lt;/code&gt;&lt;/a&gt; (or the equivalent on your chosen RDBMS) if you're serious about  SQL performance - note this should only be added &lt;em&gt;reactively&lt;/em&gt; based on actual performance investigations)&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;a id="dapper-loves-parameters" class="anchor" href="#dapper-loves-parameters" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dapper loves parameters&lt;/h2&gt;&lt;p&gt;Parameterization is great; Dapper loves parameterization, and does &lt;em&gt;everything it can&lt;/em&gt; to make it easy for you to parameterize your queries. So: whatever criticism you want to throw at Dapper: SQL injection isn't really a fair one. The only time Dapper will be complicit in SQL injection is when you feed it a query that &lt;em&gt;already has&lt;/em&gt; an injection bug before Dapper ever sees it. We can't fix stupid.&lt;/p&gt;&lt;p&gt;For full disclosure: there &lt;em&gt;is&lt;/em&gt; actually one case where Dapper allows literal injection. Consider our &lt;code&gt;GetOpenOrders&lt;/code&gt; query from above. This can &lt;em&gt;also&lt;/em&gt; be written:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;List&amp;lt;Order&amp;gt; GetOpenOrders(int customerId) =&amp;gt; _connection.Query&amp;lt;Order&amp;gt;(&lt;br /&gt;        "select * from Orders where CustomerId=@customerId and Status={=Open}",&lt;br /&gt;        new { customerId, OrderStatus.Open }).AsList();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that instead of &lt;code&gt;@Open&lt;/code&gt; we're now using &lt;code&gt;{=Open}&lt;/code&gt;. This is not SQL syntax - it is telling &lt;em&gt;Dapper&lt;/em&gt; to do an injection of a literal value. This is intended for things &lt;em&gt;that don't change per query&lt;/em&gt; such as status codes - and can result &lt;em&gt;in some cases&lt;/em&gt; in performance improvements. Dapper doesn't want to make it easy to blow your own feet off, so it &lt;em&gt;STRICTLY&lt;/em&gt; only allows this for integers (including &lt;code&gt;enum&lt;/code&gt; values, which are fundamentally integers), since integers are a: very common for this scenario, and b: follow predictable rules as literals.&lt;/p&gt;&lt;h2&gt;&lt;a id="part-the-second-what-are-prepared-statements" class="anchor" href="#part-the-second-what-are-prepared-statements" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part The Second: What are prepared statements?&lt;/h2&gt;&lt;p&gt;There's often a slight confusion here with "stored procedures", so we'll have to touch on that too...&lt;/p&gt;&lt;p&gt;It is pretty common to issue commands to a RDBMS, where the SQL for those commands is contained &lt;em&gt;in the calling application&lt;/em&gt;. This isn't &lt;em&gt;universal&lt;/em&gt; - some applications are written with all the SQL in "stored procedures" that are deployed separately to the server, so the only SQL in the application is the names to invoke. There are merits of both approaches, which might include discussions around:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;isolation - the ability to deploy and manage the SQL separately to the application (which might be desirable in client applications in particlar, where re-deploying all the client installations to fix a small SQL bug is hard or expensive)&lt;/li&gt;&lt;li&gt;performance - &lt;em&gt;historically&lt;/em&gt; stored procedures tended to out-perform ad-hoc commands; in most modern RDBMS this simply isn't a real concern, with the query-plan-cache working virtually identically regardless of the mechanism&lt;/li&gt;&lt;li&gt;granular security - in a high security application you might not want users (even if the "user" is a central app-server) to have direct &lt;code&gt;SELECT&lt;/code&gt; permission on the tables or views - instead preferring to wrap the &lt;em&gt;allowed&lt;/em&gt; queries in stored procedures that the calling user can be granted &lt;code&gt;EXEC&lt;/code&gt; permission; of course a counter-argument there is that a blind &lt;code&gt;EXEC&lt;/code&gt; can &lt;em&gt;hide&lt;/em&gt; what a stored procedure is doing (so it does something the caller didn't expect), but ultimately if someone has pwned your RDBMS server, you're already toast&lt;/li&gt;&lt;li&gt;flexibility - being able to construct SQL to match a &lt;em&gt;specific scenario&lt;/em&gt; (for example: the exact combination of 17 search options) can be important to improving performance (compared, in our search example, to 17 &lt;code&gt;and (@someArg is null or row.SomeCol=@someArg)&lt;/code&gt; clauses). Tools like LINQ and ORMs rely extensively on runtime query generation to match the queries and model known at runtime, so allowing them to execute ad-hoc parameterized commands is required; it should also be noted that most RDBMS can &lt;em&gt;also&lt;/em&gt; execute ad-hoc parameterzied commands &lt;em&gt;from within SQL&lt;/em&gt; - via things like &lt;a href="https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql"&gt;&lt;code&gt;sp_executesql&lt;/code&gt;&lt;/a&gt; from &lt;em&gt;inside&lt;/em&gt; a stored procedure&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;You'll notice that SQL injection is not part of that discussion on the merits of "ad-hoc commands" vs "stored procedures", because parameterization &lt;em&gt;&lt;strong&gt;makes it&lt;/strong&gt;&lt;/em&gt; a non-topic.&lt;/p&gt;&lt;p&gt;So: let's assume that we've had the conversation about stored procedures and we've decided to use ad-hoc statements.&lt;/p&gt;&lt;h2&gt;&lt;a id="what-does-is-mean-to-prepare-a-statement" class="anchor" href="#what-does-is-mean-to-prepare-a-statement" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What does it mean to "prepare" a statement?&lt;/h2&gt;&lt;p&gt;"Preparing a statement" is a sometimes-optional / sometimes-mandatory (depending on the RDBMS) step required to issue ad-hoc SQL commands. Conceptually, it takes our &lt;code&gt;"select * from Orders where CustomerId=@customerId and Status=@Open"&lt;/code&gt; query - along with the defined parameters - and says "I'm going to want to run this in a moment; kindly figure out what that means to you and get everything in place". In terms of ADO.NET, this means calling the &lt;a href="https://msdn.microsoft.com/en-us/library/system.data.common.dbcommand.prepare(v=vs.110).aspx"&gt;&lt;code&gt;DbCommand.Prepare()&lt;/code&gt;&lt;/a&gt; method. There are 3 possible outcomes of a &lt;code&gt;Prepare()&lt;/code&gt; call (ignoring errors):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it does &lt;em&gt;literally nothing&lt;/em&gt; - a no-op; this might commonly be the case if you've told it that you're running a stored procedure (it is already as prepared as it will ever be), or if your chosen RDBMS isn't interested in the concept of prepared statements&lt;/li&gt;&lt;li&gt;it runs an &lt;em&gt;additional optional&lt;/em&gt; operation that it wouldn't have done otherwise - adding a round trip&lt;/li&gt;&lt;li&gt;it runs a &lt;em&gt;required&lt;/em&gt; operation that it was &lt;em&gt;otherwise going to do automatically&lt;/em&gt; when we executed the query&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So &lt;em&gt;on the surface&lt;/em&gt;, the &lt;strong&gt;best case&lt;/strong&gt; is that we achieve no benefit (the first and third options). The &lt;strong&gt;worst case&lt;/strong&gt; is that we've added a round trip. You might be thinking "so why does &lt;code&gt;Prepare()&lt;/code&gt; exist, if it is only ever harmful?" - and the reason is: I've only talked about running the operation &lt;em&gt;once&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The main scenario in which &lt;code&gt;Prepare()&lt;/code&gt; helps us is when you're going to be issuing &lt;em&gt;exactly the same command&lt;/em&gt; (including the parameter definition, but not values), on &lt;em&gt;exactly the same connection&lt;/em&gt;, &lt;em&gt;many many times&lt;/em&gt;, and especially when your RDBMS &lt;em&gt;requires&lt;/em&gt; command preparation. In that scenario, preparing a statement can be a very important &lt;em&gt;performance tweak&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;You'll notice - similarly to stored procedures - that SQL injection is not part of that discussion on the merits of "prepared statements".&lt;/p&gt;&lt;p&gt;It is entirely true to say that Dapper does not currently call &lt;code&gt;Prepare()&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a id="why-doesnt-dapper-prepare-statements" class="anchor" href="#why-doesnt-dapper-prepare-statements" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why doesn't Dapper &lt;code&gt;Prepare()&lt;/code&gt; statements?&lt;/h2&gt;&lt;p&gt;There are various reasons for this, but the most important one is: on most providers, a prepared statement is scoped to the connection &lt;em&gt;and&lt;/em&gt; is stored as part of the &lt;code&gt;DbCommand&lt;/code&gt;. To &lt;em&gt;actually provide&lt;/em&gt; a useful prepared statement story, Dapper would need to store and re-use every &lt;code&gt;DbCommand&lt;/code&gt; for every &lt;code&gt;DbConnection&lt;/code&gt;. Dapper &lt;em&gt;really, really&lt;/em&gt; doesn't want to store your connections. It is designed with high concurrency in mind, and typically works in scenarios where the &lt;code&gt;DbConnection&lt;/code&gt; is short-lived - perhaps scoped to the context of a single web-request. Note that connection pooling doesn't mean that the &lt;em&gt;underlying&lt;/em&gt; connection is short-lived, but Dapper only gets to see the managed &lt;code&gt;DbConnection&lt;/code&gt;, so anything else is opaque to it.&lt;/p&gt;&lt;p&gt;Without tracking every &lt;code&gt;DbConnection&lt;/code&gt; / &lt;code&gt;DbCommand&lt;/code&gt; and &lt;em&gt;without a new abstraction&lt;/em&gt;, the best Dapper could do would be to call &lt;code&gt;.Prepare()&lt;/code&gt; on every &lt;code&gt;DbCommand&lt;/code&gt; immediately before executing it - but this is &lt;em&gt;exactly&lt;/em&gt; the situation we discussed previously where the only two options are "has no effect" and "makes things worse".&lt;/p&gt;&lt;p&gt;Actually, there &lt;em&gt;is&lt;/em&gt; one scenario &lt;em&gt;using the current API&lt;/em&gt; in which Dapper &lt;em&gt;could&lt;/em&gt; usefully consider doing this, which is the scenario:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;connection.Execute(someSql, someListOfObjects);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In &lt;em&gt;this case&lt;/em&gt;, Dapper &lt;em&gt;unrolls&lt;/em&gt; &lt;code&gt;someListOfObjects&lt;/code&gt;, executing &lt;code&gt;someSql&lt;/code&gt; with the parameters from each object in turn - on the same connection. I will acknowledge that a case could be made for Dapper to call &lt;code&gt;.Prepare()&lt;/code&gt; in anticipation of the loop here, although it would require some changes to implement.&lt;/p&gt;&lt;p&gt;But fundamentally, the main objection that dapper has to prepared statements is that &lt;em&gt;typically&lt;/em&gt;, the &lt;em&gt;connections&lt;/em&gt; that Dapper works with are transient and short-lived.&lt;/p&gt;&lt;h2&gt;&lt;a id="could-dapper-usefully-offer-a-prepare-api-for-systems-with-long-lived-connections" class="anchor" href="#could-dapper-usefully-offer-a-prepare-api-for-systems-with-long-lived-connections" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Could Dapper usefully offer a &lt;code&gt;Prepare()&lt;/code&gt; API for systems with long-lived connections?&lt;/h2&gt;&lt;p&gt;Hypothetically, yes: there &lt;em&gt;is&lt;/em&gt; something that Dapper could do here, &lt;em&gt;specifically targeted&lt;/em&gt; at the scenario:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;I have a long-lived connection and an RDBMS that needs statements to be prepared, and I want the best possible performance when issuing repeated ad-hoc parameterized commands.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;We could conceptualize an API that pins a command to a single connection:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;var getOrders = connection.Prepare&amp;lt;Order&amp;gt;(&lt;br /&gt;        "select * from Orders where CustomerId=@customerId",&lt;br /&gt;        new { customerId = 123 }); // dummy args, for type inference&lt;br /&gt;// ...&lt;br /&gt;var orders = getOrders.Query(new { customerId }).AsList();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that in this imaginary API the connection is trapped and pinned inside the object that we stored in &lt;code&gt;getOrders&lt;/code&gt;. There are some things that would need to be considered - for example, how does this work for literal injection and Dapper's fancy "&lt;code&gt;in&lt;/code&gt;" support. A trivial answer might be: just don't support those features when used with &lt;code&gt;.Prepare()&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;I think there's plenty of merit to have this kind of discussion, and I'm 100% open to discussing API features and additions. &lt;em&gt;As long as&lt;/em&gt; we are discussing the right thing - i.e. the "I have a long-lived..." discussion from above.&lt;/p&gt;&lt;p&gt;If, however, we start that conversation (via a security consultant) via:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;I want to use prepared statements to avoid SQL injection&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;then: that &lt;em&gt;is not&lt;/em&gt; a useful discussion.&lt;/p&gt;&lt;h2&gt;&lt;a id="tldr" class="anchor" href="#tldr" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tl;dr:&lt;/h2&gt;&lt;p&gt;If you want to avoid your car skidding  in icy weather, you fit appropriate tyres. You don't change the screen-wash.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/jtWcO1gYrtw" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/5061648232667716643" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/5061648232667716643" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/jtWcO1gYrtw/dapper-prepared-statements-and-car-tyres.html" title="Dapper, Prepared Statements, and Car Tyres" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/12/dapper-prepared-statements-and-car-tyres.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-213552646711780746</id><published>2017-06-21T04:34:00.002-07:00</published><updated>2017-06-21T05:44:48.433-07:00</updated><title type="text">protobuf-net gets proto3 support</title><content type="html">&lt;h1&gt;protobuf-net gets proto3&lt;/h1&gt; &lt;p&gt;For quite a little while, protobuf-net hasn't seen any &lt;em&gt;major&lt;/em&gt; changes. Sure, I've been pottering along with ongoing maintenance and things like .NET Core support, but it hasn't had any step changes in behavior. Until recently.&lt;/p&gt; &lt;h1&gt;2.3.0 Released&lt;/h1&gt; &lt;p&gt;I'm pleased to say that &lt;a href="https://www.nuget.org/packages/protobuf-net/2.3.0-alpha"&gt;2.3.0 has finally dropped&lt;/a&gt;. The most significant part of this is "proto3", which ties into the 3.0.0 version of Protocol Buffers - released by Google at the end of July 2016. There are a few reasons why I haven't looked at this for protobuf-net before now, including:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;zero binary format changes; so &lt;em&gt;ultimately&lt;/em&gt;, even without any library or tooling changes: everything that can be done in proto2 can be done in proto3, &lt;em&gt;interchangeably&lt;/em&gt;; I didn't feel under immense pressure to rush out a release&lt;/li&gt;&lt;li&gt;significant DSL changes for "proto3" syntax, coupled with the fact protobuf-net's existing DSL tools were in bad shape; not least, they were tied into some technologies with a bad cross-platform story. Since I knew I needed a new answer for DSL tooling, it seemed a poor investment to hack the new features into the end-of-life tooling. A significant portion of protobuf-net's usage is from code-first users who don't even &lt;em&gt;have&lt;/em&gt; a DSL version of their schema, hence why this wasn't at the top of my list of priorities&lt;/li&gt;&lt;li&gt;some new data contracts targeting commonly exchanged types, but this is tied into the DSL changes&lt;/li&gt;&lt;li&gt;I misunderstood the nature of the "proto3" syntax changes; I assumed it would be *adding features and complexity, when in fact it &lt;em&gt;removes&lt;/em&gt; a lot of the more awkward features. The few pieces that it did actually add were backported into "proto2" &lt;em&gt;anyway&lt;/em&gt;&lt;/li&gt;&lt;li&gt;I've been busy with lots of other things, including a lot of .NET Core work for multiple libraries&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;But; I've finally managed to get enough time together to look at things properly.&lt;/p&gt; &lt;p&gt;First, some notes on proto3:&lt;/p&gt; &lt;h2&gt;proto3 is simpler than proto2&lt;/h2&gt; &lt;p&gt;This genuinely surprised me, but it was a very pleasant surprise. When writing protobuf-net, I made a conscious decision to make it &lt;em&gt;easy and natural&lt;/em&gt; to implement the most common scenarios. I &lt;em&gt;supported&lt;/em&gt; the full range of protobuf features, but some of them were more awkward to use. As such, I made some random decisions towards making it simple and obvious to use:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;implicit zero defaults: most people don't have complex default values, where-as this makes it simple and efficient to store "empty" data (in zero bytes) without any configuration&lt;/li&gt;&lt;li&gt;don't worry about implicitly set vs explicitly set values: values are value are values; the library &lt;em&gt;supports&lt;/em&gt; a few common .NET patterns for explicit assignment (&lt;code&gt;ShouldSerialize*&lt;/code&gt; / &lt;code&gt;*Specified&lt;/code&gt; / &lt;code&gt;Nullable&amp;lt;T&amp;gt;&lt;/code&gt; + &lt;code&gt;null&lt;/code&gt;), but it doesn't &lt;em&gt;demand&lt;/em&gt; them and is perfectly fine without them&lt;/li&gt;&lt;li&gt;extensions and unknown data entirely optional: the question here is what to do if the serialized data contains unexpected / unknown values - which could be from external "extensions", or could just be new fields that the code doesn't know about. protobuf-net &lt;em&gt;supports&lt;/em&gt; this type of usage, but accepts that it isn't something that most folks need or even want - they just want to get the &lt;em&gt;expected&lt;/em&gt; data in and out&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;It turns out that proto3 makes some striking &lt;em&gt;omissions&lt;/em&gt; from proto2:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;default values are gone - implicit zero values are assumed and are the &lt;em&gt;only&lt;/em&gt; permitted defaults&lt;/li&gt;&lt;li&gt;explicit assignment is gone - if something has a value other than the zero default, it is serialized, &lt;em&gt;and that's it&lt;/em&gt;&lt;/li&gt;&lt;li&gt;extensions are largely missing&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;A  part of me feels that these changes &lt;em&gt;totally validate&lt;/em&gt; the decisions I made when making protobuf-net as simple to use as possible. Note that protobuf-net still retains full support for the wider set of protobuf features (including all the proto2 features) - they're not going anywhere.&lt;/p&gt; &lt;h2&gt;what about protobuf JSON?&lt;/h2&gt; &lt;p&gt;protobuf 3.0.0 added a well-defined JSON encoding for protobuf data. I confess that I'm deeply conflicted on this. In the .NET world, JSON is a solved problem. If I want my data serialized as JSON, I'm probably going to look at JIL (if I want raw performance) or Json.NET (if I want greater flexibility and range of features, or just want to use the de-facto platform serializer). Since protobuf-net targets idiomatic .NET types that would &lt;em&gt;already&lt;/em&gt; serialize &lt;strong&gt;just fine&lt;/strong&gt; with either of these, it seems to me of very little benefit to spend a large amount of time writing JSON support directly for protobuf-net. As such, protobuf-net still &lt;em&gt;does not support this&lt;/em&gt;. If there is a genuine need for this, the first thing I would do would be to look at JIL or Json.NET to see if there is some combination of configuration options that I can specify that would conveninetly be compatible with the expected JSON encoding. At the very worst case, I could see either some PRs to JIL or a fork of JIL to support it, but frankly I'm going to defer on touching the JSON option until I understand the use-case. On the surface, it &lt;em&gt;seems&lt;/em&gt; like the JSON option here takes all the main reasons for using protobuf and throws them out the window. My reservations here are probably because I'm spoiled by working in a platform where I can take &lt;em&gt;virtually any object&lt;/em&gt;, and both JIL and Json.NET will be able to serialize and deserialize it for me. &lt;/p&gt; &lt;h1&gt;So what do we get in protobuf-net 2.3.0?&lt;/h1&gt; &lt;h2&gt;Brand new protogen tooling for both proto2 and proto3&lt;/h2&gt; &lt;p&gt;This release completely replaces the protogen DSL parsing tool; it has been 100% rewritten from scratch using pure managed code. The old version used to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;shell execute to call Google's "protoc" tool to emit a compiled schema (in the protobuf serialization format, naturally) as a file&lt;/li&gt;&lt;li&gt;then deserialize that file into the corresponding type model using protobuf-net&lt;/li&gt;&lt;li&gt;serialize that same object as xml&lt;/li&gt;&lt;li&gt;run the xml through an xslt 1.0 engine to generate C#&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;This worked, but is a cross-platform nightmare as well as being a maintenance nightmare. I doubt that xslt was a &lt;em&gt;good&lt;/em&gt; choice for codegen even when it was written, but today... just painful. I looked at a range of parsing engines, but ultimately decided on a manual tokenizer and imperative forwards-only parser. It turned out to not be anything like as much work as I had feared, which was nice. In order to have confidence in the parser, I have tested it on every .proto schema I can find, including about 220 schemas that describe a good portion of Google's public API surface. I've tested these against protoc's binary output to ensure that &lt;em&gt;not only&lt;/em&gt; does it parse the files meaningfully, but it produces &lt;em&gt;the exact same bytes&lt;/em&gt; (as a compiled / serialized schema) that protoc produces.&lt;/p&gt; &lt;p&gt;This parser is then tied into a relatively basic codegen system. At the moment this is relatively crude, and is subject to significant change. The good thing is that &lt;em&gt;now that everything is in place&lt;/em&gt;, this can be reworked relatively easily - perhaps to use one of the many templating systems that are available in .NET.&lt;/p&gt; &lt;p&gt;As an illustration of how the parser and codegen are neatly decoupled, &lt;a href="https://twitter.com/RogerAlsing"&gt;Roger Johansson&lt;/a&gt; has also independently converted his &lt;a href="https://github.com/AsynkronIT/protoactor-go"&gt;Proto Actor code&lt;/a&gt; to use protobuf-net's parser rather than protoc, which is great! &lt;a href="https://twitter.com/RogerAlsing/status/871829162218184704"&gt;https://twitter.com/RogerAlsing/status/871829162218184704&lt;/a&gt;. If you want to use the parser and code-generation tools outside of the tools I provide, &lt;a href="https://www.nuget.org/packages/protobuf-net.Reflection/"&gt;protobuf-net.Reflection&lt;/a&gt; may be useful to you.&lt;/p&gt; &lt;h3&gt;How do I use it?&lt;/h3&gt; &lt;p&gt;OK, you have a .proto schema (proto2 or proto3). At the moment, you have 2 options for codegen from protobuf-net:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;compile, build and execute the &lt;code&gt;protogen&lt;/code&gt; command line tool (which deliberately shares command-line switches with Google's &lt;code&gt;protoc&lt;/code&gt; tool)&lt;/li&gt;&lt;li&gt;use &lt;a href="https://protogen.marcgravell.com/"&gt;https://protogen.marcgravell.com/&lt;/a&gt; to do it online&lt;/li&gt;&lt;/ol&gt; &lt;p&gt;(as a 2.1 option you could also clone that same website from git and host it locally; that's totally fine)&lt;/p&gt; &lt;p&gt;I want to introduce much better tooling options, including something that ties into msbuild and dotnet CLI, and (optionally) devenv, but so far this is looking like hard work, so I wanted to ship 2.3.0 before tackling it. It is my opinion that &lt;a href="https://protogen.marcgravell.com/"&gt;https://protogen.marcgravell.com/&lt;/a&gt; is now perhaps the &lt;em&gt;easiest&lt;/em&gt; way to play with .proto schemas - and to show willing, it also includes support for all official protoc output languages, &lt;em&gt;and&lt;/em&gt; includes the entire public Google API surface as readily avaialble imports (those same 220 schemas from before).&lt;/p&gt; &lt;h2&gt;Support for maps&lt;/h2&gt; &lt;p&gt;Maps (&lt;code&gt;map&amp;lt;key_type, value_type&amp;gt;&lt;/code&gt;) in .proto are the equivalent of dictionaries in .NET. If you're familiar with protobuf-net, you'll know that it has offered dictionary support &lt;em&gt;for many years&lt;/em&gt;. Fortunately, Google's idea of how this should be implemented matches perfectly with the arbitrary and unilateral decisions I stumbled into, so maps are 99.95% interchangeable with how protobuf-net already handles dictionaries. The 0.05% relates to what happens with duplicate keys. Basically: historically, protobuf-net used &lt;code&gt;theData.Add(key, value)&lt;/code&gt;, which would throw if a key was duplicated. However, maps are defined such as the last value &lt;em&gt;replaces&lt;/em&gt; previous values - so: &lt;code&gt;theData[key] = value;&lt;/code&gt;. This is a very small difference, and doesn't impact any data that would &lt;em&gt;currently successfully deserialize&lt;/em&gt;, so I've made the executive decision that from 2.3.0 all dictionaries should follow the "map" rules by default (when appropriate). To allow full control, protobuf-net has a new &lt;code&gt;ProtoMapAttribute&lt;/code&gt; (&lt;code&gt;[ProtoMap]&lt;/code&gt;). This has options to use the old &lt;code&gt;.Add&lt;/code&gt; behavior, and also has options to control the sub-format used for the key and value. The protogen tool will always include the appropriate &lt;code&gt;[ProtoMap]&lt;/code&gt; options for your data.&lt;/p&gt; &lt;h2&gt;Support for &lt;code&gt;Timestamp&lt;/code&gt; and &lt;code&gt;Duration&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;&lt;code&gt;Timestamp&lt;/code&gt; and &lt;code&gt;Duration&lt;/code&gt; refer to a point in time (think: &lt;code&gt;DateTime&lt;/code&gt;) and an &lt;em&gt;amount&lt;/em&gt; of time (think: &lt;code&gt;TimeSpan&lt;/code&gt;). Again, protobuf-net has had support for &lt;code&gt;DateTime&lt;/code&gt; and &lt;code&gt;TimeSpan&lt;/code&gt; &lt;em&gt;for many years&lt;/em&gt;, but this time my arbitrary interpretation and Google's differs significantly. I have added native support for these formats, but because it is different to (and fundamentally incompattible with) what protobuf-net has done historically, this has to be done on an opt-in basis. I've added a new &lt;code&gt;DataFormat.WellKnown&lt;/code&gt; option that indicates that you want to use these formats. For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ProtoMember(7, DataFormat = DataFormat.WellKnown)]&lt;br /&gt;pubic DateTime CreationDate {get; set;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;will be serialized as a &lt;code&gt;Timestamp&lt;/code&gt;. The protogen tool recognises &lt;code&gt;Timestamp&lt;/code&gt; and &lt;code&gt;Duration&lt;/code&gt; and will emit the appropriate options.&lt;/p&gt; &lt;h2&gt;Simpler &lt;code&gt;enum&lt;/code&gt; handling&lt;/h2&gt; &lt;p&gt;Historically, enums in .proto were a bit awkward when it came to unknown values, and protobuf-net defaulted to the most paranoid options of panicking if it saw a value it didn't explicitly expect. However, the guidance now includes the new remark:&lt;/p&gt; &lt;blockquote&gt;  &lt;p&gt;During deserialization, unrecognized enum values will be preserved in the message, though how this is represented when the message is deserialized is language-dependent. In languages that support open enum types with values outside the range of specified symbols, such as C++ and Go, the unknown enum value is simply stored as its underlying integer representation.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Enums in .NET are open enum types, so it makes sense to relax the handling here. Additionally, historically protobuf-net didn't &lt;em&gt;really&lt;/em&gt; properly implelemt the older "make it available as an extension value" approach from proto2 (it would throw an exception instead) - far from ideal. So: from 2.3.0 onwards, all enums will be (by default) interpreted directly and without checking against expected values, &lt;em&gt;with the exception&lt;/em&gt; of the unusual scenario where &lt;code&gt;[ProtoEnum(Value=...)]&lt;/code&gt; has been used to &lt;em&gt;re-map&lt;/em&gt; any enum such that the serialized value is different to the natural value. In this case, it can't assume that a direct interpretation will be valid, so the legacy checks will remain. Emphasis: this is a very rare scenario, and probably won't impact anyone except me (and my test suite). Because of this, the &lt;code&gt;[ProtoContract(EnumPassthru = ...)]&lt;/code&gt; option is now &lt;em&gt;mostly&lt;/em&gt; redundant: the only time it is useful is to explicitly set this to &lt;code&gt;false&lt;/code&gt; to revert to the previous "throw an exception" behaviour.&lt;/p&gt; &lt;h2&gt;Discriminated unions, aka one-of&lt;/h2&gt; &lt;p&gt;One of the features introduced in proto3 (and back-ported to proto2) is the ability for multiple fields to overlap such that only one of them can contain a value at a time. The ideal in-memory representation of this is a discriminated union, which C# can't really represent &lt;i&gt;directly&lt;/i&gt;, but which can be simulated via a &lt;c&gt;struct&lt;/c&gt; with explicit layout; so that's exactly what we now do! A family of discriminated union structs have been introduced for this purpose,  and are &lt;i&gt;mainly&lt;/i&gt; intended to be used with generated code. But if you want to use them directly: have fun!&lt;/p&gt; &lt;h2&gt;proto3 schema generation&lt;/h2&gt; &lt;p&gt;Since the DSL tools accept proto2 or proto3 syntax, it makes sense that we should be able to &lt;em&gt;emit&lt;/em&gt; both proto2 and proto3 syntax, so there are now overloads of &lt;code&gt;GetSchema&lt;/code&gt; / &lt;code&gt;GetProto&amp;lt;T&amp;gt;&lt;/code&gt; that allow this. These tools have also been updated to be aware of maps, &lt;code&gt;Timestamp&lt;/code&gt;, &lt;code&gt;Duration&lt;/code&gt; etc. &lt;/p&gt; &lt;h2&gt;New custom option DSL support&lt;/h2&gt; &lt;p&gt;The new DSL tooling makes use of the "extensions" feature to add custom syntax options to your .proto files. At the moment the options here are &lt;a href="https://raw.githubusercontent.com/mgravell/protobuf-net/master/src/protogen.site/wwwroot/protoc/protobuf-net/protogen.proto"&gt;pretty limited&lt;/a&gt;, allowing you to control the accessibility and naming of elements, but as new controls becomes necessary: that's where they will go. &lt;/p&gt;&lt;h2&gt;General bug fixes&lt;/h2&gt;&lt;p&gt;This build also includes a range of more general fixes for specific scenarios, as covered by the &lt;a href="http://mgravell.github.io/protobuf-net/releasenotes"&gt;release notes&lt;/a&gt;&lt;/p&gt;  &lt;h1&gt;What next?&lt;/h1&gt; &lt;p&gt;I'm keeping a basic future roadmap on the &lt;a href="https://mgravell.github.io/protobuf-net/releasenotes"&gt;release notes&lt;/a&gt;. There are some significant pieces of work ahead, including (almost certainly) a major rework of the core serializer to support &lt;code&gt;async&lt;/code&gt; IO, "Pipelines", etc. I also want to improve the buid-time tooling. My work here is very much not done.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/mR0CWxetqb0" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/213552646711780746" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/213552646711780746" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/mR0CWxetqb0/protobuf-net-gets-proto3-support.html" title="protobuf-net gets proto3 support" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/06/protobuf-net-gets-proto3-support.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-1083904238598386013</id><published>2017-05-17T13:43:00.000-07:00</published><updated>2017-05-18T06:29:32.540-07:00</updated><title type="text">protobuf-net: large data, and the future</title><content type="html">&lt;h1&gt;protobuf-net was born into a different world&lt;/h1&gt;&lt;p&gt;On Jul 17, 2008 I pushed &lt;a href="https://code.google.com/archive/p/protobuf-net/source/default/commits?page=7"&gt;the first commits of protobuf-net&lt;/a&gt;. It is easy to forget, but &lt;em&gt;back then&lt;/em&gt;, most machines had access to a lot less memory than they do today, with x86 still being a common choice, meaning that 2GB user space (or maybe a little more if you fancied fighting with /3GB+LAA) was a hard upper limit. In reality, your usable memory was much less. Processors were much less powerful - user desktops were doing well if their single core had hyper-threading support (dual and quad cores existed, but were much rarer).&lt;/p&gt;&lt;h1&gt;&lt;a id="user-content-thanks-for-the-2gb-memories" class="anchor" href="#thanks-for-the-2gb-memories" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thanks for the 2GB memories&lt;/h1&gt;&lt;p&gt;It is in this context that protobuf-net was born, and in which many of the early design decisions were made. Although to be fair, even Google (who designed the thing) suggested an upper bound in the low hundreds of MB. Here's the original author (Kenton Varda) saying on Stack Overflow &lt;a href="http://stackoverflow.com/questions/34128872/google-protobuf-maximum-size"&gt;that 10MB is "pushing it"&lt;/a&gt; - although he does also note that 1GB works, but that 2GB is a hard limit.&lt;/p&gt;&lt;p&gt;protobuf-net took these limitations on board, and many aspects of the code could only work inside these borders. In particular, one of the key design questions in protobuf-net was how, when serializing general purpose objects, to handle the length prefix.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-protobuf-strings" class="anchor" href="#protobuf-strings" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;protobuf strings&lt;/h2&gt;&lt;p&gt;Protobuf is actually a relatively simple binary format; it has few primitives, one of which is the length-prefixed string (where "string" means "arbitrary payload", not just text). The encoding of this is a &lt;em&gt;variable length&lt;/em&gt; "varint" that tells it how many bytes are involved, then &lt;em&gt;that many bytes&lt;/em&gt; of the payload:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[field x, "string"]&lt;br /&gt;[n, 1-10 bytes]&lt;br /&gt;[payload, n bytes]&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The requirement to know the length in advance is fine for the Google implementation - as I understand it, the "builder" approach means that the length is calculated when the "builder" creates the actual object, which is long before serialization happens (note: I'm happy to be corrected here if I've misunderstood). But protobuf-net doesn't work with "builder" types; it works against gereral every-day POCOs - usually written without any DSL schema ("code-first"). We can't rely on any construction-time calculations. So: how to write the length?&lt;/p&gt;&lt;p&gt;Essentially, there's two ways of doing this:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;serialize the data &lt;em&gt;&lt;strong&gt;first&lt;/strong&gt;&lt;/em&gt; (perhaps hoping that the length prefix will fit in a single byte, and leaving a space for it); when you've finished serializing, you know the length - so now backfill that into the original space, which might mean nudging the data over a bit if the prefix took more space than expected&lt;/li&gt;&lt;li&gt;&lt;em&gt;compute&lt;/em&gt; the actual required length, write the prefix, &lt;em&gt;then&lt;/em&gt; serialize the data&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both have advantages and disadvantages. The first requires you to buffer all the data in the payload (you can't flush something that you might need to update later), and might need us to move a lot of data. The second requires us to do more thinking without actually writing anything - which might mean doing a lot of work twice.&lt;/p&gt;&lt;p&gt;At the current time, protobuf-net chooses the first approach. For quite a lot of small leaf types, this doesn't actually mean much more than backfilling a single byte of length data, but it becomes progressively more expensive as the payload size increases.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-i-hate-limits" class="anchor" href="#i-hate-limits" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I hate limits&lt;/h2&gt;&lt;p&gt;Over the time since then, I have seen &lt;em&gt;many, many&lt;/em&gt; requests from people asking for protobuf-net to support larger data sizes - at least an order of magnitude above what has previously been usable, tens of GB or more, which makes perfect sense when you consider the data that some apps load into the plentiful RAM available on even a mid-range server. In &lt;em&gt;principle&lt;/em&gt; this is simple (mostly making sure that the reader and writer use 64-bit tracking internally), but there are 2 stumbling blocks:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the need to buffer vast quantities of data would demand excessive amounts of RAM&lt;/li&gt;&lt;li&gt;the current buffer implementation woud be prohibitively hard to refactor to go above 2GB&lt;/li&gt;&lt;li&gt;even if we did, it would then take a &lt;em&gt;loooong&lt;/em&gt; time to output the buffered data after backfilling&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I've recently pushed some commits intended to address the 64-bit reader/writer issue - unblocking some users, but the other factors are much harder to solve in the current implementation.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-wait-how-does-that-unblock-anyone" class="anchor" href="#wait-how-does-that-unblock-anyone" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wait... how does that unblock anyone?&lt;/h2&gt;&lt;p&gt;Good catch; indeed, simply enabling 64-bit readers and writers doesn't fix the buffering problem - but: there is a workaround. A long time in protobuf's past, there were two ways of encoding sub-messages. One was the length-prefixed string that we've discussed; the other was the "group". At the binary level, the difference is that "groups" don't have a length prefix - instead a sentinel value &lt;em&gt;suffix&lt;/em&gt; is used to denote the end of the message:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[field x, "start group"]&lt;br /&gt;[payload]&lt;br /&gt;[field x, "end group"]&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(the protocol itself means that "end group" could not occur as an immediate child of the payload, so this is unambiguous)&lt;/p&gt;&lt;p&gt;As with most things, this has various advantages and disadvantages - but most significantly in our case here, it means we &lt;em&gt;don't need to know the length in advance&lt;/em&gt;. And if we don't need to know the length, then we don't need to &lt;em&gt;buffer&lt;/em&gt; anything - we can write the data in a purely forwards direction without any need to backfill data. There's just one problem: it is out of favor with the protobuf specification owners - it was marked as deprecated but supported in the proto2 DSL, and there is no syntax for it &lt;em&gt;at all&lt;/em&gt; in the proto3 DSL (these all just describe data against the same binary format).&lt;/p&gt;&lt;p&gt;But: I &lt;em&gt;really, really like groups&lt;/em&gt;, at least at the binary format level. Essentially, the current 2GB+ unblocking in an upcoming deploy of protobuf-net is limited to scenarios where it is possible to use groups &lt;em&gt;extensively&lt;/em&gt;. The closer something is to being a leaf, the more it'll be OK to use length-prefixed strings; the closer something is to the root object, the more it will benefit from being treated as a "group". With this removing the need to buffer+backfill, arbitrarily large files can be produced. The cost, however, is that you won't be able to interop with data that is expressed as proto3 schemas.&lt;/p&gt;&lt;p&gt;Historically, you have been able to indicate that a &lt;em&gt;member&lt;/em&gt; should be treated as a group via:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// for field number "n"&lt;br /&gt;[ProtoMember(n, DataFormat = DataFormat.Group)]&lt;br /&gt;public SomeType MemberName { get; set; }&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However, this is hard to express in some cases (such as dictionaries), so this has been &lt;em&gt;extended&lt;/em&gt; to allow declaration at the type-level:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[ProtoContract(IsGroup = true)]&lt;br /&gt;public class SomeType {...}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(both of which can also be expressed via the &lt;code&gt;RuntimeTypeModel&lt;/code&gt; API for runtime configuration)&lt;/p&gt;&lt;p&gt;These changes move us forward, at least - but are mainly appropriate when using protobuf-net as the only piece of the puzzle, since it simply cannot be expressed in the proto3 DSL.&lt;/p&gt;&lt;h2&gt;&lt;a id="user-content-the-future" class="anchor" href="#the-future" aria-hidden="true"&gt;&lt;svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The future&lt;/h2&gt;&lt;p&gt;This is all great, but isn't ideal. So &lt;em&gt;in parallel&lt;/em&gt; with that, I have some work-in-progress early-stages work that is taking a much more aggressive look at the future of protobuf-net and what it needs to move forward. I have many lofty aims on the list:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;true 2GB+ support including length-prefix, achieved by a redesign of the writer API, including switching to precalculation of lengths as required&lt;/li&gt;&lt;li&gt;optimized support for heterogeneous backend targets, including in-memory serialization, &lt;code&gt;Stream&lt;/code&gt;s, "Channels" (the experimental redesign of the .NET IO stack), memory-mapped-files, etc&lt;/li&gt;&lt;li&gt;making use of new concepts like &lt;code&gt;Utf8String&lt;/code&gt;, &lt;code&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; where appropriate&lt;/li&gt;&lt;li&gt;full support for &lt;code&gt;async&lt;/code&gt; backend targets, making optimal use of &lt;code&gt;ValueTask&amp;lt;T&amp;gt;&lt;/code&gt; as appropriate so that performance is retained in the case where it is possible to complete entirely synchronously&lt;/li&gt;&lt;li&gt;rework of the codegen / meta-programming layer, reducing or removing the dependency on IL-emit, and moving more towards compile-time code-gen (ideally fully automated and silent) using Roslyn&lt;/li&gt;&lt;li&gt;in doing so, greatly improve the experience for AOT scenarios, where meta-programming is restricted or impossible&lt;/li&gt;&lt;li&gt;improve the performance of a range of common scenarios by every mechanism imaginable&lt;/li&gt;&lt;li&gt;and maybe, just maybe: getting around to implementing updated DSL parsing tooling (but realistically: that isn't the key selling-point of protobuf-net)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As counterpoints, I &lt;em&gt;also&lt;/em&gt; imagine that I'll be dropping support for everything that isn't either ".NET Framework recent-enough to build via &lt;code&gt;dotnet build&lt;/code&gt;" (4.0 and avove, IIRC) or ".NET Standard (something)". The reality is that I'm not in a position to support some obscure PCL configuration or an ancient version of Silverlight. If you can make it compile: great! I'm also entirely open to including targets for things like Xamarin or Unity as long as &lt;em&gt;somebody else&lt;/em&gt; can make them work in the build - I'm simply not a user of those tools, and it would be artificial to say that I've seen it work. I'm also moving away from my historic aim of being able to compile on down-level compiler versions. These days, with NuGet as the de-facto package manager, and &lt;code&gt;dotnet build&lt;/code&gt; readily available, and the free &lt;a href="https://www.visualstudio.com/vs/community/"&gt;Visual Studio Community&lt;/a&gt; edition, I'm not sure it makes sense to worry about old compilers.&lt;/p&gt;&lt;p&gt;As you can see, there's a lot in the planning. I've been experimenting with various pieces of it to see how it fits together, and I'm confident that I see a viable route forward. Now all I need is to make it happen.&lt;/p&gt;&lt;p&gt;The first step there is to get the "longification" changes shipped; this has now seen real-world usage, so it is just some packaging work to do. I hope to have that available on NuGet before next week.&lt;/p&gt;&lt;p&gt;Fun times!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/GM1EeiZyY9g" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/1083904238598386013" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/1083904238598386013" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/GM1EeiZyY9g/protobuf-net-large-data-and-future.html" title="protobuf-net: large data, and the future" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/05/protobuf-net-large-data-and-future.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-6080378115277241886</id><published>2017-04-29T11:27:00.003-07:00</published><updated>2017-04-29T11:27:59.783-07:00</updated><title type="text">StackExchange.Redis and Redis 4.0 Modules</title><content type="html">&lt;h2&gt;StackExchange.Redis and Redis Modules&lt;/h2&gt;&lt;p&gt;This is largely a brain-dump of my plans for Redis 4.0 Modules and the StackExchange.Redis client library.&lt;/p&gt;&lt;p&gt;Redis 4.0 &lt;a href="https://raw.githubusercontent.com/antirez/redis/4.0/00-RELEASENOTES"&gt;is in RC 3&lt;/a&gt;, which is great for folks interested in Redis. As the primary maintainer of &lt;a href="https://www.nuget.org/packages/StackExchange.Redis/"&gt;StackExchange.Redis&lt;/a&gt;, new releases also bring me some extra work in terms of checking whether there are new features that I need to incorporate into the client library. Some client libraries expose a very raw API surface, leaving the individual commands etc to the caller - this has the advantagee of simplicity, but it has disadvantages too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it presents a higher barrier to entry, as users need to learn the redis command semantics&lt;/li&gt;&lt;li&gt;it prevents the library offering any special-case optimizations or guidance&lt;/li&gt;&lt;li&gt;it makes it hard to ensure that key-based sharding is being implemented correctly (as to do that you need to know &lt;em&gt;with certainty&lt;/em&gt; which tokens are &lt;em&gt;keys&lt;/em&gt; vs &lt;em&gt;values&lt;/em&gt; vs &lt;em&gt;command semantics&lt;/em&gt;)&lt;/li&gt;&lt;li&gt;it is hard to optimize the API&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For all these reasons, StackExchange.Redis has historically offered a more method-per-command experience, allowing full intellisense, identification of &lt;em&gt;keys&lt;/em&gt;, helper enums for options, santity checking of operands, and various scenario-specific optimizations / fallback strategies. And ... if that isn't enough, you can always use hack by using Lua to do things at the server directly.&lt;/p&gt;&lt;h2&gt;Along comes Modules&lt;/h2&gt;&lt;p&gt;A key feature in Redis 4.0 is the introduction of &lt;em&gt;modules&lt;/em&gt;. This allows &lt;em&gt;anyone&lt;/em&gt; to write a module that does something interesting and useful that they want to run &lt;em&gt;inside&lt;/em&gt; Redis, and load that module into their Redis server - then invoke it using whatever exotic commands they choose. If you're interested in Redis, you should go check it out! There's already a &lt;a href="http://redismodules.com/"&gt;gallery of useful modules started by Redis Labs&lt;/a&gt; - things like JSON support, Machine Learning, or Search - with an option to submit your own modules to the community.&lt;/p&gt;&lt;p&gt;Clearly, my old approach of "manually update the API when new releases come out" doesn't scale to the advent of modules, and saying "use Lua to run them" is ... ungainly. We need a different approach.&lt;/p&gt;&lt;h2&gt;Adding &lt;code&gt;Execute&lt;/code&gt; / &lt;code&gt;ExecuteAsync&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;As a result, in an upcoming (not yet released) version, the plan is to add some new methods to StackExchange.Redis to allow more direct and raw access to the pipe; for example the &lt;code&gt;rejson&lt;/code&gt; module adds a &lt;code&gt;JSON.GET&lt;/code&gt; command that takes a key to an existing JSON value, and a path inside that json - we can invoke this via:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;string foo = (string)db.Execute(&lt;br /&gt;    "JSON.GET", key, "[1].foo");&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(there's a similar &lt;code&gt;ExecuteAsync&lt;/code&gt; method)&lt;/p&gt;&lt;p&gt;The return value of these methods is the flexible &lt;code&gt;RedisResult&lt;/code&gt; type that the Lua API already exposes, which handles all the expected scenarios of primitives, arrays, etc. The parameters are simply a &lt;code&gt;string&lt;/code&gt; command name, and a &lt;code&gt;params object[]&lt;/code&gt; of &lt;em&gt;everything else&lt;/em&gt; - with appropriate handling of the types you're likely to use with redis commands (&lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;, etc). It also recognises parameters &lt;em&gt;typed as &lt;code&gt;RedisKey&lt;/code&gt;&lt;/em&gt; and uses them for routing / sharding purposes as necessary.&lt;/p&gt;&lt;p&gt;The key from all of this is that it should be easy to quickly hook into any modules that you write or want to consume.&lt;/p&gt;&lt;h2&gt;What about more graceful handling for well-known modules?&lt;/h2&gt;&lt;p&gt;My hope here is that or &lt;em&gt;well-known&lt;/em&gt; but non-trivial modules, "someone" (maybe me, maybe the wider community) will be able to write helper methods &lt;em&gt;as C# extension methods&lt;/em&gt; against the client library, and package them as module-specific NuGet packages; for example, a package could add:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;public static RedisValue JsonGet(this IDatabase db, RedisKey key,&lt;br /&gt;    string path = ".", CommandFlags flags = CommandFlags.None)&lt;br /&gt;{&lt;br /&gt;    return (RedisValue)db.Execute("JSON.GET",&lt;br /&gt;        new object[] { key, path }, flags);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to expose &lt;em&gt;raw&lt;/em&gt; json functionality, or could choose to add serialization / deserialization into the mix too:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;public static T JsonGet&amp;lt;T&amp;gt;(this IDatabase db, RedisKey key,&lt;br /&gt;    string path = ".", CommandFlags flags = CommandFlags.None)&lt;br /&gt;{&lt;br /&gt;    byte[] bytes = (byte[])db.Execute("JSON.GET",&lt;br /&gt;        new object[] { key, path }, flags);&lt;br /&gt;    using (var ms = new MemoryStream(bytes))&lt;br /&gt;    {&lt;br /&gt;        return SomeJsonSerializer.Deserialize&amp;lt;T&amp;gt;(ms);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;em&gt;slight&lt;/em&gt; wrinkle here is that it is still using the &lt;code&gt;Execute[Async]&lt;/code&gt; API; as a general-purpose API it is very &lt;em&gt;convenient and flexible&lt;/em&gt;, but slightly more expensive than it &lt;em&gt;absolutely needs to be&lt;/em&gt;. But being realistic, it is probably fine for 95% of use-cases, so: let's get that shipped and iterate from there.&lt;/p&gt;&lt;p&gt;I'd &lt;em&gt;like&lt;/em&gt; to add a second API specifically intended for extensions like this (more direct, less allocations, etc), but a: ideally I'd want to ensure that I can subsequently tie it cleanly into the "pipelines" concept (which is currently just a corefxlab dream, without a known ETA for "real" .NET), and b: it would be good to gauge interest and uptake before spending any time doing this.&lt;/p&gt;&lt;h2&gt;But what should consumers target?&lt;/h2&gt;&lt;p&gt;This &lt;em&gt;also&lt;/em&gt; makes "strong naming" rear it's ugly head. I'm not going to opine on strong naming here - the discussion is not very interesting and has been done to death. Tl,dr: currently, there are two packages for the client library - strong named and not strong named. It would be sucky if there was a mix of external extensions targeting one, the other, or both. The mid range plan is to make a &lt;strong&gt;breaking package change&lt;/strong&gt; and re-deploy StackExchange.Redis (which currently is not strong-named) as: strong-named. The StackExchange.Redis.StrongName would be &lt;em&gt;essentially&lt;/em&gt; retired, although I guess it could be an empty package with a StackExchange.Redis dependency for convenience purposes, possibly populated entirely by &lt;a href="https://msdn.microsoft.com/en-us/library/system.runtime.compilerservices.typeforwardedtoattribute(v=vs.110).aspx"&gt;&lt;code&gt;[assembly:TypeForwardedTo(...)]&lt;/code&gt;&lt;/a&gt; markers. I'm open to better ideas, of course!&lt;/p&gt;&lt;h2&gt;So that's "The Plan"&lt;/h2&gt;&lt;p&gt;If you have strong views, &lt;a href="https://twitter.com/marcgravell"&gt;hit me on twitter (@marcgravell)&lt;/a&gt;, or &lt;a href="https://github.com/StackExchange/StackExchange.Redis/"&gt;log an issue&lt;/a&gt; and we can discuss it.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/x56TnpONeBg" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6080378115277241886" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6080378115277241886" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/x56TnpONeBg/stackexchangeredis-and-redis-40-modules.html" title="StackExchange.Redis and Redis 4.0 Modules" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/04/stackexchangeredis-and-redis-40-modules.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-7250378045006547475</id><published>2017-04-23T15:40:00.001-07:00</published><updated>2017-05-05T01:47:05.278-07:00</updated><title type="text">Spans and ref part 2 : spans</title><content type="html">       &lt;h1 id="spans-and-ref-part-2--spans"&gt;Spans and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; part 2 : spans&lt;/h1&gt; &lt;p&gt;In &lt;a href="http://blog.marcgravell.com/2017/04/spans-and-ref-part-1-ref.html"&gt;part 1&lt;/a&gt;, we looked at &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals and &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;, and hinted at a connection to “spans”; this time we’re going to take a deeper look at what this connection might be, and how we can use make use of it.&lt;/p&gt; &lt;h2 id="disclaimer"&gt;Disclaimer&lt;/h2&gt; &lt;p&gt;I’m &lt;em&gt;mostly&lt;/em&gt; on the outside of this - looking in at the public artefacts, playing with the API etc - maybe the odd PR or issue report. It is entirely possible that I’ve misunderstood some things, and it is possible that things will change between now and general availability.&lt;/p&gt; &lt;h2 id="what-are-spans"&gt;What are spans?&lt;/h2&gt; &lt;p&gt;By spans, I mean &lt;code class="highlighter-rouge"&gt;System.Span&amp;lt;T&amp;gt;&lt;/code&gt;, which is part of .NET Core, living in the &lt;code class="highlighter-rouge"&gt;System.Memory&lt;/code&gt; assembly. It is also available for .NET via the &lt;code class="highlighter-rouge"&gt;System.Memory&lt;/code&gt; package. But please note: &lt;em&gt;it is a loaded gun to use at the moment&lt;/em&gt; - you can currently compile code that has &lt;strong&gt;undefined behavior&lt;/strong&gt;, and which &lt;strong&gt;may not&lt;/strong&gt; compile at some point in the future. Although to be fair, to get into any of the terrible scenarios you need to use the &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; keyword, at which point you already said “I take full responsibility for everything that goes wrong here”. I’ll discuss this more below, but I wanted to mention that at the top in case you stop reading and don’t get to that important point.&lt;/p&gt; &lt;p&gt;Note that some of the code in this post uses unreleased features; I’m using:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&amp;lt;PackageReference Include="System.Memory"&lt;br /&gt;    Version="4.4.0-preview1-25219-04" /&amp;gt;&lt;br /&gt;&amp;lt;PackageReference Include="System.Runtime.CompilerServices.Unsafe"&lt;br /&gt;    Version="4.4.0-preview1-25219-04" /&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Obviously &lt;a href="http://idioms.thefreedictionary.com/all+bets+are+off"&gt;all bets are off&lt;/a&gt; with preview code; things may change.&lt;/p&gt; &lt;h2 id="why-do-spans-need-to-exist"&gt;Why do spans need to exist?&lt;/h2&gt; &lt;p&gt;We &lt;a href="http://blog.marcgravell.com/2017/04/spans-and-ref-part-1-ref.html"&gt;saw previously&lt;/a&gt; how &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; can be used similarly to pointers (&lt;code class="highlighter-rouge"&gt;T*&lt;/code&gt;) to represent a reference to a single value. Basically, anything that allows us to talk about complex scenarios without needing pointers is a good thing. But: representing a &lt;em&gt;single&lt;/em&gt; value is not the only use-case of pointers. The &lt;em&gt;much more common&lt;/em&gt; scenario for pointers is for talking about a &lt;em&gt;range&lt;/em&gt; of contiguous data, usually when paired with a count of the elements.&lt;/p&gt; &lt;p&gt;At the most basic level, a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; represents a strongly typed contiguous chunk of elements of type &lt;code class="highlighter-rouge"&gt;T&lt;/code&gt; with a known and enforced length. In many ways, very comparable to an array (&lt;code class="highlighter-rouge"&gt;T[]&lt;/code&gt;) or segment &lt;code class="highlighter-rouge"&gt;ArraySegment&amp;lt;T&amp;gt;&lt;/code&gt;) - but… more. They also provide &lt;em&gt;safe&lt;/em&gt; (by which I mean: not &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; in the C# sense) access to features that would previously have required pointers (&lt;code class="highlighter-rouge"&gt;T*&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;I’m probably missing a few things here, but the most immediate features are:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;provide a unified type system over all contiguous memory, including: arrays, unmanaged pointers, stack pointers, fixed / pinned pointers to managed data, and references into the interior of values&lt;/li&gt;  &lt;li&gt;allow type coercion for primitives and value-types&lt;/li&gt;  &lt;li&gt;work with generics (unlike pointers, which don’t)&lt;/li&gt;  &lt;li&gt;respect garbage collection (GC) semantics by using &lt;em&gt;references&lt;/em&gt; instead of &lt;em&gt;pointers&lt;/em&gt; (the GC only walks references)&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Now: if none of the above sounds like things you ever need to do, then great: you probably won’t ever need to use &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; - and that’s perfectly OK. Most &lt;em&gt;application&lt;/em&gt; code will never need to use these features. Ultimately, these tools are designed for &lt;em&gt;lower level&lt;/em&gt; code (usually: library code) that is performance critical. That said, there &lt;em&gt;are&lt;/em&gt; some great uses in regular code, that we’ll get onto.&lt;/p&gt; &lt;h2 id="but-what-is-a-span"&gt;But… what is a span?&lt;/h2&gt; &lt;p&gt;OK, OK. &lt;em&gt;Conceptually&lt;/em&gt;, a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; can be thought of as a reference and a length:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public struct Span&amp;lt;T&amp;gt; {&lt;br /&gt;    ref T _reference;&lt;br /&gt;    int _length;&lt;br /&gt;    public ref T this[int index] { get {...} }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;with a cousin:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public struct ReadOnlySpan&amp;lt;T&amp;gt; {&lt;br /&gt;    ref T _reference;&lt;br /&gt;    int _length;&lt;br /&gt;    public T this[int index] { get {...} }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;You would be perfectly correct to complain “but… but… in the last part you said no &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; fields!”. That’s fair, but I did say &lt;em&gt;conceptually&lt;/em&gt;. At least… for now!&lt;/p&gt; &lt;h2 id="spans-as-ranges-of-an-array"&gt;Spans as ranges of an array&lt;/h2&gt; &lt;p&gt;As a completely trivial (and rather pointless) example, we can see how we can use a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; very similarly to how we might have used a &lt;code class="highlighter-rouge"&gt;T[]&lt;/code&gt;:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void ArrayExample() {&lt;br /&gt;    byte[] data = new byte[1024];&lt;br /&gt;    // not shown: populate data&lt;br /&gt;    ProcessData(data);&lt;br /&gt;}&lt;br /&gt;void ProcessData(Span&amp;lt;byte&amp;gt; span) {&lt;br /&gt;    for (int i = 0; i &amp;lt; span.Length; i++) {&lt;br /&gt;        DoSomething(span[i]);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Here we implicitly convert the &lt;code class="highlighter-rouge"&gt;byte[]&lt;/code&gt; to &lt;code class="highlighter-rouge"&gt;Span&amp;lt;byte&amp;gt;&lt;/code&gt; when calling the method, but at this point you would still be justified in being underwhelmed - we could have done everything here with just an array.&lt;/p&gt; &lt;p&gt;Similarly, we can talk about just a &lt;em&gt;portion&lt;/em&gt; of the array:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void ArrayExample() {&lt;br /&gt;    byte[] data = new byte[1024];&lt;br /&gt;    // not shown: populate data&lt;br /&gt;    ProcessData(new Span&amp;lt;byte&amp;gt;(data, 10, 512));&lt;br /&gt;}&lt;br /&gt;void ProcessData(Span&amp;lt;byte&amp;gt; span) {&lt;br /&gt;    for (int i = 0; i &amp;lt; span.Length; i++) {&lt;br /&gt;        DoSomething(span[i]);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;And again you could observe that we could have just used &lt;code class="highlighter-rouge"&gt;ArraySegment&amp;lt;T&amp;gt;&lt;/code&gt;. Actually, let’s be realistic: very few people use &lt;code class="highlighter-rouge"&gt;ArraySegment&amp;lt;T&amp;gt;&lt;/code&gt; - but we could have just passed &lt;code class="highlighter-rouge"&gt;int offset&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;int count&lt;/code&gt; as additional parameters, it would have worked fine. But I mentioned pointers earlier…&lt;/p&gt; &lt;h2 id="spans-as-ranges-of-pointers"&gt;Spans as ranges of pointers&lt;/h2&gt; &lt;p&gt;The &lt;em&gt;second&lt;/em&gt; way we can use &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is &lt;em&gt;over a pointer&lt;/em&gt;; which could be any of:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;a &lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; pointer for a small value that we want to work on without allocating an array&lt;/li&gt;  &lt;li&gt;a managed array that we previously &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt;&lt;/li&gt;  &lt;li&gt;a managed array that we previously pinned with &lt;code class="highlighter-rouge"&gt;GCHandle.Alloc&lt;/code&gt;&lt;/li&gt;  &lt;li&gt;a fixed-sized buffer that we previously &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt;&lt;/li&gt;  &lt;li&gt;the contents of a &lt;code class="highlighter-rouge"&gt;string&lt;/code&gt; that we previously &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt;&lt;/li&gt;  &lt;li&gt;a &lt;em&gt;coerced&lt;/em&gt; pointer from any of the above (I’ll explain what this means below)&lt;/li&gt;  &lt;li&gt;a chunk of unmanaged memory obtained with &lt;code class="highlighter-rouge"&gt;Marshal.AllocHGlobal&lt;/code&gt; or any other unmanaged memory API&lt;/li&gt;  &lt;li&gt;etc&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;All of these will necessarily involve &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;, but: we’ll tread carefully! Let’s have a look at a &lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; example (&lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; is where you obtain a chunk of data directly on the call-stack):&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void StackAllocExample() {&lt;br /&gt;    unsafe {&lt;br /&gt;        byte* data = stackalloc byte[128];&lt;br /&gt;        var span = new Span&amp;lt;byte&amp;gt;(data, 128);&lt;br /&gt;        // not shown: populate data / span&lt;br /&gt;        ProcessData(span);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;void ProcessData(Span&amp;lt;byte&amp;gt; span) {&lt;br /&gt;    for (int i = 0; i &amp;lt; span.Length; i++) {&lt;br /&gt;        DoSomething(span[i]);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;That’s… actually pretty huge! We just used the exact same processing code to handle an array and a pointer, and we didn’t need to use &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; (except in the code that initially obtained the pointer). This opens up a &lt;em&gt;huge&lt;/em&gt; range of possibilities, especially for things like network IO and serialization. Even better, it means that we can do all of the above with a “zero copy” mentality: rather than having managed code writing to a &lt;code class="highlighter-rouge"&gt;byte[]&lt;/code&gt; that later gets copied to some unmanaged chunk (for whatever IO we need), we can write &lt;em&gt;directly&lt;/em&gt; to the unmanaged memory via a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2 id="slice-and-dice"&gt;Slice and dice&lt;/h2&gt; &lt;p&gt;A very common scenario when working with buffers and buffer segments is the need to sub-divide the buffer. &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; makes this easy via the &lt;code class="highlighter-rouge"&gt;Slice()&lt;/code&gt; method, best illustrated by an example:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void ProcessData(Span&amp;lt;byte&amp;gt; span) {&lt;br /&gt;    while(span.Length &amp;gt; 0) {&lt;br /&gt;        // first byte is single-byte length-prefix&lt;br /&gt;        int len = span[0];&lt;br /&gt;&lt;br /&gt;        // process the next "len" bytes&lt;br /&gt;        ProcessChunk(span.Slice(1, len));&lt;br /&gt;&lt;br /&gt;        // move forward len+1 bytes&lt;br /&gt;        span = span.Slice(len + 1);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This isn’t something we couldn’t do other ways, but it is very &lt;em&gt;convenient&lt;/em&gt; here. Importantly, we haven’t &lt;em&gt;allocated&lt;/em&gt; anything here - there’s no “new array” or similar - we just have a reference to a &lt;em&gt;different part&lt;/em&gt; of the existing range, and / or a different length.&lt;/p&gt; &lt;h2 id="coercion"&gt;Coercion&lt;/h2&gt; &lt;p&gt;A more interesting example is coercion; this is something that you can do with pointers, but is very hard to do with arrays. A classic scenario here would be IO / serialization: you have a chunk of bytes, and at &lt;em&gt;some point&lt;/em&gt; in that data you need to treat the data as fixed-size &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt;, &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt;, &lt;code class="highlighter-rouge"&gt;double&lt;/code&gt;, etc data. In the world of pointers, you just… &lt;em&gt;do that&lt;/em&gt;:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;byte* raw = ...&lt;br /&gt;float* floats = (float*)raw;&lt;br /&gt;float x = floats[0], y = floats[1]; // consume 8 bytes&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;With arrays, there is no &lt;em&gt;direct&lt;/em&gt; way to do this; you’d either need to use &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; hacks, or you can use &lt;code class="highlighter-rouge"&gt;BitConverter&lt;/code&gt; if the types you need are supported. But this is easy with &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;Span&amp;lt;byte&amp;gt; raw = ...&lt;br /&gt;var floats = raw.NonPortableCast&amp;lt;byte, float&amp;gt;();&lt;br /&gt;float x = floats[0], y = floats[1]; // consume 8 bytes&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Not only can we &lt;em&gt;do it&lt;/em&gt;, but we have the added advantage that it has correctly tracked the end range for us during the conversion - we will find that &lt;code class="highlighter-rouge"&gt;floats.Length&lt;/code&gt; is equal to &lt;code class="highlighter-rouge"&gt;raw.Length / 4&lt;/code&gt; (since each &lt;code class="highlighter-rouge"&gt;float&lt;/code&gt; requires 4 bytes). The important thing to realise here is that we haven’t &lt;em&gt;copied any data&lt;/em&gt; - we’re still looking at the exact same place in memory - but instead of treating it as a &lt;code class="highlighter-rouge"&gt;ref byte&lt;/code&gt;, we’re treating it as a &lt;code class="highlighter-rouge"&gt;ref float&lt;/code&gt;.&lt;/p&gt; &lt;h2 id="except-better"&gt;Except… better!&lt;/h2&gt; &lt;p&gt;We observed that with pointers we could coerce from &lt;code class="highlighter-rouge"&gt;byte*&lt;/code&gt; to &lt;code class="highlighter-rouge"&gt;float*&lt;/code&gt;. That’s fine, but you can’t use pointers with all types. &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; has &lt;em&gt;much stronger&lt;/em&gt; support here. A particularly interesting illustration is &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;SIMD&lt;/a&gt;, which is exposed in .NET via &lt;a href="https://msdn.microsoft.com/en-us/library/dn858385(v=vs.111).aspx"&gt;&lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt;. A vexing limitation of pointers is that we &lt;em&gt;cannot&lt;/em&gt; talk about a &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;float&amp;gt;*&lt;/code&gt; pointer (for example). This means that we can’t use pointer coercion as a convenient way of reading and writing SIMD vectors (you’ll usually have to use &lt;a href="https://www.nuget.org/packages/System.Runtime.CompilerServices.Unsafe/"&gt;&lt;code class="highlighter-rouge"&gt;Unsafe.Read&amp;lt;T&amp;gt;&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;Unsafe.Write&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; instead). But we &lt;em&gt;can&lt;/em&gt; coerce directly to &lt;code class="highlighter-rouge"&gt;Vector&amp;lt;T&amp;gt;&lt;/code&gt; from a span! Here’s an example that might come up in things like applying the web-sockets xor mask to a received frame’s payload:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void ApplyXor(Span&amp;lt;byte&amp;gt; span, uint mask) {&lt;br /&gt;    if(Vector.IsHardwareAccelerated) {&lt;br /&gt;        // apply the mask to SIMD-width bytes at a time&lt;br /&gt;        var vectorMask = new Vector&amp;lt;uint&amp;gt;(mask);&lt;br /&gt;        var typed = span.NonPortableCast&amp;lt;byte, Vector&amp;lt;uint&amp;gt;&amp;gt;();&lt;br /&gt;        for (int i = 0; i &amp;lt; typed.Length; i++) {&lt;br /&gt;            typed[i] ^= vectorMask;&lt;br /&gt;        }&lt;br /&gt;        // move past that data (might be a few bytes left)&lt;br /&gt;        span = span.Slice(Vector&amp;lt;uint&amp;gt;.Count * typed.Length);&lt;br /&gt;    }&lt;br /&gt;    // not shown - finish any remaining data &lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;That’s pretty minimal code for vectorizing something; it is especially nice that we didn’t even need to do the math to figure out the vectorizable range - &lt;code class="highlighter-rouge"&gt;typed.Length&lt;/code&gt; did everything we wanted. It would be premature for me to know for sure, but I’m also hopeful that these &lt;code class="highlighter-rouge"&gt;0&lt;/code&gt;-&lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;.Length&lt;/code&gt; loops will also elide the bounds check in the same way that array access from &lt;code class="highlighter-rouge"&gt;0&lt;/code&gt;-&lt;code class="highlighter-rouge"&gt;T[].Length&lt;/code&gt; elides the bounds check.&lt;/p&gt; &lt;h2 id="and-readonly-too"&gt;And readonly too!&lt;/h2&gt; &lt;p&gt;Pointers are notoriously permissive; if you have a pointer: you can do anything. You can use &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt; to obtain the &lt;code class="highlighter-rouge"&gt;char*&lt;/code&gt; pointer inside a &lt;code class="highlighter-rouge"&gt;string&lt;/code&gt;: if you change the data via the pointer, the &lt;code class="highlighter-rouge"&gt;string&lt;/code&gt; now has different contents. &lt;code class="highlighter-rouge"&gt;string&lt;/code&gt; is not immutable if you allow &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;: &lt;em&gt;nothing&lt;/em&gt; is immutable if you allow &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;. But just as we can obtain a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;, we can also get a &lt;code class="highlighter-rouge"&gt;ReadOnlySpan&amp;lt;T&amp;gt;&lt;/code&gt;. If you only expect a method to read the data, you can give them a &lt;code class="highlighter-rouge"&gt;ReadOnlySpan&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2 id="zero-cost-substrings"&gt;Zero-cost substrings&lt;/h2&gt; &lt;p&gt;In the “corefxlab” preview code, there’s a method-group with signatures along the lines of:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt; public static  ReadOnlySpan&amp;lt;char&amp;gt; Slice(this string text, ...)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;(where the overloads allow an initial range to be specified). This gives us a &lt;code class="highlighter-rouge"&gt;ReadOnlySpan&amp;lt;char&amp;gt;&lt;/code&gt; that points directly at a range &lt;em&gt;inside&lt;/em&gt; the &lt;code class="highlighter-rouge"&gt;string&lt;/code&gt;. If we want a substring, we can just &lt;code class="highlighter-rouge"&gt;Slice()&lt;/code&gt; again and again - with zero allocations and zero string copying - we just have different spans over the same data. A rich set of APIs already exists in the corefxlab code for working with this type of string-like data. If you do a lot of text processing, this could have some really interesting aspects.&lt;/p&gt; &lt;h2 id="this-all-sounds-too-good-to-be-true---whats-the-catch"&gt;This all sounds too good to be true - what’s the catch?&lt;/h2&gt; &lt;p&gt;Here’s the gotcha: in order to have the appropriate &lt;em&gt;correctness&lt;/em&gt; guarantees when discussing something that &lt;em&gt;could&lt;/em&gt; be a managed object, &lt;em&gt;could&lt;/em&gt; be data on the stack, or &lt;em&gt;could&lt;/em&gt; be unmanaged data, we run into very similar problems that make it impossible to store a &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; local as a field. Remember that a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is &lt;em&gt;conceptually&lt;/em&gt; a &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; (reference) and &lt;code class="highlighter-rouge"&gt;int&lt;/code&gt; (length) - well: we still need to obey the rules imposed by that “conceptually”. For a trivial example of how we can get in a mess, we can tweak our &lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; example:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;private Span&amp;lt;byte&amp;gt; _span;&lt;br /&gt;unsafe void StackAllocExample() {&lt;br /&gt;    byte* data = stackalloc byte[128];&lt;br /&gt;    _span = new Span&amp;lt;byte&amp;gt;(data, 128);&lt;br /&gt;    ...&lt;br /&gt;}&lt;br /&gt;void SomeWhileLater() {&lt;br /&gt;    ProcessData(_span);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Where does &lt;code class="highlighter-rouge"&gt;_span&lt;/code&gt; refer to in &lt;code class="highlighter-rouge"&gt;SomeWhileLater&lt;/code&gt;? I can’t tell you. We get into similar problems with anything that used &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt; to get a pointer - the pointer is only guaranteed to make sense inside the &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt;. Conceptually the issue is not restricted to pointers - it would apply equally if we could initialize &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; directly with a &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; constuctor:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;private Span&amp;lt;SomeStruct&amp;gt; _span;&lt;br /&gt;void StackRefExample() {&lt;br /&gt;    var val = new SomeStruct(123, 456);&lt;br /&gt;    _span = new Span&amp;lt;SomeStruct&amp;gt;(ref val);&lt;br /&gt;    // ^^^ hypothetical span of length 1&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;We didn’t even need &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; to break things this time. No such constructor currently exists, very wisely!&lt;/p&gt; &lt;p&gt;We &lt;em&gt;should&lt;/em&gt; be OK if we only ever use managed heap objects (arrays, etc) to initialize a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;, but the &lt;em&gt;entire point&lt;/em&gt; of &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is to provide feature parity between things like arrays and pointers while making it hard to shoot yourself in the foot.&lt;/p&gt; &lt;p&gt;In addition to this, we also need to worry about &lt;em&gt;atomicity&lt;/em&gt;. The runtime and language guarantee that a &lt;em&gt;single reference&lt;/em&gt; can be read atomically (in one CPU instruction), but it makes no guarantees about anything larger. If we have a reference &lt;em&gt;and a length&lt;/em&gt;, we start getting into very complex issues around &lt;a href="http://joeduffyblog.com/2006/02/07/threadsafety-torn-reads-and-the-like/"&gt;“torn” values&lt;/a&gt; (an invalid pair of the reference and length that didn’t actually exist, due to two threads squabbling). A torn value is vexing at the best of times, but in this case it would lead to valid-looking code accessing unexpected memory - a very bad thing.&lt;/p&gt; &lt;p&gt;The &lt;code class="highlighter-rouge"&gt;stackalloc&lt;/code&gt; example above is a perfect example of code that will compile without complaint today, but will end &lt;em&gt;very very badly&lt;/em&gt; - although we used &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;, so: self-inflicted. But this and the atomicity issue are both illustrations of why we have…&lt;/p&gt; &lt;h2 id="the-important-big-rule-of-spans"&gt;The Important Big Rule Of Spans&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;&lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; has undefined behavior off the stack&lt;/strong&gt;. And in the future: may not be allowed off the stack at all - this means no fields, no arrays, no boxing, etc. In the same way that &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; only has defined behavior on the stack (locals, parameters, return values) - so &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; only has defined behavior on the stack. You are not meant to &lt;strong&gt;ever&lt;/strong&gt; put a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; in a field (including all those times when things look like locals but are actually fields, that I touched on last time). An immediate consequence of this is that atomicity is no longer an issue: each stack is specific to a single thread; if our value can’t escape the stack, then two threads can’t have competing reads and writes.&lt;/p&gt; &lt;p&gt;There’s &lt;a href="https://github.com/dotnet/csharplang/pull/472"&gt;some in-progress discussion&lt;/a&gt; on how the rules for this requirement should work, but it &lt;em&gt;looks&lt;/em&gt; like the concept of a “ref-like” stack-only type is being introduced. &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; as a field would be ref-like, and &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; would be ref-like. Any ref-like type would only be valid directly on the stack, or as an instance field (not a &lt;code class="highlighter-rouge"&gt;static&lt;/code&gt; field) on a ref-like type. If I had to &lt;strong&gt;&lt;em&gt;speculate&lt;/em&gt;&lt;/strong&gt; at syntax, I’d expect this to look something like:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public ref struct Span&amp;lt;T&amp;gt; {&lt;br /&gt;    ref T _reference;&lt;br /&gt;    int _length;&lt;br /&gt;    public ref T this[int index] { get {...} }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Emphasis: this syntax is pure speculation based on the historic reluctance to introduce new keywords, but the &lt;code class="highlighter-rouge"&gt;ref struct&lt;/code&gt; here denotes a ref-like type. It could also be done via attributes or a range of other ideas, but note that we’re now allowed to embed the ref-like &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; field. Additionally, the compiler and runtime would verify that &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; is never used illegally as a field or in an array etc. Notionally, we could also do this for our own types that shouldn’t escape the stack, if we have similar semantics but &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; doesn’t represent our scenario.&lt;/p&gt; &lt;p&gt;Thinking back to the &lt;code class="highlighter-rouge"&gt;StackRefExample&lt;/code&gt;, if we &lt;em&gt;wanted&lt;/em&gt; to safely support usage like:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;var val = new SomeStruct(123, 456);&lt;br /&gt;var span = new Span&amp;lt;SomeStruct&amp;gt;(ref val); // local, not field&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;then presumably it &lt;em&gt;could&lt;/em&gt; work, but we’d have to have similar logic about &lt;em&gt;returning&lt;/em&gt; ref-like types as currently exists for &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;, further complicated by the fact that we don’t have the single-assignment guarantee - we can &lt;em&gt;reassign&lt;/em&gt; a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;. If ref-like types work in the general case, then the logic about &lt;em&gt;passing and returning&lt;/em&gt; such a value needs ironing out. And that’s complex. I’m very happy to &lt;a href="https://github.com/dotnet/csharplang/pull/472"&gt;defer to Vladimir Sadov&lt;/a&gt; on this!&lt;/p&gt; &lt;p&gt;EDIT: to clarify - it is only the &lt;em&gt;pair of &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;length&lt;/code&gt;&lt;/em&gt; (together known as a span, &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;ReadOnlySpan&amp;lt;T&amp;gt;&lt;/code&gt;) that need to stay on the stack; the memory that we're &lt;em&gt;spanning&lt;/em&gt; can be &lt;em&gt;anywhere&lt;/em&gt; - and will often be part of a regular array (&lt;code class="highlighter-rouge"&gt;T[]&lt;/code&gt;) on the managed heap. It &lt;em&gt;could&lt;/em&gt; also be a reference to the unmanaged heap, or to a separate part of the current stack.&lt;/p&gt; &lt;h2 id="so-how-am-i-meant-to-work-with-spans"&gt;So how am I meant to work with spans?&lt;/h2&gt; &lt;p&gt;Sure, not everything is on the stack.&lt;/p&gt; &lt;p&gt;This isn’t as much of a limitation as it sounds. Instead of storing the &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; itself, you just need to store &lt;em&gt;something that can manifest a span&lt;/em&gt;. For example, if you’re actually using arrays you might have a type that &lt;em&gt;contains&lt;/em&gt; an &lt;code class="highlighter-rouge"&gt;ArraySegment&amp;lt;T&amp;gt;&lt;/code&gt;, but which has a property:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public Span&amp;lt;T&amp;gt; Span { get { ... } }&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;As long as you can switch into &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; mode when you’re inside an appropriate method, all is good.&lt;/p&gt; &lt;p&gt;For a more unified model, the corefxlab code contains the &lt;code class="highlighter-rouge"&gt;Buffer&amp;lt;T&amp;gt;&lt;/code&gt; concept, but it is still very much a work in progress. We’ll have to see how it shakes out in time.&lt;/p&gt; &lt;h2 id="wait-why-so-much-ref-previously"&gt;Wait… why so much &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; previously?&lt;/h2&gt; &lt;p&gt;We covered a &lt;em&gt;lot&lt;/em&gt; of &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; details - you might feel cheated. Well, &lt;em&gt;partly&lt;/em&gt; we needed that information to &lt;em&gt;understand&lt;/em&gt; the stack-only semantics of &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;. But there’s more! &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; also exposes the &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; directly via the aptly named &lt;code class="highlighter-rouge"&gt;DangerousGetPinnableReference()&lt;/code&gt; method. This is a &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;, and allows us to do any of:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;store the &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; into a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; local and work with it&lt;/li&gt;  &lt;li&gt;pass the &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; as a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;out&lt;/code&gt; parameter to another method&lt;/li&gt;  &lt;li&gt;use &lt;code class="highlighter-rouge"&gt;fixed&lt;/code&gt; to convert the &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to a pointer (preventing GC movement at the same time)&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;The latter option means that &lt;em&gt;not only&lt;/em&gt; can we get from &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; to &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;, but we can go the other direction if we need:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;fixed(byte* ptr = &amp;amp;span.DangerousGetPinnableReference())&lt;br /&gt;{ ... }&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h2 id="if-i-can-get-a-ref-can-i-escape-the-bounds"&gt;If I can get a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;, can I escape the bounds?&lt;/h2&gt; &lt;p&gt;The &lt;code class="highlighter-rouge"&gt;DangerousGetPinnableReference()&lt;/code&gt; method give us back a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to the start of the range, comparable to how a &lt;code class="highlighter-rouge"&gt;T*&lt;/code&gt; pointer refers to the start of a range in pointer terms. So: can we use this to get around the range constraints? Well… yes… ish:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;ref int somewhere = ref Unsafe.Add(&lt;br /&gt;    ref span.DangerousGetPinnableReference(), 5000);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This cheeky duo gives us a reference to &lt;em&gt;whatever&lt;/em&gt; is 5000-integers ahead of the span we were thinking of. It &lt;em&gt;might&lt;/em&gt; still be part of our data (if we have a large array, for example), or it might be something completely random. But the sharp eyed might have noticed some key words in that expression… “&lt;code class="highlighter-rouge"&gt;Unsafe...&lt;/code&gt;” and “&lt;code class="highlighter-rouge"&gt;Dangerous...&lt;/code&gt;”. If you keep sprinting past signs with words like that on: expect to hit rocks. There’s nothing here that you couldn’t already do with &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; code, note.&lt;/p&gt; &lt;h2 id="doing-crazy-things-with-unmanaged-memory"&gt;Doing crazy things with unmanaged memory&lt;/h2&gt; &lt;p&gt;Sometimes you need to use unmanaged memory - this could be because of memory / collection issues, or could be because of interfacing with unmanaged systems - I use it in CUDA work, for example, where the CUDA driver has to allocate the memory in a special way to get optimal performance. Historically, working with unmanaged memory &lt;em&gt;is hard&lt;/em&gt; - you will be using pointers all the time. But we can simplify everything by using spans. Here’s our dummy type that we will store in unmanaged memory:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// could be explict layout to match external definition&lt;br /&gt;struct SomeType&lt;br /&gt;{&lt;br /&gt;    public SomeType(int id, DateTime creationDate)&lt;br /&gt;    {&lt;br /&gt;        Id = id;&lt;br /&gt;        _creationDate = creationDate.ToEpochTime();&lt;br /&gt;        // ...&lt;br /&gt;    }&lt;br /&gt;    public int Id { get; }&lt;br /&gt;    private long _creationDate;&lt;br /&gt;    public DateTime CreationDate =&amp;gt; _creationDate.FromEpochTime();&lt;br /&gt;    // ...&lt;br /&gt;    public override string ToString()&lt;br /&gt;        =&amp;gt; $"{Id}: {CreationDate}, ...";&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;We’ll need to allocate some memory and ensure it is collected, usually via a finalizer in a wrapper class:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;unsafe class UnmanagedStuff : IDisposable&lt;br /&gt;{&lt;br /&gt;    private SomeType* ptr;&lt;br /&gt;    public UnmanagedStuff(int count)&lt;br /&gt;    {&lt;br /&gt;        ptr = (SomeType*) Marshal.AllocHGlobal(&lt;br /&gt;            sizeof(SomeType) * count).ToPointer();&lt;br /&gt;    }&lt;br /&gt;    ~UnmanagedStuff() { Dispose(false); }&lt;br /&gt;    public void Dispose() =&gt; Dispose(true);&lt;br /&gt;    private void Dispose(bool disposing)&lt;br /&gt;    {&lt;br /&gt;        if(disposing) GC.SuppressFinalize(this);&lt;br /&gt;        var ip = new IntPtr(ptr);&lt;br /&gt;        if (ip != IntPtr.Zero) Marshal.Release(ip);&lt;br /&gt;        ptr = default(SomeType*);&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The wrapper type needs to know about the pointers, so is going to be &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; - but does the &lt;em&gt;rest&lt;/em&gt; of the code need to? Sure, we could add an indexer that uses &lt;code class="highlighter-rouge"&gt;Unsafe.Read&lt;/code&gt; / &lt;code class="highlighter-rouge"&gt;Unsafe.Write&lt;/code&gt; to access individual elements, but that means copying the data constantly, which is probably not what we want - and it doesn’t help us represent ranges. But &lt;em&gt;spans&lt;/em&gt; do: we can return a &lt;em&gt;span&lt;/em&gt; of the data (perhaps via a &lt;code class="highlighter-rouge"&gt;Slice()&lt;/code&gt; API):&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public Span&amp;lt;SomeType&amp;gt; Slice(int offset, int count)&lt;br /&gt;    =&amp;gt; new Span&amp;lt;SomeType&amp;gt;(ptr + offset, count);&lt;br /&gt;// ^^^ not shown: validate range first&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;And we can consume this pretty naturally &lt;em&gt;without&lt;/em&gt; &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// "stuff" is our UnmanagedStuff object&lt;br /&gt;// easily talk about a slice of unmanaged data&lt;br /&gt;var slice = stuff.Slice(5, 10);&lt;br /&gt;slice[0] = new SomeType(123, DateTime.Now);                &lt;br /&gt;&lt;br /&gt;// (separate slices work)&lt;br /&gt;slice = stuff.Slice(0, 25);&lt;br /&gt;Console.WriteLine(slice[5]); // 123: 23/04/2017 09:09:51, ...&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;If we want to talk about &lt;em&gt;individual&lt;/em&gt; elements (rather than a range), then a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; local (via a &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;) is what we want; we &lt;em&gt;could&lt;/em&gt; use the &lt;code class="highlighter-rouge"&gt;DangerousGetPinnableReference()&lt;/code&gt; API on a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; for this, but in this case it is probably easier just to use &lt;code class="highlighter-rouge"&gt;Unsafe&lt;/code&gt; directly:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;public ref SomeType this[int index]&lt;br /&gt;    =&amp;gt; ref Unsafe.AsRef&amp;lt;SomeType&amp;gt;(ptr + index);&lt;br /&gt;// ^^^ not shown: validate range first &lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;We can consume this with similar ease:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;// talk about a *reference* to unmanaged data&lt;br /&gt;ref SomeType item = ref stuff[5];&lt;br /&gt;Console.WriteLine(item); // 123: 23/04/2017 09:09:51, ...&lt;br /&gt;item = new SomeType(42, new DateTime(2016, 1, 8));&lt;br /&gt;&lt;br /&gt;// prove that updated *inside* the slice&lt;br /&gt;Console.WriteLine(slice[5]); // 42: 08/01/2016 00:00:00, ...&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;And now from &lt;strong&gt;any&lt;/strong&gt; code, we can talk directly to the unmanaged memory simply by passing it in as a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameter - it will never be copied, just dereferenced. If you &lt;em&gt;want&lt;/em&gt; to talk about an isolated copy or store a copy as a field, &lt;em&gt;then&lt;/em&gt; you can dereference, but that is easy:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;SomeType isolated = item;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;If you’ve ever worked with unmanaged memory from C#, this is a &lt;em&gt;huge&lt;/em&gt; difference - and opens up a whole range of interesting scenarios for allocation-free systems &lt;em&gt;without&lt;/em&gt; requiring the entire codebase to be &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;. For context, in an allocation-free system, the lifetime of a set of data is strictly defined by some unit of work - processing an inbound request, for example. This means we don’t &lt;em&gt;need&lt;/em&gt; reference tracking and garbage collection (and GC pauses can hurt high performance systems), so instead we simply take some slabs of memory, work from them (incrementing counters as we consume space), and then when we’ve finished the request we just set all the counters back to zero and we’re ready for the next request, no mess. Spans and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals and &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; make this friendly, even in the unmanaged memory scenario. The only caveat being - once again: &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; and &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; cannot legally escape the stack. But as we’ve seen, we can &lt;em&gt;expose on-demand&lt;/em&gt; a &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;ref T&lt;/code&gt; - so it &lt;em&gt;isn’t a burden&lt;/em&gt;.&lt;/p&gt; &lt;h2 id="summary"&gt;Summary&lt;/h2&gt; &lt;p&gt;Spans; they’re very powerful &lt;em&gt;if&lt;/em&gt; you need that kind of thing. And they force a range of new concepts into C#, giving us all the combined strong points of arrays, pointers, references and generics - with very few of the pain points. If you &lt;em&gt;don’t care&lt;/em&gt; about pointers, buffers, etc - you probably won’t need to learn about spans. But if you &lt;em&gt;do&lt;/em&gt;, &lt;em&gt;they’re awesome&lt;/em&gt;. The amount of effort the .NET folks (and the community, but mostly Microsoft) have made making this span concept so rich and powerful is &lt;em&gt;huge&lt;/em&gt; - it impacts the compiler, the JIT, the runtime, and multiple libraries both pre-existing and brand new. And it impacts both .NET and .NET Core. As someone who works &lt;em&gt;a lot&lt;/em&gt; in the areas affected by spans and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; - it is also hugely appreciated. Good things are coming.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/0bOU7BZjzrI" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7250378045006547475" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7250378045006547475" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/0bOU7BZjzrI/spans-and-ref-part-2-spans.html" title="Spans and ref part 2 : spans" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/04/spans-and-ref-part-2-spans.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-7917607356771670717</id><published>2017-04-22T14:29:00.001-07:00</published><updated>2018-06-21T12:31:48.054-07:00</updated><title type="text">Spans and ref part 1 : ref</title><content type="html">&lt;h1 id="spans-and-ref-part-1--ref"&gt;Spans and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; part 1 : &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;&lt;/h1&gt; &lt;p&gt;One of the new features in C# 7 is by-reference (&lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;) return values and locals. This is a complex topic to explain, but a good example of why we might want this is “spans” (&lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt;). I don’t have any inside knowledge on the design meetings, but I’d go further and speculate that if &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; wasn’t a thing, the &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;-changes wouldn’t have happened, so it makes sense to consider them together. Most of the fun things you can do with &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; returns / locals start to make a lot more sense when we look at &lt;code class="highlighter-rouge"&gt;Span&amp;lt;T&amp;gt;&lt;/code&gt; - which we’ll do in part 2, but first we need to remind ourselves what &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; &lt;em&gt;means&lt;/em&gt;, and explore the new &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; changes.&lt;/p&gt; &lt;h2 id="ref-returns-and-locals"&gt;&lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; returns and locals&lt;/h2&gt; &lt;p&gt;There’s a reason that &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; (and cousin &lt;code class="highlighter-rouge"&gt;out&lt;/code&gt;) aren’t used extensively on many APIs: &lt;em&gt;they are hard to fully understand&lt;/em&gt;. A lot of people will describe them in terms of “changes being visible”, but that is just a side-effect, not the meaning. I don’t mean this as a criticism: it &lt;em&gt;isn’t necessary&lt;/em&gt; for every C# developer to have a deep knowledge of the inner workings of these things.&lt;/p&gt; &lt;p&gt;But consider the following common question:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void PassByRef()&lt;br /&gt;{&lt;br /&gt;    int i = 42;&lt;br /&gt;    IncrementByRef(ref i);&lt;br /&gt;    // what does this line print, and why?&lt;br /&gt;    Console.WriteLine(i);&lt;br /&gt;}&lt;br /&gt;void IncrementByRef(ref int x)&lt;br /&gt;{&lt;br /&gt;    x = x + 1; // increment&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Most developers will be able to correctly understand that this will output “43”, but asking them exactly what happened can reveal very different levels of understanding. The short summary is that &lt;em&gt;a reference to the variable &lt;code class="highlighter-rouge"&gt;i&lt;/code&gt;&lt;/em&gt; was passed to &lt;code class="highlighter-rouge"&gt;IncrementByRef&lt;/code&gt;; all the code in &lt;code class="highlighter-rouge"&gt;IncrementByRef&lt;/code&gt; that looks like it is reading / writing to the parameter is actually &lt;em&gt;dereferencing&lt;/em&gt; the parameter at each stage. This is clearer if we write it in &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt; pointer code instead:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;unsafe void PassByPointer()&lt;br /&gt;{&lt;br /&gt;    int i = 42;&lt;br /&gt;    IncrementByPointer(&amp;amp;i);&lt;br /&gt;    // what does this line print, and why?&lt;br /&gt;    Console.WriteLine(i);&lt;br /&gt;}&lt;br /&gt;unsafe void IncrementByPointer(int* x)&lt;br /&gt;{&lt;br /&gt;    *x = *x + 1; // increment&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Here we can clearly see the “take a reference to” operation (&lt;code class="highlighter-rouge"&gt;&amp;amp;&lt;/code&gt;) and “dereference” (&lt;code class="highlighter-rouge"&gt;*&lt;/code&gt;) operations, but there’s a lot of problems with pointers:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;the garbage collector (GC) refuses to even &lt;em&gt;try&lt;/em&gt; to walk pointers, which means you need to be very careful to only access memory that won’t move (unmanaged memory, stack memory, or pinned managed objects)&lt;/li&gt;  &lt;li&gt;pointer arithmetic makes it trivially possible to access adjacent memory without any bounds checking&lt;/li&gt;  &lt;li&gt;it forces us to use &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;, which makes it &lt;em&gt;very&lt;/em&gt; easy to make subtle but major bugs that cause just about any level of silliness imaginable&lt;/li&gt;  &lt;li&gt;pointers only work for a small subset of types - essentially primitives and &lt;code class="highlighter-rouge"&gt;struct&lt;/code&gt;s composed of primitives&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;The point of &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameters is to get the best of both worlds. &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; is &lt;em&gt;essentially just like&lt;/em&gt; pointers, but with enough restrictions to stop us getting into messes. The aditional sanity checks and restrictions mean that the IL knows enough about the meaning for the GC to sensibly be able to navigate them without getting confused, so we don’t need to worry about the reference suddenly being meaningless - and since we can’t do anything &lt;em&gt;too&lt;/em&gt; silly, we don’t need to drop to &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;. And it should work for &lt;em&gt;any&lt;/em&gt; regular type.&lt;/p&gt; &lt;p&gt;But, historically, this ability to add automatic dereferencing and talk about &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; has been restricted to &lt;em&gt;method parameters&lt;/em&gt;; no fields, no locals, and no return values.&lt;/p&gt; &lt;h2 id="ref-locals"&gt;&lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals&lt;/h2&gt; &lt;p&gt;The first change in C#7 allows us to talk about automatically dereferenced &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; items as &lt;em&gt;local (method) variables&lt;/em&gt;. In the same way that a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameter is denoted by a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; prefix before the type, so it is with &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals, with the added bonus that &lt;code class="highlighter-rouge"&gt;ref var&lt;/code&gt; is legal:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;void ByRefLocal()&lt;br /&gt;{&lt;br /&gt;    int i = 42;&lt;br /&gt;    ref var x = ref i;&lt;br /&gt;    x = x + 1;&lt;br /&gt;    // what does this line print, and why?&lt;br /&gt;    Console.WriteLine(i);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This prints “43” for exactly the same reasons as before - the only difference is that we now have a syntax to express &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; when talking about locals. Previously, we would have to have added an additional method to switch to &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; semantics for a local. One slight peculiarity here is that &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals &lt;em&gt;must&lt;/em&gt; be assigned a value at the point of declaration - and we can &lt;strong&gt;only&lt;/strong&gt; assign it a value at this point. Any further attempt to assign a value to a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; local is interpreted as a &lt;em&gt;dereferencing&lt;/em&gt; assignment - the &lt;code class="highlighter-rouge"&gt;*x = &lt;/code&gt; in our pointer example.&lt;/p&gt; &lt;p&gt;This ability is &lt;em&gt;nice&lt;/em&gt;, but it isn’t very &lt;em&gt;useful&lt;/em&gt; until we combine it with…&lt;/p&gt; &lt;h2 id="ref-return"&gt;&lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;A much more interesting and powerful addition in C# 7 is &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; returns. As the name suggests, this allows us to &lt;strong&gt;return&lt;/strong&gt; a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; value from a method. We can capture this value into a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; local as long as we include an additional &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; just before the value to make it very clear that we don’t want to &lt;em&gt;dereference&lt;/em&gt; - which is the regular behaviour whenever touching a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameter or local:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;ref int GetArrayReference(int[] items, int index)&lt;br /&gt;    =&amp;gt; ref items[index];&lt;br /&gt;&lt;br /&gt;void IncrementInsideArrayByRef()&lt;br /&gt;{&lt;br /&gt;    int[] values = { 1, 2, 3 };&lt;br /&gt;&lt;br /&gt;    ref int item = ref GetArrayReference(values, 1);&lt;br /&gt;    IncrementByRef(ref item);&lt;br /&gt;    // what does this line print, and why?&lt;br /&gt;    Console.WriteLine(string.Join(",", values));&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Here the &lt;code class="highlighter-rouge"&gt;GetArrayReference&lt;/code&gt; method provides the caller a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to &lt;em&gt;inside&lt;/em&gt; the array. Note that the ability to get a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; into an array is not by itself new - this has &lt;em&gt;always&lt;/em&gt; worked:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;IncrementByRef(ref values[item]);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The bit that is new and different is only the ability to &lt;code class="highlighter-rouge"&gt;return&lt;/code&gt; a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; value.&lt;/p&gt; &lt;p&gt;Since we increment a &lt;code class="highlighter-rouge"&gt;ref int&lt;/code&gt; that refers to the array index &lt;code class="highlighter-rouge"&gt;1&lt;/code&gt; (the second element), the result is “1,3,3”.&lt;/p&gt; &lt;p&gt;Note that we don’t need to capture the &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; value before we use it - we can also pass a &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; result directly into a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;out&lt;/code&gt; parameter:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;IncrementByRef(ref GetArrayReference(values, 1));&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h2 id="are-there-restrictions-on-what-we-can-ref-return"&gt;Are there restrictions on what we can &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt;?&lt;/h2&gt; &lt;p&gt;Yes, yes there are. Figuring out the rules on what can and can’t be safely returned as &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; without letting the author get into an accidental ugly mess of landmines is probably why it has never been supported in the past. We’ve seen that we can return &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; references into arrays. And we’ve seen that we can take a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; of a local variable. But a local only exists in the context of the current stack-frame: very bad things would happen if we could &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to a local - the &lt;em&gt;caller&lt;/em&gt; would have a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to a position outside the active stack, which would be undefined behavior:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;ref int ReturnInvalidStackReference()&lt;br /&gt;{&lt;br /&gt;    int i = 32;&lt;br /&gt;    return ref i; // can't do this&lt;br /&gt;}&lt;br /&gt;void WhatHappensHere()&lt;br /&gt;{&lt;br /&gt;    ref int v = ref ReturnInvalidStackReference();&lt;br /&gt;    CallSomeOtherMethods(); // to use the stack&lt;br /&gt;    int i = v; // dereference the ref&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;You’ll be relieved to know that the compiler doesn’t let us do this - the compiler is very strict to ensure that &lt;strong&gt;if&lt;/strong&gt; we want to &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; something, then it must &lt;em&gt;demonstrably&lt;/em&gt; refer to a safe value. Put very simply: as long as the assignment doesn’t involve a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; to a local, we’ll be fine. &lt;code class="highlighter-rouge"&gt;return ref i;&lt;/code&gt; clearly involves the local &lt;code class="highlighter-rouge"&gt;i&lt;/code&gt;, so can’t be returned. The &lt;code class="highlighter-rouge"&gt;return ref&lt;/code&gt; expression is inspected for safety; each part of the expression must be safe. This includes any &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameters that we have passed &lt;em&gt;into&lt;/em&gt; any method calls:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;ref int ReturnInvalidStackReference()&lt;br /&gt;{&lt;br /&gt;    int j = 42;&lt;br /&gt;    return ref DoSomething(ref j);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This might look like a confusing restriction, but note that &lt;code class="highlighter-rouge"&gt;DoSomething&lt;/code&gt; could be implemented as:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;ref int DoSomething(ref int evil) =&amp;gt; ref evil;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;which would expose the &lt;code class="highlighter-rouge"&gt;ref j&lt;/code&gt; stack reference to the &lt;em&gt;caller&lt;/em&gt; of &lt;code class="highlighter-rouge"&gt;ReturnInvalidStackReference&lt;/code&gt;, so any such possibility is excluded. The implementation here is pretty solid, so if it refuses to let you &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; something, you’ve &lt;em&gt;probably&lt;/em&gt; attempted something that looks too much like you’re involving locals of the current method.&lt;/p&gt; &lt;h2 id="but-no-ref-fields"&gt;But no &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; fields&lt;/h2&gt; &lt;p&gt;We have &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; parameters, &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; returns. However, there is no currently support for &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; &lt;em&gt;fields&lt;/em&gt; (instance or static variables). Specifically, this is not legal:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;struct Foo {&lt;br /&gt;    ref int _reference;&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;or:&lt;/p&gt; &lt;div class="highlighter-rouge"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;class Foo {&lt;br /&gt;    ref int _reference;&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The reason for this again relates to undefined behaviour and escaping the stack-frame. If we could put a &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; into a field, then we are at grave danger of proving invalid access (at some point later on) to a position in memory that now means something completely unrelated to what it meant when we took the &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;. Strictly speaking we can prove that &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; fields would be valid if the assigned value comes from inside an &lt;code class="highlighter-rouge"&gt;object&lt;/code&gt;, and we’ll discuss another safe scenario in part 2,but currently the rule is simple: &lt;strong&gt;no &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; fields&lt;/strong&gt;.&lt;/p&gt; &lt;h2 id="when-is-a-local-not-a-local"&gt;When is a local not a local?&lt;/h2&gt; &lt;p&gt;This has some additional consequences for a number of code concepts that &lt;em&gt;look&lt;/em&gt; like locals, but which are actually fields; for example:&lt;/p&gt; &lt;ul&gt;  &lt;li&gt;locals in an iterator block (&lt;code class="highlighter-rouge"&gt;yield return&lt;/code&gt;)&lt;/li&gt;  &lt;li&gt;locals in an &lt;code class="highlighter-rouge"&gt;async&lt;/code&gt; method&lt;/li&gt;  &lt;li&gt;captured variables in lambdas, anonymous methods, and LINQ syntax comprehensions&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;All of these situations are - and for similar reasons (with the added bonus of issues of lifetime) - the scenarios where you can’t use &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; or &lt;code class="highlighter-rouge"&gt;out&lt;/code&gt; parameters or &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;, so basically: if you can’t use &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt;, &lt;code class="highlighter-rouge"&gt;out&lt;/code&gt; parameters or &lt;code class="highlighter-rouge"&gt;unsafe&lt;/code&gt;, you won’t be able to use &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals or &lt;code class="highlighter-rouge"&gt;ref return&lt;/code&gt; either.&lt;/p&gt; &lt;p&gt;One additional scenario, though, is tuples: as I &lt;a href="http://blog.marcgravell.com/2017/04/exploring-tuples-as-library-author.html"&gt;discussed previously&lt;/a&gt;, tuples are secretly implemented as fields on the &lt;code class="highlighter-rouge"&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; family. So: no &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; values in tuples.&lt;/p&gt; &lt;h2 id="summary"&gt;Summary&lt;/h2&gt; &lt;p&gt;This &lt;em&gt;should&lt;/em&gt; give you enough to start understanding what &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; locals and &lt;code class="highlighter-rouge"&gt;ref&lt;/code&gt; returns &lt;em&gt;are&lt;/em&gt;, but for them really start to &lt;strong&gt;&lt;em&gt;make sense&lt;/em&gt;&lt;/strong&gt; we need a concrete example. And we get that in “spans”, coming up next!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/g3b8Im2yugo" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7917607356771670717" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7917607356771670717" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/g3b8Im2yugo/spans-and-ref-part-1-ref.html" title="Spans and ref part 1 : ref" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/04/spans-and-ref-part-1-ref.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-28626824073231224</id><published>2017-04-09T11:41:00.002-07:00</published><updated>2017-04-09T12:52:34.481-07:00</updated><title type="text">Exploring Tuples as a Library Author</title><content type="html">&lt;h1&gt;&lt;a id="Exploring_Tuples_as_a_Library_Author_0"&gt;&lt;/a&gt;Exploring Tuples as a Library Author&lt;/h1&gt;&lt;h2&gt;&lt;a id="tldr_2"&gt;&lt;/a&gt;tl;dr:&lt;/h2&gt;&lt;p&gt;This works in an upcoming version of dapper; we like it:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// very lazy dapper query &lt;br /&gt;var users = db.Query&amp;lt;(int id, string name)&amp;gt;(&lt;br /&gt;    &amp;quot;select Id, Name from Users&amp;quot;).AsList();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And this works in an upcoming release of protobuf-net:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// poor man's serialization contract&lt;br /&gt;var token = Serializer.Deserialize&amp;lt;(DateTime dob, string key)&amp;gt;(source);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;But - all is not necessarily as it seems. Here we will discuss the nuances of C# 7 tuple usage, and look at the limitations and restrictions that the implementation imposes. In doing so, we can look at what this means for library authors, and what expectations consumers should have.&lt;/p&gt;&lt;h2&gt;&lt;a id="What_is_new_in_the_world_of_tuples_17"&gt;&lt;/a&gt;What is new in the world of tuples?&lt;/h2&gt;&lt;p&gt;One of the fun new tools in C# 7 is language-level tuple support. Sure, we’ve had the &lt;code&gt;Tuple&amp;lt;...&amp;gt;&lt;/code&gt; family for ages, but that forces you to use &lt;code&gt;Item1&lt;/code&gt;, &lt;code&gt;Item2&lt;/code&gt; (etc) naming, and it is a &lt;code&gt;class&lt;/code&gt; - so involves allocations, but… it works:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// System.Tuple usage&lt;br /&gt;var tuple = Tuple.Create(123, &amp;quot;abc&amp;quot;);&lt;br /&gt;int foo = tuple.Item1; // 123&lt;br /&gt;string bar = tuple.Item2; // &amp;quot;abc&amp;quot;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The ever-terser syntax for read-only properties makes it not too hard to write your own manual tuples, but it isn’t necessarily the best use of your time (especially if you want to do it &lt;em&gt;well&lt;/em&gt; - the code is less trivial than people imagine); a &lt;em&gt;simple&lt;/em&gt; example might look like:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// manual tuple struct definition&lt;br /&gt;struct MyType&lt;br /&gt;{&lt;br /&gt;    public MyType(int id, string name)&lt;br /&gt;    {&lt;br /&gt;        Id = id;&lt;br /&gt;        Name = name;&lt;br /&gt;    }&lt;br /&gt;    public int Id { get; }&lt;br /&gt;    public string Name { get; }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Maneagable, but… not very friendly if you’re just trying to hack a few values around in close proximity to each-other.&lt;/p&gt;&lt;p&gt;More recently, the &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; family has been &lt;a href="https://www.nuget.org/packages/System.ValueTuple/"&gt;added on nuget&lt;/a&gt;; this solves the allocation problem, but still leaves us with names like &lt;code&gt;Item1&lt;/code&gt;, &lt;code&gt;Item2&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// System.ValueTuple usage&lt;br /&gt;var tuple = ValueTuple.Create(123, &amp;quot;abc&amp;quot;);&lt;br /&gt;int foo = tuple.Item1; // 123&lt;br /&gt;string bar = tuple.Item2; // &amp;quot;abc&amp;quot;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The new thing, then, is language support to prettify this. As an illustration, we can re-write that as:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// language tuple usage&lt;br /&gt;var tuple = (id: 23, name: &amp;quot;abc&amp;quot;);&lt;br /&gt;int foo = tuple.id; // 123&lt;br /&gt;string bar = tuple.name; // &amp;quot;abc&amp;quot;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This allows us to conveniently represent some named pieces of data. It looks broadly similar to anonymous types:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// anonymous type&lt;br /&gt;var obj = new { Id = 23, Name = &amp;quot;abc&amp;quot; };&lt;br /&gt;int foo = obj.Id; // 123&lt;br /&gt;string bar = obj.name; // &amp;quot;abc&amp;quot;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;But it is implemented very differently, as we’ll see. Anonymous types are very limited in scope - bacause you can’t &lt;em&gt;say their name&lt;/em&gt;, you can’t expose them on any API boundary (even &lt;code&gt;internal&lt;/code&gt; methods). Tuples, however, don’t have this limitation - they can be used in parameters and return types:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// language tuples on method signatures&lt;br /&gt;public static (int x, int y) DoSomething(&lt;br /&gt;    string foo, (int id, DateTime when) bar) {...}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And you can even express them in generics:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// language tuples as generic type parameters&lt;br /&gt;var points = new List&amp;lt;(int x, int y)&amp;gt;();&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a id="How_are_language_tuples_implemented_74"&gt;&lt;/a&gt;How are language tuples implemented?&lt;/h2&gt;&lt;p&gt;Given how similar they look to anonymous types, you’d be forgiven for assuming that they were implemented similarly - perhaps defining a struct instead of a class. Let’s test that by decompiling a basic console exe:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// test console using language tuples&lt;br /&gt;static class Program&lt;br /&gt;{&lt;br /&gt;    static void Main()&lt;br /&gt;    {&lt;br /&gt;        var tuple = (id: 23, name: &amp;quot;abc&amp;quot;);&lt;br /&gt;        System.Console.WriteLine(tuple.name); // &amp;quot;abc&amp;quot;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We’ll compile that in release mode and look at the IL:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;ildasm mytest.exe&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and find the &lt;code&gt;Main()&lt;/code&gt; method - we see:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;.method private hidebysig static void  Main() cil managed&lt;br /&gt;{&lt;br /&gt;.entrypoint&lt;br /&gt;// Code size       23 (0x17)&lt;br /&gt;.maxstack  8&lt;br /&gt;IL_0000:  ldc.i4.s   23&lt;br /&gt;IL_0002:  ldstr      &amp;quot;abc&amp;quot;&lt;br /&gt;IL_0007:  newobj     instance void valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;int32,string&amp;gt;::.ctor(!0, !1)&lt;br /&gt;IL_000c:  ldfld      !1 valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;int32,string&amp;gt;::Item2&lt;br /&gt;IL_0011:  call       void [mscorlib]System.Console::WriteLine(string)&lt;br /&gt;IL_0016:  ret&lt;br /&gt;} // end of method Program::Main&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I don’t expect everyone to be able to read IL, but what you &lt;strong&gt;won’t&lt;/strong&gt; see in there is any mention of &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;name&lt;/code&gt; - and there are no additional types in the assembly (just &lt;code&gt;Program&lt;/code&gt;). Instead, it is using &lt;code&gt;System.ValueTuple&amp;lt;int,string&amp;gt;&lt;/code&gt;. The IL we have here is &lt;strong&gt;exactly&lt;/strong&gt; the same as we would get if we had compiled:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// test console using System.ValueTuple&amp;lt;...&amp;gt;&lt;br /&gt;static class Program&lt;br /&gt;{&lt;br /&gt;    static void Main()&lt;br /&gt;    {&lt;br /&gt;        var tuple = new System.ValueTuple&amp;lt;int, string&amp;gt;(23, &amp;quot;abc&amp;quot;);&lt;br /&gt;        System.Console.WriteLine(tuple.Item2); // &amp;quot;abc&amp;quot;&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is by design and is always the case - which is why you need a reference to &lt;code&gt;System.ValueTuple&lt;/code&gt; in order to use language tuples. In fact, to make things more fun, you’re *still allowed to use the &lt;code&gt;Item1&lt;/code&gt; etc names:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// accessing a tuple via the implementation names&lt;br /&gt;var tuple = (id: 23, name: &amp;quot;abc&amp;quot;);&lt;br /&gt;System.Console.WriteLine(tuple.Item2); // &amp;quot;abc&amp;quot;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Things would obviously get &lt;em&gt;very&lt;/em&gt; confusing if you had used &lt;code&gt;Item2&lt;/code&gt; as a name in your tuple syntax - the compiler has enough sense to check that if you declare &lt;code&gt;Item2&lt;/code&gt;, it &lt;em&gt;must&lt;/em&gt; be in the second spot; and a few other well-known method names are blocked too, but generally: you’ll be fine.&lt;/p&gt;&lt;h2&gt;&lt;a id="But_if_it_is_just_ValueTuple_how_does_it_work_on_the_public_API_127"&gt;&lt;/a&gt;But if it is just &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt;, how does it work on the public API?&lt;/h2&gt;&lt;p&gt;After all, this works:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// declare a method that uses tuples on the public API&lt;br /&gt;public static class TypeInAssemblyA&lt;br /&gt;{&lt;br /&gt;    public static List&amp;lt;(int x, int y)&amp;gt; GetCoords((string key, string value) foo)&lt;br /&gt;    {&lt;br /&gt;        return new[] { (1, 2) }.ToList();&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;// consume a method that uses tuples on the public API&lt;br /&gt;static class TypeInAssemblyB&lt;br /&gt;{&lt;br /&gt;    static void ConsumeData()&lt;br /&gt;    {&lt;br /&gt;        foreach (var pair in TypeInAssemblyA.GetCoords((&amp;quot;abc&amp;quot;, &amp;quot;def&amp;quot;)))&lt;br /&gt;        {&lt;br /&gt;            System.Console.WriteLine($&amp;quot;{pair.x}, {pair.y}&amp;quot;);&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So the logical names (&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;) are clearly being conveyed somehow, but this is still &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt;. If we look at the IL again, we see the trick in the top of the declaration of &lt;code&gt;GetCoords&lt;/code&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;.method public hidebysig static class [mscorlib]System.Collections.Generic.List`1&amp;lt;valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;int32,int32&amp;gt;&amp;gt; &lt;br /&gt;        GetCoords(valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;string,string&amp;gt; foo) cil managed&lt;br /&gt;{&lt;br /&gt;.param [0]&lt;br /&gt;.custom instance void [System.ValueTuple]System.Runtime.CompilerServices.TupleElementNamesAttribute::.ctor(string[])&lt;br /&gt;= ( 01 00 02 00 00 00 01 78 01 79 00 00 )&lt;br /&gt;// .......x.y..&lt;br /&gt;.param [1]&lt;br /&gt;.custom instance void [System.ValueTuple]System.Runtime.CompilerServices.TupleElementNamesAttribute::.ctor(string[])&lt;br /&gt;= ( 01 00 02 00 00 00 03 6B 65 79 05 76 61 6C 75 65 00 00 )&lt;br /&gt;// .......key.value&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Don’t worry if you’re not an IL expert - they key point is that it declares attributes (&lt;code&gt;[TupleElementNames(...)]&lt;/code&gt;) against the parameter and return type that include the names, &lt;em&gt;essentially&lt;/em&gt; as though we had done:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// equivalent: use tuple element name attributes explicitly&lt;br /&gt;[return: TupleElementNames(new[] { &amp;quot;x&amp;quot;, &amp;quot;y}&amp;quot; })]&lt;br /&gt;public static List&amp;lt;ValueTuple&amp;lt;int, int&amp;gt;&amp;gt; GetCoords(&lt;br /&gt;    [TupleElementNames(new[] {&amp;quot;key&amp;quot;,&amp;quot;value&amp;quot;})]&lt;br /&gt;    ValueTuple&amp;lt;string,string&amp;gt; foo)&lt;br /&gt;{&lt;br /&gt;    return new[] { (1, 2) }.ToList();&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(the C# 7 compiler doesn’t let us do this manually, though - it tells us to use tuple syntax instead)&lt;/p&gt;&lt;p&gt;This tells us once and for all that the names are nothing to do with the type, and are either removed completely, or exposed simply as decorations via attributes (when used on an API).&lt;/p&gt;&lt;h3&gt;&lt;a id="But_tuples_can_be_arbitrarily_complex_180"&gt;&lt;/a&gt;But tuples can be arbitrarily complex&lt;/h3&gt;&lt;p&gt;Note that we don’t just need to consider returning flat tuples - the individual elements of tuples can themselves be tuples, or involve tuples (lists of tuples, arrays of tuples, tuple fields, etc). To be honest I don’t expect to see this much in the wild (if you’re getting this complex, you should probably define your own type), but this works:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// expose nested tuples&lt;br /&gt;public static (int x, (int y, (float a, float b))[] coords) Nested()&lt;br /&gt;    =&amp;gt; (1, new[] { (2, (3.0f, 4.0f)) });&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which is described as:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;.param [0]&lt;br /&gt;.custom instance void [System.ValueTuple]System.Runtime.CompilerServices.TupleElementNamesAttribute::.ctor(string[])&lt;br /&gt;= ( 01 00 06 00 00 00 01 78 06 63 6F 6F 72 64 73 01 79 FF 01 61 01 62 00 00 )&lt;br /&gt;// .......x.coords.y..a.b..&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which is the equivalent of:&lt;/p&gt;&lt;pre&gt;&lt;code&gt; [return: TupleElementNames(new[] {&lt;br /&gt;     &amp;quot;x&amp;quot;, &amp;quot;coords&amp;quot;, &amp;quot;y&amp;quot;, null, &amp;quot;a&amp;quot;, &amp;quot;b&amp;quot; })]&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In truth, I don’t know the significance of the &lt;code&gt;null&lt;/code&gt;, or how &lt;em&gt;exactly&lt;/em&gt; the mapping works in terms of position in the compound tuple vs position in the array.&lt;/p&gt;&lt;p&gt;EDIT: oops, silly me - the &lt;code&gt;null&lt;/code&gt; is because I &lt;em&gt;didn't name&lt;/em&gt; the element described by &lt;code&gt;(float a, float b)&lt;/code&gt; - yes, that's allowed.&lt;/p&gt;&lt;h2&gt;&lt;a id="We_can_get_names_out_of_an_API__can_we_push_them_in_202"&gt;&lt;/a&gt;We can get names &lt;em&gt;out&lt;/em&gt; of an API - can we push them &lt;em&gt;in&lt;/em&gt;?&lt;/h2&gt;&lt;p&gt;It is very common in code that uses libraries to create an insteance and then push it into an API either as &lt;code&gt;object&lt;/code&gt; or via generics (&lt;code&gt;&amp;lt;T&amp;gt;&lt;/code&gt;). For example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;var val = (id: 123, name: &amp;quot;Fred&amp;quot;);&lt;br /&gt;SomeLibrary.SomeObjectMethod(val);&lt;br /&gt;SomeLibrary.SomeGenericMethod(val); // &amp;lt;T&amp;gt; inferred&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;with (for a simple example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;public static void SomeGenericMethod&amp;lt;T&amp;gt;(T val) {&lt;br /&gt;    Console.WriteLine(typeof(T).Name);&lt;br /&gt;    foreach(var field in typeof(T).GetFields())&lt;br /&gt;        Console.WriteLine(field.Name);&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;public static void SomeObjectMethod(object val)&lt;br /&gt;{&lt;br /&gt;    Console.WriteLine(val.GetType().Name);&lt;br /&gt;    foreach (var field in val.GetType().GetFields())&lt;br /&gt;        Console.WriteLine(field.Name);&lt;br /&gt;}&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we run this, we see:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;ValueTuple`2&lt;br /&gt;Item1&lt;br /&gt;Item2&lt;br /&gt;ValueTuple`2&lt;br /&gt;Item1&lt;br /&gt;Item2&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This output confirms that once again we’re seeing &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; in play. As before, let’s look at the relevant part of the IL for these two calls (note I added some other code, not shown, to force the value into locals for simplicity):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;ldloc.0&lt;br /&gt;box        valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;int32,string&amp;gt;&lt;br /&gt;call       void SomeLibrary::SomeObjectMethod(object)&lt;br /&gt;&lt;br /&gt;ldloc.0&lt;br /&gt;call       void SomeLibrary::SomeGenericMethod&amp;lt;valuetype [System.ValueTuple]System.ValueTuple`2&amp;lt;int32,string&amp;gt;&amp;gt;(!!0)&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first call converts the tuple to &lt;code&gt;object&lt;/code&gt; by “boxing” it; the second call uses generics without boxing. But &lt;em&gt;neither&lt;/em&gt; of them include anything whatsoever about the names. This is consistent with what we saw by dumping the fields, and gives us the summary statement:&lt;/p&gt;&lt;h2&gt;&lt;a id="Tuple_names_pass_outwards_never_inwards_245"&gt;&lt;/a&gt;Tuple names pass outwards, never inwards&lt;/h2&gt;&lt;p&gt;Tuple names can be passed &lt;strong&gt;outwards&lt;/strong&gt; (names declared in the public signature of a method can be used in code that consumes that method), but can not be passed &lt;strong&gt;inwards&lt;/strong&gt; (names declared in one method will never be available to general library code that consumes those values).  This has significant implications in a wide range of scenarios that we normally expect to be able to access names via reflection: the names &lt;em&gt;simply won’t exist&lt;/em&gt;. For example, if we serialized a tuple to JSON, we would expect to see &lt;code&gt;Item1&lt;/code&gt;, &lt;code&gt;Item2&lt;/code&gt;, etc instead of the names we thought we declared. And there is &lt;strong&gt;nothing whatsoever&lt;/strong&gt; the library author can do to get the original names. It would be &lt;em&gt;nice&lt;/em&gt; if there was something akin to the method caller attributes (&lt;code&gt;[CallerMemberName]&lt;/code&gt; etc), that library authors could attach to an optional parameter to receive the dummy names in some structured form (one part per generic type parameter), but: I can see that it would be very hard to define the semantics of this in the general case (it could be a generic method on a generic type nested in a generic type, for example).&lt;/p&gt;&lt;h2&gt;&lt;a id="So_what_can_library_authors_do_249"&gt;&lt;/a&gt;So what &lt;em&gt;can&lt;/em&gt; library authors do?&lt;/h2&gt;&lt;p&gt;OK, so names are off the table. But that doesn’t mean that tuples are completely useless for use with library code. We still have &lt;em&gt;position&lt;/em&gt;. And if a library can do something sensible just using the position semantics, then it doesn’t matter that the library doesn’t know the names that the caller had in mind.&lt;/p&gt;&lt;p&gt;Let’s take 2 examples; &lt;a href="https://www.nuget.org/packages/protobuf-net/"&gt;protobuf-net&lt;/a&gt; and &lt;a href="https://www.nuget.org/packages/dapper"&gt;dapper&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a id="protobufnet_255"&gt;&lt;/a&gt;protobuf-net&lt;/h2&gt;&lt;p&gt;This is a binary serialization library that follows the &lt;a href="https://developers.google.com/protocol-buffers/"&gt;protocol buffers&lt;/a&gt; (protobuf) serialization format. One of the ways that protobuf achieves efficent encoding is that it &lt;em&gt;doesn’t care about names&lt;/em&gt;. At all (at least, in the binary protocol). All it knows is that there is an integer in field 1, a utf8 string in field 2, and an array of floats in field 3 (for example). In this case, we don’t lose anything when we see &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; - all we need to do is to make sure that the library &lt;em&gt;recognises&lt;/em&gt; the pattern and knows how to infer the contract from the type (so &lt;code&gt;Item1&lt;/code&gt; maps to field 1, &lt;code&gt;Item2&lt;/code&gt; maps to field 2, etc).&lt;/p&gt;&lt;p&gt;As it happens, protobuf-net already had code to recognise a general family of tuple patterns, so this &lt;em&gt;very nearly&lt;/em&gt; worked for free without any changes. The only thing that tripped it up was that historically the “is this a tuple” check made the assumption that any sensible tuple would be immutable. As it happens, &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; is implemented as fully mutable (with mutable public fields). By softening that check a little bit, protobuf-net now works with any such tuple it sees.&lt;/p&gt;&lt;h2&gt;&lt;a id="dapper_261"&gt;&lt;/a&gt;dapper&lt;/h2&gt;&lt;p&gt;This is an ADO.NET helper utility, that makes it really really simple to correctly execute parameterized commands and queries, and populate records from the results. In terms of the latter, when executing &lt;code&gt;Query&amp;lt;T&amp;gt;(...)&lt;/code&gt;, it executes the ADO.NET query, then maps the results &lt;em&gt;by name&lt;/em&gt; into the fields and properties of &lt;code&gt;T&lt;/code&gt;, one instance of &lt;code&gt;T&lt;/code&gt; per row received. As we’ve discovered before, we are never going to able to respect the caller’s names when using C# 7 tuples, and it is &lt;em&gt;unlikely&lt;/em&gt; that people will conveniently call their columns &lt;code&gt;Item1&lt;/code&gt;, &lt;code&gt;Item2&lt;/code&gt;, etc. That means that it probably isn’t going to be useful to use value tuples with domain entity objects, but… if you’re using value tuples for your domain entity objects you already need a stiff talking to.&lt;/p&gt;&lt;p&gt;However! There is another very common query scenario with dapper; ad-hoc local queries that get trivial data. In a lot of these cases, it really isn’t worth going to the trouble of declaring a custom type to receive the results, so dapper supports &lt;code&gt;dynamic&lt;/code&gt; as a “meh, it’ll work” helper:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// query dapper via &amp;quot;dynamic&amp;quot;&lt;br /&gt;int id = ...&lt;br /&gt;var row = conn.QuerySingle(&amp;quot;select Name, LastUpdated from Records where Id=@id&amp;quot;, new {id});&lt;br /&gt;string name = row.Name; // &amp;quot;dynamic&amp;quot;&lt;br /&gt;DateTime lastUpdated = row.LastUpdated;&lt;br /&gt;// use name and lastUpdated&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This &lt;em&gt;works&lt;/em&gt;, but: it involves the DLR (&lt;em&gt;relatively&lt;/em&gt; expensive compared to direct C#), and has a few other overheads. What we &lt;em&gt;can&lt;/em&gt; do is change dapper to recognise that &lt;em&gt;names&lt;/em&gt; aren’t useful in &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt;, but rely instead on the &lt;em&gt;column position&lt;/em&gt;:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;// query dapper via C# 7 tuples&lt;br /&gt;int id = ...&lt;br /&gt;var row = conn.QuerySingle&amp;lt;(string name, DateTime lastUpdated)&amp;gt;(&lt;br /&gt;     &amp;quot;select Name, LastUpdated from Records where Id=@id&amp;quot;, new {id});&lt;br /&gt;// use row.name and row.lastUpdated&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That’s actually pretty handy for a wide range of scenarios where we’re going to consume the data immediately adjacent to the query. And because they are value types we don’t even pay heap allocation costs. I wouldn’t use it for populating real domain objects, though.&lt;/p&gt;&lt;h3&gt;&lt;a id="dapper_what_about_parameters_284"&gt;&lt;/a&gt;dapper: what about parameters?&lt;/h3&gt;&lt;p&gt;you’ll notice that we also have a &lt;code&gt;new {id}&lt;/code&gt; anonymous type usage to pass parameters &lt;em&gt;into&lt;/em&gt; dapper. There are a number of reasons that I’ve left this alone and have made no attempt to support value tuples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;in ADO.NET, most providers support names, and some important providers (&lt;code&gt;SqlClient&lt;/code&gt; for SQL Server for example) do not support positional parameters; while we &lt;em&gt;could in theory&lt;/em&gt; expose a positional parameter syntax in the API, the use would be somewhat limited&lt;/li&gt;&lt;li&gt;at the moment, the parameters are passed as &lt;code&gt;object&lt;/code&gt;; if we pass a C# tuple it would be boxed, which means an allocation - and at that point, we might as well have just used an anonymous type object (which would have conveyed names)&lt;/li&gt;&lt;li&gt;if you’re thinking “make the method generic so the parameter is passed as &lt;code&gt;T&lt;/code&gt; to avoid boxing”, the problem is that the method is &lt;em&gt;already&lt;/em&gt; generic in the return type, and in C# when calling a generic method you can either supply all the generic types explicitly, or let the compiler infer all of them; there is no middle ground. Making the parameters a generic would be &lt;em&gt;inconvenient&lt;/em&gt; in all cases, and &lt;em&gt;impossible&lt;/em&gt; in others - we &lt;strong&gt;cannot&lt;/strong&gt; name the &lt;code&gt;T&lt;/code&gt; used in code that uses anonymous types, precisely because it is anonymous. There are hacks around this, but: nothing great&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All in all, I’m happy to just say “use anonymous types for the parameters; use tuples for output rows” - it is a pragmatic compromise that covers the huge majority of useful real-world scenarios. I have added code to &lt;em&gt;detect&lt;/em&gt; &lt;code&gt;ValueTuple&amp;lt;...&amp;gt;&lt;/code&gt; being used in the parameters and raise an appropriate exception guiding the user on the problem an how to fix it.&lt;/p&gt;&lt;h1&gt;&lt;a id="Summary_295"&gt;&lt;/a&gt;Summary&lt;/h1&gt;&lt;p&gt;We’ve explored C# 7 tuples and their implementation. Armed with the implementation details, we’ve explored what that means for code that consumes C# 7 tuples - both upstream and downstream. We’ve discussed the limitations for use with libary code, but we’ve also seen that they still have plenty of uses with library code &lt;em&gt;in some scenarios&lt;/em&gt;. If I’ve missed anything, &lt;a href="https://twitter.com/marcgravell/"&gt;let me know on twitter&lt;/a&gt;!&lt;/p&gt;&lt;h1&gt;Addendum&lt;/h1&gt;&lt;p&gt;After publishing, &lt;a href="https://twitter.com/DanielCrabtree"&gt;Daniel Crabtree&lt;/a&gt; very rightly notes that the &lt;code&gt;Item*&lt;/code&gt; fields only go up to &lt;code&gt;Item7&lt;/code&gt;, and that additional fields are stored in the &lt;code&gt;Rest&lt;/code&gt; field which can itself be a tuple. You can read more about that &lt;a href="https://www.danielcrabtree.com/blog/99/c-sharp-7-dynamic-types-and-reflection-cannot-access-tuple-fields-by-name"&gt;in his post here&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/Q9G7n5T9KxY" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/28626824073231224" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/28626824073231224" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/Q9G7n5T9KxY/exploring-tuples-as-library-author.html" title="Exploring Tuples as a Library Author" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2017/04/exploring-tuples-as-library-author.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-2227655872135991569</id><published>2016-09-17T16:26:00.001-07:00</published><updated>2018-07-30T04:25:39.474-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="pipelines" /><title type="text">Channelling my inner geek</title><content type="html">&lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;I do a lot of IO programming – both from a client perspective (for example, &lt;a href="https://www.nuget.org/packages/StackExchange.Redis/"&gt;StackExchange.Redis&lt;/a&gt;), and from a server perspective (for example, our custom web-sockets server usually has between 350k and 500k open connections, depending on how long it has been since we gave it a kick). So I’m always interested in new “players” when it comes to IO code, and a few weeks ago Microsoft’s &lt;a href="https://github.com/davidfowl/"&gt;David Fowler&lt;/a&gt; came through with some interesting goodies – specifically the “&lt;a href="https://github.com/davidfowl/channels"&gt;Channels&lt;/a&gt;” API.&lt;/p&gt; &lt;p&gt;First, a couple of disclaimers: everything I’m talking about here is completely experimental, evolving rapidly, depends on “&lt;a href="https://github.com/dotnet/corefxlab/"&gt;corefxlab&lt;/a&gt;” concepts (i.e. stuff the .NET team is playing with to see what works well), and is virtually untested. Even the name is purely a working title. But let’s be bold and take a dive anyway!&lt;/p&gt; &lt;h3&gt;What are Channels, and what problem do they solve?&lt;/h3&gt; &lt;p&gt;The short description of Channels would be something like: “high performance zero-copy buffer-pool-managed asynchronous message pipes”. Which is quite a mouthful, so we need to break that down a bit. I’m going to talk primarily in the context of network IO software (aka network client libraries and server applications), but the same concepts apply equally to anything where data goes in or out of a stream of bytes. It is &lt;strong&gt;&lt;em&gt;relatively easy&lt;/em&gt;&lt;/strong&gt; to write a basic client library or server application for a simple protocol; heck, spin up a Socket via a listener, maybe wrap it in a NetworkStream, call Read until we get a message we can process, then spit back a response. But trying to do that &lt;em&gt;efficiently&lt;/em&gt; is remarkably hard – especially when you are talking about high volumes of connections. I don’t want to get clogged down in details, but it quickly gets massively complicated, with concerns like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;threading – don’t want a thread per socket  &lt;li&gt;connection management (listeners starting the read/write cycle when connections are accepted)  &lt;li&gt;buffers to use to wait for incoming data  &lt;li&gt;buffers to use to store any backlog of data you can’t process yet (perhaps because you don’t have an entire message)  &lt;li&gt;dealing with partial messages (i.e. you read all of one message and half of another – do you shuffle the remainder down in the existing buffer? or…?)  &lt;li&gt;dealing with “frame” reading/writing (another way of saying: how do we decode a stream in to multiple successive messages for sequential processing)  &lt;li&gt;exposing the messages we parse as structured data to calling code – do we copy the data out into new objects like string? how “allocatey” can we afford to be? (don’t believe people who say that objects are cheap; they become expensive if you allocate enough of them)  &lt;li&gt;do we re-use our buffers? and if so, how? lots of small buffers? fewer big buffers and hand out slices? how do we handle recycling?&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Basically, before long it becomes an ugly mess of code. And almost none of this has any help from the framework. A few years ago I put together &lt;a href="https://github.com/StackExchange/NetGain/tree/master/StackExchange.NetGain"&gt;NetGain&lt;/a&gt; to help with some of this; NetGain is the code that drives our web-socket server, and has pieces to cover most of the above, so implementing a new protocol mostly involves writing the code to read and write frames. It works OK, but it isn’t &lt;em&gt;great&lt;/em&gt;. Some bits of it are decidedly ropey.&lt;/p&gt; &lt;p&gt;But here’s the good news: &lt;strong&gt;Channels does all of these things, and does it better.&lt;/strong&gt;&lt;/p&gt; &lt;h3&gt;You didn’t really answer my question; what is a Channel?&lt;/h3&gt; &lt;p&gt;Damn, busted. OK; let’s put it like this – a Channel is like a &lt;a href="https://msdn.microsoft.com/en-us/library/system.io.stream(v=vs.110).aspx"&gt;Stream&lt;/a&gt; that pushes data to you rather than having you pull. One chunk of code feeds data into a Channel, and another chunk of code &lt;em&gt;awaits&lt;/em&gt; data to pull from the channel. In the case of a network socket, there are two Channels – one in each direction:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“input” for our application code:  &lt;ul&gt; &lt;li&gt;the socket-aware code borrows a buffer from the memory manager and talks to the socket; as data becomes available it pushes it into the channel then repeats  &lt;li&gt;the application code awaits data from the channel, and processes it &lt;/li&gt;&lt;/ul&gt; &lt;li&gt;“output” for our application code:  &lt;ul&gt; &lt;li&gt;our application may at any point (but typically as a response to a request) choose to write data to the channel  &lt;li&gt;the socket-aware code awaits data from the channel, and pushes it to the network infrastructure&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;In most cases you should not expect to have to write any code at all that talks to the underlying socket; that piece is abstracted away into library code. But if you &lt;em&gt;want &lt;/em&gt;to write your own channel backend you &lt;em&gt;can&lt;/em&gt;. A small selection of backend providers have been written to explore the API surface (mostly focusing on files and networking; there are actually already 3 different backend ‘sockets’ libraries: libuv, Windows RIO, and async managed sockets). So let’s focus on what the application code might look like, with the example of a basic “echo” server:&lt;/p&gt;&lt;pre&gt;IChannel channel = ...&lt;br /&gt;while (true)&lt;br /&gt;{&lt;br /&gt;    // await data from the input channel&lt;br /&gt;    var request = await channel.Input.ReadAsync();&lt;br /&gt;&lt;br /&gt;    // check for "end of stream" condition&lt;br /&gt;    if (request.IsEmpty &amp;amp;&amp;amp; channel.Input.Completion.IsCompleted)&lt;br /&gt;    {&lt;br /&gt;        request.Consumed();&lt;br /&gt;        break;&lt;br /&gt;    }&lt;br /&gt;&lt;br /&gt;    // start writing to the output channel&lt;br /&gt;    var response = channel.Output.Alloc();&lt;br /&gt;    // pass whatever we got as input to the output&lt;br /&gt;    response.Append(ref request);&lt;br /&gt;    // finish writing (this should activate the socket driver)&lt;br /&gt;    await response.FlushAsync();&lt;br /&gt;    // tell the engine that we have read all of that data&lt;br /&gt;    // (note: we could also express that we read *some* of it,&lt;br /&gt;    // in which case the rest would be retained and supplied next time)&lt;br /&gt;    request.Consumed();&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;p&gt;What isn’t obvious here is the types involved; “request” is a ReadableBuffer, which is an abstraction over one or more slices of memory from the buffer pool, from which we can consume data. “response” is a WritableBuffer, which is again an abstraction over any number of slices of memory from the buffer pool, to which we can write data (with more blocks being leased as we need them). The idea is that you can “slice” the request as many times as you need without allocation cost:&lt;/p&gt;&lt;pre&gt;var prefix = request.Slice(0, 4);&lt;br /&gt;var len = prefix.Read&amp;lt;int&amp;gt;();&lt;br /&gt;var payload = request.Slice(4, len);...&lt;/pre&gt;&lt;p&gt;Note that ReadableBuffer is a value-type, so slicing it like this is not allocating - the new value simply has a different internal tracking state of the various buffers. Each block is also &lt;em&gt;reference counted&lt;/em&gt;, so the block is only made available back to the pool when it has been fully released. Normally this would happen as we call “Consumed()”, but it has more interesting applications. The “echo” server already gave an example of this – the “Append” call incremented the counter, which means that the data remains valid even after “request.Consumed()” has been called. Instead, it is only when the &lt;strong&gt;socket driver&lt;/strong&gt; says that it too has consumed it, that it gets released back to the pool. But we can do the same thing ourselves to expose data to our application without any copies; our previous example can become:&lt;/p&gt;&lt;pre&gt;var prefix = request.Slice(0, 4);&lt;br /&gt;var len = prefix.Read&lt;int&gt;();&lt;br /&gt;var payload = request.Slice(4, len).Preserve();...&lt;/pre&gt;&lt;p&gt;Here “Preserve” is essentially “increment the reference counter”. We can then pass that value around and the data will be valid until we “Dispose()” it (which will decrement the counter, releasing it to the pool if it becomes zero). Some of you might now be twitching uncontrollably at the thought of explicit counting, but for high volume scenarios it is far more scalable than garbage collection, allocations, finalizers, and all that jazz.&lt;/p&gt;&lt;h4&gt;So where did the zero copy come in?&lt;/h4&gt;&lt;p&gt;A consequence of this is that the memory we handed to the network device is the &lt;strong&gt;exact same memory&lt;/strong&gt; that our application code can access for values. Recall that the socket-aware code asked for a buffer from the pool and then handed the populated data to the channel. That is the exact same blocks as the block that “payload” is accessing. Issues like data backlog is handled simply by only releasing blocks once the consumer has indicated that they’ve consumed all the data we know valid on that block; if the caller only reads 10 bytes, it just slices into the existing block. Of course, if the application code chooses to access the data in terms of “string” etc, there are mechanisms to read that, but the data can &lt;em&gt;also&lt;/em&gt; be accessed via the new “Span&amp;lt;byte&amp;gt;” API, and potentially as “Utf8String” (although there are some complications there when it comes to data in non-contiguous blocks).&lt;/p&gt;&lt;h4&gt;Synopsis&lt;/h4&gt;&lt;p&gt;It looks really &lt;strong&gt;really&lt;/strong&gt; exciting for anyone involved in high performance applications – as either a client or server. Don’t get me wrong: there’s no “magic” here; most of the tricks can already be done and are done, but it is &lt;strong&gt;really hard&lt;/strong&gt; to do this stuff well. This API essentially tries to make it easy to get it right, but at the same time has performance considerations at every step by some folks who really really care about absurd performance details.&lt;/p&gt;&lt;h4&gt;But is it practical? How complete is it?&lt;/h4&gt;&lt;p&gt;It is undeniably very early days. But is is enough to be useful. There are a range of trivially simple examples in the repository, and some non-trivial; David Fowler is looking to hack the entire thing underneath ASP.NET to see what kind of difference it makes. At the less ambitious end of things, I have re-written the web-sockets code from NetGain on top of the Channels API; and frankly, it was way less work than you might expect. This means that there is a &lt;a href="https://github.com/StackExchange/Channels.WebSockets/"&gt;fully working web-sockets server and web-sockets client&lt;/a&gt;. I’ve also been playing with the &lt;a href="https://github.com/StackExchange/StackExchange.Redis/tree/channels/RedisCore"&gt;StackExchange.Redis multiplexed redis client code on top of Channels&lt;/a&gt; (although that is far less complete). So it is very definitely possible to implement non-trivial systems. But be careful: the API is changing rapidly – you definitely shouldn’t be writing your mission critical pacemaker monitoring and ambulance scheduling software on top of this just yet!&lt;/p&gt;&lt;p&gt;There’s also a lot of missing pieces – SSL is a huge hole that is going to take a lot of work to fill; and probably a few more higher level abstractions to make the application code even simpler (David has mentioned a frame encoder/decoder layer, for example). The most important next step is to explore what the framework offers in terms of raw performance: how much “better” does it buy you? I genuinely don’t have a simple answer to that currently, although I’ve been trying to explore it.&lt;/p&gt;&lt;h4&gt;Can I get involved?&lt;/h4&gt;&lt;p&gt;If you want to explore the API and help spot the missing pieces, the ugly pieces, and the broken pieces – and perhaps even offer the fixes, then the &lt;a href="https://github.com/davidfowl/Channels"&gt;code is on GitHub&lt;/a&gt; and there’s a &lt;a href="https://gitter.im/davidfowl/Channels"&gt;discussion room&lt;/a&gt;. Note: nothing here is up to me – I’m just an interested party; David Fowler is commander in chief on this.&lt;/p&gt;&lt;h4&gt;And what next?&lt;/h4&gt;&lt;p&gt;And then, once all those easy hurdles have been jumped – then starts the &lt;em&gt;really really&lt;/em&gt; hard work: figuring out what, in terms of formal API names, to call it.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/wv4iYF3PjPI" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2227655872135991569" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2227655872135991569" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/wv4iYF3PjPI/channelling-my-inner-geek.html" title="Channelling my inner geek" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2016/09/channelling-my-inner-geek.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-6956192531818964970</id><published>2016-05-09T14:32:00.001-07:00</published><updated>2016-05-12T04:04:31.375-07:00</updated><title type="text">CUDA–getting started in .NET</title><content type="html">&lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;In my two previous posts (&lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag.html"&gt;part 1&lt;/a&gt;, &lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag_9.html"&gt;part 2&lt;/a&gt;) I described how &lt;strong&gt;&lt;em&gt;we&lt;/em&gt;&lt;/strong&gt; have found uses for CUDA. I am, however, aware that this abstract text doesn’t necessarily help the reader (aka “you”) make inroads into the world of CUDA. So: the purpose of this post is to take the concepts discussed previously and put them together into some actual code.&lt;/p&gt; &lt;p&gt;Firstly, a word on OpenCL: so far, I’ve spoken about CUDA – NVIDIA’s framework for GPGPU programming. Other frameworks exist, and the largest “other” is undoubtedly &lt;a href="https://en.wikipedia.org/wiki/OpenCL"&gt;OpenCL&lt;/a&gt;. This framework is designed to let you use the same code over a range of technologies – GPUs from different providers, CPUs, etc. I am aware of it – I just haven’t even tried to address it. If that’s your bag: have fun. I don’t feel hugely limited by restricting myself to CUDA &lt;em&gt;for what I need&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The first things you need for CUDA, then, are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a CUDA capable GPU device  &lt;li&gt;the &lt;a href="https://developer.nvidia.com/cuda-toolkit"&gt;CUDA toolkit&lt;/a&gt;  &lt;li&gt;a programming environment&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Since I’m focused on .NET, we’re also going to need one of the tools for interacting with CUDA from .NET, or we’ll need to write &lt;code&gt;extern&lt;/code&gt; wrappers for the C API. A range of CUDA tools for .NET exist, including &lt;a href="https://cudafy.codeplex.com/"&gt;CUDAfy.NET&lt;/a&gt;, &lt;a href="http://kunzmi.github.io/managedCuda/"&gt;managedCUDA&lt;/a&gt;, &lt;a href="http://www.quantalea.com/"&gt;Alea GPU / QuantAlea&lt;/a&gt;, &lt;a href="http://campynet.com/"&gt;Campy.NET&lt;/a&gt;, and several others that Campy.NET &lt;a href="http://campynet.com/"&gt;list and describe on their home page&lt;/a&gt;. These fall into two major categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;wrappers for the driver API (you write your kernels in C etc and launch them from C#)  &lt;li&gt;IL-to-PTX generators (you write and launch your kernels in C#)&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;While I’m a huge fan of C#, I’m also a huge fan of keeping close to the metal, and in a heartbeat will sacrifice &lt;em&gt;some &lt;/em&gt;programming convenience for performance.&lt;/p&gt; &lt;h2&gt;Our first kernel&lt;/h2&gt; &lt;p&gt;So without further ado, I’m going to jump straight in with a basic CUDA kernel written in C. Rather than trying to discuss the tag-engine, we’re going to do something simpler… like multiplying numbers. Multiplication is hard, right?&lt;/p&gt;&lt;pre&gt;typedef struct {&lt;br /&gt;    int Id;&lt;br /&gt;    unsigned int Value;&lt;br /&gt;} SomeBasicType;&lt;br /&gt;&lt;br /&gt;extern "C" {&lt;br /&gt;    __global__ void Multiply(const int N, SomeBasicType* __restrict data, int factor) {&lt;br&gt;        for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &amp;lt; N;&lt;br&gt;            i += blockDim.x * gridDim.x)&lt;br&gt;        {&lt;br /&gt;            (data + i)-&amp;gt;Value *= factor;&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;p&gt;The first thing this does is to define a data-type (&lt;code&gt;SomeBasicType&lt;/code&gt;) for us to share data between the CPU and GPU code. The &lt;code&gt;__global__&lt;/code&gt; indicates that we’re declaring a kernel, along with a given name and signature. We’re passing in &lt;code&gt;N&lt;/code&gt;, the number of elements to process, &lt;code&gt;data&lt;/code&gt;, a pointer to some of the data to work against, and &lt;code&gt;factor&lt;/code&gt; - the number to multiply the values by. The significance of the &lt;code&gt;N&lt;/code&gt; comes into play when we see the &lt;code&gt;i &amp;lt; N&lt;/code&gt; in the &lt;code&gt;for&lt;/code&gt; loop on the next line. Recall that we usually launch a number of blocks, each consisting of a set number of threads. It is not always the case that the number of elements to process is a convenient multiple of some block size, so it is common for kernels to be passed a count to compare the position against, exiting if the current thread is not needed. This sounds like a contradiction to the “don’t branch” guidance, but as long as &lt;em&gt;most&lt;/em&gt; of the threads make the same choice, this isn’t going to be a performance problem.&lt;/p&gt;&lt;p&gt;Recall also that I described a “monolithic kernel” earlier, where-by we launch a thread for every element to process. But: there is a hard limit to the number of threads in a block, and the number of blocks in any dimension – and as always: the more threads there are, the more management overhead there is (although this works differently to CPU threads). As such, it is common to use a “grid-stride loop” rather than a monolithic kernel. As in our example, a &lt;code&gt;for&lt;/code&gt; loop is used to that each thread processes multiple elements. But unlike the CPU version shown previously (where each thread processed a separate chunk of data), we instead have &lt;em&gt;each thread in an iteration&lt;/em&gt; process adjacent elements, and then move forwards by the &lt;em&gt;stride&lt;/em&gt; – namely the width of each block multiplied by the number of blocks. This has multiple side benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;it can cope with data of any size – it is not subject to the 75M limitation  &lt;li&gt;it requires less overhead for the GPU engine  &lt;li&gt;it &lt;em&gt;can still be used&lt;/em&gt; as a monolithic kernel (by setting the dimensions such that it only performs one iteration per thread), or as a single-threaded debugging kernel that processes all the data (by setting the dimensions to 1 thread in 1 block)&lt;/li&gt;&lt;li&gt;the work being done by a warp or block is contiguous, minimizing memory access overheads&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Finally, we get to the actual meat of the kernel, which does an in-place multiplication of one of the fields of our data by one of the parameters.&lt;/p&gt;&lt;p&gt;That’s our first kernel, all done!&lt;/p&gt;&lt;h2&gt;A side note on data width&lt;/h2&gt;&lt;p&gt;You’ll note that the above example is doing 32-bit arithmetic. For doing serious CUDA work, it is important to be aware of whether you are doing 16-bit, 32-bit or 64-bit arithmetic, as it can significantly impact your choice of hardware – considering them in release order:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the “Kepler” architecture (server devices: K40, K80) will serve as a baseline – let’s arbitrarily call it “good” at both 32-bit and 64-bit; 16-bit operations are performed using 32-bit arithmetic  &lt;li&gt;the “Maxwell” architecture (server device: M40) is faster than Kepler at 32-bit operations, but is relatively poor at 64-bit operations; 16-bit operations are performed using 32-bit arithmetic  &lt;li&gt;to complicate this: the fact that the K80 packs two devices on a physical package means that even though a single M40 may be faster than a single K80 device, code that appropriately uses both K80 devices on a package may still out-perform a single M40  &lt;li&gt;the “Pascal” architecture apparently (not yet available for testing; server device: P100) significantly increases both 32-bit and 64-bit performance, while also introducing specific optimized instructions for 16-bit processing&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Because of this, there is no single way of saying “this device is faster” – it depends a lot on what exactly you are doing!&lt;/p&gt;&lt;h2&gt;Compiling our kernel (and: hello managedCUDA)&lt;/h2&gt;&lt;p&gt;A kernel as a text source-file isn’t much use by itself. We need to convert that into GPU-runnable code – either intermediate (“PTX”) or fully compiled to bytecode (“cubin”). There is also a middle-ground that combines both bytecode and intermediate instructions – a “fatbin”; this allows a compiled kernel to target multiple physical architectures and still allow usage on other architectures via a JIT step. There are two common ways of compiling a C kernel – compile-time (NVCC) and runtime (NVRTC). I’ve &lt;a href="http://blog.marcgravell.com/2016/05/using-windows-subsystem-for-linux-to.html"&gt;spoken previously about using NVCC&lt;/a&gt;, so for this example I’ll use NVRTC instead.&lt;/p&gt;&lt;p&gt;Since we want to do this from .NET, we need a wrapper library. My examples today are going to use managedCUDA, written by Michael Kunz. This is &lt;a href="https://www.nuget.org/packages/ManagedCuda-75-x64/"&gt;available on NuGet&lt;/a&gt; and is licenced under the LGPL – which I understand (and: IANAL) is &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; the same as the notoriously infectious and restrictive GPL. What I particularly like about this library is that &lt;em&gt;in addition&lt;/em&gt; to exposing an idiomatic .NET object model that maps to the C API, it also exposes the raw C API directly – which makes it convenient to use even when the method you want doesn’t map naturally to a .NET concept.&lt;/p&gt;&lt;p&gt;So let’s load our kernel from a text file and compile it at runtime:&lt;/p&gt;&lt;pre&gt;string path = "MyKernels.c";&lt;br /&gt;ManagedCuda.NVRTC.nvrtcResult result;&lt;br /&gt;using (var rtc = new ManagedCuda.NVRTC.CudaRuntimeCompiler(&lt;br&gt;    File.ReadAllText(path), Path.GetFileName(path)))&lt;br /&gt;{&lt;br /&gt;    try {&lt;br /&gt;        rtc.Compile(new string[0]);&lt;br /&gt;        result = ManagedCuda.NVRTC.nvrtcResult.Success;&lt;br /&gt;    } catch(ManagedCuda.NVRTC.NVRTCException ex) {&lt;br /&gt;        result = ex.NVRTCError;&lt;br /&gt;    }&lt;br /&gt;    log = rtc.GetLogAsString();&lt;br /&gt;&lt;br /&gt;    if (result == ManagedCuda.NVRTC.nvrtcResult.Success)&lt;br /&gt;    {&lt;br /&gt;        byte[] ptx = rtc.GetPTX();&lt;br /&gt;        // ...&lt;br /&gt;    }&lt;br /&gt;}&lt;/pre&gt;&lt;p&gt;This takes our file contents as input, and (if successful) produces a blob that contains the compiled kernel. We can also obtain the log which will contain detained error messages if our kernel is invalid.&lt;/p&gt;&lt;h2&gt;The CUDA context&lt;/h2&gt;&lt;p&gt;One of the key objects in CUDA is the &lt;em&gt;context&lt;/em&gt;. This wraps the state for a long-running CUDA scenario. Most operations, including memory allocation and kernel launches typically happen within a CUDA context. managedCUDA exposes a class that wraps this. Note:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;a CUDA context is bound to a particular GPU device, and it is when creating the CUDA context that you can target specific devices  &lt;li&gt;a CUDA context is associated with the CPU thread that creates it; as such, CUDA code would work well for scenarios such as a dedicated worker thread processing items from a queue, but would &lt;strong&gt;not&lt;/strong&gt; work well from .NET &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; code that switches between multiple threads&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So; let’s create our context and load the kernel:&lt;/p&gt;&lt;pre&gt;CudaContext ctx = new CudaContext(deviceId, true);&lt;br /&gt;CudaKernel multiply = ctx.LoadKernelFatBin(ptx, "Multiply");&lt;/pre&gt;&lt;p&gt;Note that the context (like many CUDA objects) is &lt;code&gt;IDisposable&lt;/code&gt;, and you should ensure to &lt;code&gt;Dispose()&lt;/code&gt; it properly, via &lt;code&gt;using&lt;/code&gt; or otherwise.&lt;/p&gt;&lt;h2&gt;CUDA streams and asynchronous processing&lt;/h2&gt;&lt;p&gt;In the simplest usage, the CUDA methods are blocking, however it is often the case that either the CPU can do other useful work while the GPU churns, or we can overlap multiple GPU operations – memory transfers running in parallel with kernel execution, or (depending on the hardware) even multiple kernels running at the same time. To allow this, CUDA introduces the notion of a “stream”. Work on the same stream is processed in order, but the work on different streams may overlap. I’m not going to make extensive usage of streams in this walkthrough, but it is easier to include stream considerations from the start than to add it later.&lt;/p&gt;&lt;h2&gt;Initializing memory at the host and on the device&lt;/h2&gt;&lt;p&gt;Traditionally, the host (CPU) memory and the device (GPU) memory were completely separate, and all CUDA code had to specifically be cautious of whether it was referring to host or device memory. More recently, the concept of “unified memory” has been introduced that hides this distinction, and performs transfers automatically as needed – it is intended to make it &lt;em&gt;simpler&lt;/em&gt; to use the API, but for the maximum performance it is recommended to take full control over allocation and copy. To that end, I’ll stick to the traditional approach, which means explicitly allocating the data at both ends.&lt;/p&gt;&lt;p&gt;We’ll start by allocating the host memory:&lt;/p&gt;&lt;pre&gt;struct SomeBasicType {&lt;br /&gt;    public int Id;&lt;br /&gt;    public uint Value;&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;IntPtr hostPointer = IntPtr.Zero;&lt;br /&gt;var res = DriverAPINativeMethods.MemoryManagement.cuMemAllocHost_v2(&lt;br /&gt;    ref hostPointer, count * sizeof(SomeBasicType));&lt;br /&gt;if (res != CUResult.Success) throw new CudaException(res);&lt;br /&gt;SomeBasicType* hostBuffer = (SomeBasicType*)hostPointer;&lt;/pre&gt;&lt;p&gt;As before, we declare a raw data type to share the data between CPU and GPU. Here is one of those cases where I’m making use of the raw C API rather than the wrapped objects, as this (IMO) more conveniently allows naked allocations. You might think “hang on, you’re just allocating some memory – surely a .NET array would suffice, or maybe at worst &lt;code&gt;Marshal.AllocHGlobal&lt;/code&gt;” – but: in order for the CUDA memory-copy engine to work at maximum performance (avoiding double-copies, etc) it is required to be configured in a specific way, and the best way to guarantee that is to let the CUDA API allocate the memory in the first place. A pinned .NET array certainly wouldn’t suffice. Or at least, it would work – but not as fast. Yes, this needs some &lt;code&gt;unsafe&lt;/code&gt; code; it’ll be fine, don’t panic.&lt;/p&gt;&lt;p&gt;Once allocated, we can intialize this memory with some invented data:&lt;/p&gt;&lt;pre&gt;for (int i = 0; i &amp;lt; count; i++) {&lt;br /&gt;    hostBuffer[i].Id = i;&lt;br /&gt;    hostBuffer[i].Value = (uint)i;&lt;br /&gt;}&lt;/pre&gt;&lt;p&gt;Next we’ll want to allocate a similar block of memory on the device, and start copying the data from the host to the device. Emphasis: large memory transfers are fast, but not instant – if possible you should minimize the amount of data you need to transfer for individual operations. The ideal scenario is to copy the data to the device once / periodically, and use it many times once it is there.&lt;/p&gt;&lt;pre&gt;CudaDeviceVariable&amp;lt;SomeBasicType&amp;gt; deviceBuffer = new CudaDeviceVariable&amp;lt;SomeBasicType&amp;gt;(count);&lt;br /&gt;CudaStream defaultStream = new CudaStream();&lt;br /&gt;res = DriverAPINativeMethods.AsynchronousMemcpy_v2.cuMemcpyHtoDAsync_v2(deviceBuffer.DevicePointer,&lt;br /&gt;    hostPointer, deviceBuffer.SizeInBytes, defaultStream.Stream);&lt;br /&gt;if (res != CUResult.Success) throw new CudaException(res);&lt;/pre&gt;&lt;p&gt;This time I'm using the &lt;code&gt;CudaDeviceVariable&amp;lt;T&amp;gt;&lt;/code&gt; object to represent the memory at the device, which is perfectly convenient since we will not usually need to do anything except access the &lt;code&gt;DevicePointer&lt;/code&gt; to pass into methods. Once again I'm using a raw C-style API for the actual memory copy. There are some transfer methods exposed directly on the object, but this particular combination is not exposed directly. Note that because we have created and used a stream, this is non-blocking. Related operations sent to the same stream will be queued behind this one.&lt;/p&gt;&lt;h2&gt;Launching our kernel&lt;/h2&gt;&lt;p&gt;We have data on the device; we have a kernel that we have compiled and loaded. Now finally to run the kernel against the data! First we must configure the &lt;em&gt;dimensions&lt;/em&gt; – the number of threads in a block and the number of blocks:&lt;/p&gt;&lt;pre&gt;multiply.BlockDimensions = new ManagedCuda.VectorTypes.dim3(threadsPerBlock, 1, 1);&lt;br /&gt;multiply.GridDimensions = new ManagedCuda.VectorTypes.dim3(blockCount, 1, 1);&lt;br /&gt;&lt;br /&gt;multiply.RunAsync(defaultStream.Stream, new object[] {&lt;br /&gt;    count, deviceBuffer.DevicePointer, value&lt;br /&gt;});&lt;/pre&gt;&lt;p&gt;Note that the parameters to &lt;code&gt;RunAsync&lt;/code&gt; (other than the stream) match the signature on the kernel itself. &lt;code&gt;value&lt;/code&gt; in the above is the factor to multiply by. Once we've got the data in place, running kernels is actually alarmingly simple!&lt;/p&gt;&lt;h2&gt;Getting results back&lt;/h2&gt;&lt;p&gt;It isn’t usually sufficient to run kernels – sadly we usually expect to get results. As mentioned before, we should strive to minimize the amount of data we want to transfer, but &lt;em&gt;for simplicity in this example&lt;/em&gt; I’m going to fetch back the entire buffer, over-writing the previous contents. In real code you might be extracting only small portions of the buffer, or summary / aggregate data.&lt;/p&gt;&lt;pre&gt;var res = DriverAPINativeMethods.AsynchronousMemcpy_v2.cuMemcpyDtoHAsync_v2(&lt;br /&gt;    new IntPtr(hostBuffer), deviceBuffer.DevicePointer,&lt;br /&gt;    deviceBuffer.SizeInBytes, defaultStream.Stream);&lt;br /&gt;if (res != CUResult.Success) throw new CudaException(res);&lt;/pre&gt;&lt;p&gt;Once more, for memory operations it has been easier to use the raw C API than the wrapped managed API, &lt;em&gt;and that's OK&lt;/em&gt;. Note that because we've been using streams, at this point we've only &lt;em&gt;queued&lt;/em&gt; the copy. If we've set all our wheels in motion, and want to sit back and wait while they turn, we can use either &lt;code&gt;ctx.Synchronize();&lt;/code&gt; (to wait for all streams to complete) or &lt;code&gt;defaultStream.Synchronize()&lt;/code&gt; (to wait for a specific stream to complete). Finally, when that is done, we can iterate over the data and congratulate ourselves for managing to over-complicate the multiplication of numbers!&lt;/p&gt;&lt;h2&gt;Not enough code for you?&lt;/h2&gt;&lt;p&gt;Everything I’ve shown here is available in a &lt;a href="https://github.com/mgravell/SimpleCUDAExample"&gt;github project&lt;/a&gt;, that you can clone, fork, etc. I don’t claim it is the most useful tool ever, but hopefully it might help a few people get up and running with CUDA.&lt;/p&gt;&lt;p&gt;Enjoy!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/r4btIYnPjkU" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6956192531818964970" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/6956192531818964970" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/r4btIYnPjkU/cudagetting-started-in-net.html" title="CUDA–getting started in .NET" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2016/05/cudagetting-started-in-net.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-8769924495263250964</id><published>2016-05-09T08:53:00.001-07:00</published><updated>2016-05-11T01:16:31.833-07:00</updated><title type="text">How I found CUDA, or: Rewriting the Tag Engine–part 2</title><content type="html">&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag.html"&gt;part 1&lt;/a&gt;, &lt;a href="http://blog.marcgravell.com/2016/05/cudagetting-started-in-net.html"&gt;part 3&lt;/a&gt;)&lt;/p&gt; &lt;h2&gt;So we got to work…&lt;/h2&gt;&lt;br&gt; &lt;p&gt;We &lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag.html"&gt;knew we had a problem&lt;/a&gt; where CUDA might be part of an answer, especially for performance. And in reality, there’s only one way to find out which horse is faster. &lt;a href="https://ericlippert.com/2012/12/17/performance-rant/"&gt;You race the horses&lt;/a&gt;. We knew that the v1 tag-engine was poorly designed (organically, so to speak), so it wouldn’t be meaningful to compare that for performance purposes. And even if the GPU is better, we still wanted a better CPU implementation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;it needs to run on developer local machines; not all developers have a CUDA device, especially if they are on a laptop, or on a VM, or on a VM on a laptop  &lt;li&gt;in the event of the tag-engine servers being unreachable, our fallback strategy is to load the tag-engine in-memory on the production web-servers, so it needs to be able to run there  &lt;li&gt;if the GPU approach turns out to not be what we want, we still want to have moved our code forwards  &lt;li&gt;and if the GPU approach turns out to be &lt;strong&gt;exactly&lt;/strong&gt; what we want, then we’d have physical factors like server hardware, cage configuration, etc to add lead-time, where-as we’d like to replace the code ASAP&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;So: we definitely wanted a shiny new CPU version, but one that was designed to work nicely side-by-side with the GPU version. The other thing you need to race your horses is: a course. The best way of representing your load is to &lt;em&gt;capture your load&lt;/em&gt;, in a way that you can play back. So the next thing we did was to edit our existing tag-engine to make it possible to record all the real requests it was serving, in a way that we could play back and use to compare performance, features, and results. Helpfully, tag-engine requests don’t contain any &lt;a href="https://en.wikipedia.org/wiki/Personally_identifiable_information"&gt;PII&lt;/a&gt; data – note that this isn’t possible in all circumstances.&lt;/p&gt; &lt;h2&gt;Parallel or sequential?&lt;/h2&gt; &lt;p&gt;Next up, we need to think about how GPUs work. CPUs are &lt;em&gt;often&lt;/em&gt; (not always) used to perform multiple independent operations in parallel. GPUs, by contrast, are usually used to make a single operation (per device) happen &lt;em&gt;really quickly&lt;/em&gt;, processing the operations sequentially. The parallel approach, while scalable, has a lot of complications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you need to be careful how you perform data updates without breaking running operations (or: you need to duplicate the data to perform updates) – contrast sequential, where you can just squeeze the update in as just one more thing in the queue (it is never fighting other requests), and update the data in-place with impunity  &lt;li&gt;each parallel operation needs memory for it’s processing (the results it has collected, and everything it needed to get there) – contrast sequential where you can allocate a single workspace and just keep re-using it&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;After considering these factors, and looking at our typical request rate, we concluded that our best approach for &lt;strong&gt;both GPU and CPU &lt;/strong&gt;would be to use sequential operations, using all the resources available to us (cores, etc) to make each individual operation as fast as possible. This also makes for a very simple CPU vs GPU race, while also making for some amusing CPU task-manager charts:&lt;/p&gt; &lt;p&gt;&lt;img src="https://pbs.twimg.com/media/CgLs7-RXIAE8jTo.jpg:large" width="520" height="350"&gt;&lt;/p&gt; &lt;p&gt;(yes, that’s me punishing a 36-core, HT-enabled server for 72 logical cores of goodness)&lt;/p&gt; &lt;h2&gt;The CPU bits&lt;/h2&gt; &lt;p&gt;Recall &lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag.html"&gt;from part 1&lt;/a&gt; that we have two categories of query; trivially simple, and absurdly hard. I’m going to completely ignore the first set. The interesting case is the second, and in those scenarios you can pretty much guarantee that there is no &lt;em&gt;single&lt;/em&gt; index that is ever going to fit your query, and we’re essentially trying to do a table-scan of a “where” clause, perhaps with some short-cuts. Let’s consider an example. Say we want to know all the “java and android” questions, sorted by activity. One of the main things the tag-engine stores is an index of “questions by tag” – i.e. given a tag, what questions exist in that tag (pre-sorted). Since the “and” makes this restrictive (intersection rather than union), what we can do is choose the smallest (“android”, which takes us to a manageable size) and then just &lt;strong&gt;test them all&lt;/strong&gt;. For reasonable sizes of data, this can be much cheaper than trying to do complicated index combinations, and can be done with very little memory allocation. Note that we almost always want the total count and the “intersecting related tags” data, so it won’t help to cheat and just scroll forwards until we have enough data to return.&lt;/p&gt; &lt;p&gt;Let’s say that “android” has 800,000 questions. If we want to parallelize this work (to use all of the available cores), it might be tempting to use &lt;a href="https://msdn.microsoft.com/en-gb/library/system.threading.tasks.parallel.foreach(v=vs.110).aspx"&gt;&lt;code&gt;Parallel.ForEach&lt;/code&gt;&lt;/a&gt; to outsource this work to the TPL and aggregate the results, but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;this has quite a lot of overhead in terms of doing lots of small things instead of a small number of big things  &lt;li&gt;since the order is now unpredictable, it makes it very hard to exploit the fact that we have pre-sorted the data&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;Instead, what we can do is to carve these 800,000 questions into a small number of chunks of contiguous questions (8, say, or something related to the number of available cores), and then inside each chunk (unit of work): test each question in turn for the &lt;strong&gt;remaining conditions&lt;/strong&gt;, writing successive matches &lt;strong&gt;in blocks&lt;/strong&gt; to a similarly sized workspace.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-qlYk8D8mDys/VzCybJxB7II/AAAAAAAAD1Y/xwFyLc1Y714/s1600-h/image%25255B3%25255D.png"&gt;&lt;img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-R057tySJEYk/VzCybjgvOMI/AAAAAAAAD1c/WrW1V2eUQuM/image_thumb%25255B1%25255D.png?imgmax=800" width="543" height="206"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By letting different threads process different blocks, we get lots of benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the number of units-of-work for the threading library (TPL) to manage is minimal, reducing overheads  &lt;li&gt;each unit-of-work is always looking at contiguous data, maximizing cache usage and memory locality  &lt;li&gt;we get to exploit the fact that we’ve already sorted the data once – we never need to re-sort  &lt;li&gt;we don’t have to synchronize when writing any of the results, since each unit-of-work is writing to a separate area of the workspace  &lt;li&gt;all we need to know is how many matches each unit-of-work contained and we can step through the final workspace very efficiently&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;This is actually pretty simple to do with &lt;a href="https://msdn.microsoft.com/en-gb/library/system.threading.tasks.parallel.invoke(v=vs.110).aspx"&gt;&lt;code&gt;Parallel.Invoke&lt;/code&gt;&lt;/a&gt; - something like:&lt;/p&gt;&lt;pre&gt;int chunks = DecideNumberOfChunks(questions, cpuCores);&lt;br /&gt;int workPerChunk = questions / chunks;&lt;br /&gt;WorkUnit[] workUnits = new WorkUnit[chunks];&lt;br /&gt;Action[] actions = new Action[chunks];&lt;br /&gt;for(int chunk = 0; chunk &amp;lt; chunks ; chunk++) {&lt;br /&gt;    int start = chunk * workPerChunk,&lt;br /&gt;        stop = Math.Min(start + workPerChunk, questions);&lt;br /&gt;    workUnits[chunk] = new WorkUnit(start, stop, ...);&lt;br /&gt;    actions[chunk] = workUnits[chunk].Execute;&lt;br /&gt;}&lt;br /&gt;Parallel.Invoke(actions);&lt;br /&gt;&lt;/pre&gt;&lt;p&gt;Where each &lt;code&gt;Execute&lt;/code&gt; method is essentially:&lt;/p&gt;&lt;pre&gt;public void Execute() {&lt;br /&gt;    int matches = 0, resultIndex = start;&lt;br /&gt;    for(int i = start ; i &amp;lt; stop ; i++) {&lt;br /&gt;        if(IsMatch(i)) { // some test or set of composite tests&lt;br /&gt;            resultWorkspace[resultIndex++] = i;&lt;br /&gt;            matches++;&lt;br /&gt;        }&lt;br /&gt;    }&lt;br /&gt;    this.Matches = matches;&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;p&gt;The above is a &lt;strong&gt;very simplified &lt;/strong&gt;illustration of the design that drives the CPU implementation of the re-written tag-engine. The result is that it creates a packed set of contiguous matches for each unit-of-work, while allowing us to scale the query effectively over all the available CPU cores. The &lt;code&gt;IsMatch&lt;/code&gt; method might be non-trivial, of course. We use a combination of meta-programming and special-cased tag-tests to allow us to support a wide range of queries; it works very well. So how does this compare to GPU? How does it change our approach?&lt;/p&gt;&lt;h2&gt;CUDA: Kernels, Threads, Warps, Blocks and Grids&lt;/h2&gt;&lt;p&gt;I’m going to run through some high level CUDA concepts now &lt;em&gt;before&lt;/em&gt; showing any CUDA code, and before showing how it relates to the tag-engine.&lt;/p&gt;&lt;p&gt;When developing with CUDA, the first things you need to learn about are kernels, blocks and grids. A &lt;em&gt;kernel&lt;/em&gt; is just a chunk of your code that you can invoke on the GPU device. But unlike regular code, when you “launch” a kernel, you aren’t usually asking it to call it once; you’re asking it to call the exact same method lots of times. As a trivial example, we could ask it to call the same kernel 800,000 times – once for each of our 800,000 “android” questions. A kernel designed to be called once per input element is sometimes called a “monolithic” kernel. But that isn’t quite the end of the story. Each separate invoke of the kernel is a “thread” – so in this monolithic case, we’d be launching 800,000 threads – but we don’t just ask for 800,000 – instead we might ask it to launch 3125 “blocks”, with each block consisting of 256 “threads”. Each of these numbers has limits – a block is limited to a maximum of 1024 threads on most current hardware, and you can have a maximum of 65535 blocks (per dimension, but that’s a separate story).&lt;/p&gt;&lt;p&gt;This means that for a single-dimension kernel, there’s a limit of about 67M, but don’t worry – I’ll cover how to get around that later. All I’m trying to do here is give an overview of the topology of what we’re playing in. The significance of these different concepts is that per-clock-cycle, each symmetric multiprocessor in a GPU actually works on &lt;strong&gt;multiple&lt;/strong&gt; threads in the same block. This ability to operate on multiple threads at once is what makes GPUs so much more powerful (for some tasks) than CPUs. This group of threads that are being controlled in unison is a “warp” (the warp-size is 32 in current hardware).&lt;/p&gt;&lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-cfIv-IUiuTQ/VzCyb7bt3iI/AAAAAAAAD1g/Xh44Bd1YTWQ/s1600-h/image%25255B7%25255D.png"&gt;&lt;img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-hENw8s4iJVs/VzCycUotH4I/AAAAAAAAD1k/p--deJ2WA_o/image_thumb%25255B3%25255D.png?imgmax=800" width="447" height="305"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The above is then duplicated for however many blocks you asked for – 3125 blocks of 256 threads in our example, with a warp-size of 32. In simple examples we’re often just talking about 1 dimension, but note that CUDA blocks and threads are actually 3-dimensional; this means you can actually have 65535 x 65535 x 65535 blocks, with 1024 threads per block – but… you probably shouldn’t ever do that! That’s not the way to solve the “more than 67M” problem.&lt;/p&gt;&lt;h2&gt;Why do I need to know?&lt;/h2&gt;&lt;p&gt;There are a few reasons why you need to understand the topology; firstly, in addition to knowing the limits of what you can ask for, it impacts things like memory locality and cache. Remember in the CPU example we optimized by having each thread work on a different chunk of data? In CUDA, a warp of threads will be executing at once (per symmetric multiprocessor), and the infrastructure is built specifically to share memory lookup costs between a warp. Because of this, you usually want all the threads in a warp looking at adjacent data.&lt;/p&gt;&lt;p&gt;The “secondly” is perhaps even more important: the ability of a symmetric multiprocessor to progress multiple threads &lt;strong&gt;&lt;em&gt;simultaneously &lt;/em&gt;&lt;/strong&gt;is dependent upon each of those threads &lt;strong&gt;&lt;em&gt;doing exactly the same thing&lt;/em&gt;&lt;/strong&gt;. Which makes sense when you think about it. This doesn’t mean that your CUDA code can’t ever branch (&lt;code&gt;if&lt;/code&gt;, etc - basically, any decision point), but it does mean that if different threads in a warp &lt;em&gt;branch in different directions&lt;/em&gt;, then the symmetric multiprocessor has to identify the ones in different states and progress them separately. Which means: you kill the performance.&lt;/p&gt;&lt;p&gt;So: aim to work on adjacent data, and try to branch in a single direction.&lt;/p&gt;&lt;h2&gt;How does this impact tag-engine matching?&lt;/h2&gt;&lt;p&gt;If we go back to the index that we described in the CPU example, we are now essentially applying all our tests in parallel (at least, in a single kernel launch; how the hardware schedules it is up to the hardware). With this setup, we can’t really have the concept of “write to the next position in the result index and increment the write-position”, because all matches in a single CPU cycle would want to write to the same position. CUDA of course provides mechanisms for doing atomic increments, but if these collide you’re ultimately going to be branching and breaking the lock-step on the warps – and more importantly, since we don’t control which thread runs when, we’d actually be randomizing the results, which would require us to sort the data again.&lt;/p&gt;&lt;p&gt;So; instead of trying to write packed data, we’ll instead try to create a sparse vector of just the matches – so zero (or another sentinel) for data that didn’t match, and the key otherwise:&lt;/p&gt;&lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-ysFLL9lbaYo/VzCycvJuzJI/AAAAAAAAD1o/HjpZT1bMgE4/s1600-h/image%25255B15%25255D.png"&gt;&lt;img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-itaxsew5jjI/VzCydMhSbEI/AAAAAAAAD1s/Bltsw0dlQbA/image_thumb%25255B7%25255D.png?imgmax=800" width="561" height="226"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;What we’ll then do is &lt;em&gt;pack that down&lt;/em&gt; to give just the non-zero data:&lt;/p&gt;&lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-R2NSK8oq9eM/VzCyddg79HI/AAAAAAAAD1w/4y9YRpUUCZc/s1600-h/image%25255B19%25255D.png"&gt;&lt;img title="image" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-nHj0Js0C2Sc/VzCydujSTPI/AAAAAAAAD10/_NuLDpdBrBc/image_thumb%25255B9%25255D.png?imgmax=800" width="565" height="210"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Note that we’ve preserved the sort on the data, and now we’re in a position where we can just do a memory copy from the device (GPU) to the host (CPU) of &lt;em&gt;just the page of results we want&lt;/em&gt;.&lt;/p&gt;&lt;h2&gt;Show me some code!&lt;/h2&gt;&lt;p&gt;OK, time for some CUDA. Note that there are are many ways of creating CUDA binaries, and I intend showing much more on this next time – but for now, I’m using C code, since that works directly with NVIDIA’s tooling.&lt;/p&gt;&lt;p&gt;The first thing we need to do is to figure out what element we’re meant to be processing. CUDA makes available to us the block dimensions (&lt;code&gt;blockDim&lt;/code&gt;), the logical position of the current block (&lt;code&gt;blockIdx&lt;/code&gt;), and the logical position of the current thread inside the block (&lt;code&gt;threadIdx&lt;/code&gt;). For a 1-dimensional kernel, this means that our actual index is:&lt;/p&gt;&lt;pre&gt;int i = blockIdx.x * blockDim.x + threadIdx.x;&lt;/pre&gt;&lt;p&gt;We can also assume that the kernel has access to the underlying data (I’ll demonstrate this next time), and somewhere to put the results. For simplicity, let's consider a test on the score of the post; what we want to do is essentially:&lt;/p&gt;&lt;pre&gt;Question* q = allData + i; // pointer arithmetic &lt;br /&gt;results[i] = (q-&amp;gt;score &amp;gt;= minScore &amp;amp;&amp;amp; q-&amp;gt;score &amp;lt;= maxScore) ? i : 0;&lt;/pre&gt;&lt;p&gt;However, the observant might notice that both the short-circuiting "and" (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;) and the ternary-conditional (&lt;code&gt;?&lt;/code&gt; &lt;code&gt;:&lt;/code&gt;) are branching operations, and are best avoided. Fortunately, it isn't hard to rewrite this using non-branching equivalents. The "and" is simple - we'll just make it non-short-curcuiting. The conditional is harder, but "true" in C is expressed by the number 1. This means that if we negate (numerically, not bitwise) the result of our boolean test we get 0 and -1; and -1 in two's-complement binary is "all the ones". This means we can just do a bitwise "and" between this and the number we want to store (&lt;code&gt;i&lt;/code&gt;):&lt;/p&gt;&lt;pre&gt;results[i] = (-(q-&amp;gt;score &amp;gt;= minScore &amp;amp; q-&amp;gt;score &amp;lt;= maxScore)) &amp;amp; i;&lt;/pre&gt;&lt;p&gt;While it might not be as intuitive, this type of thinking is key to keeping warps in lock-step to get the maximum performance from the GPU.&lt;/p&gt;&lt;h2&gt;OK, I see why this might be useful, but how well does it work in practice? Is it worth it?&lt;/h2&gt;&lt;p&gt;Note: as I have tried to emphasize: all performance data is deeply dependent on your exact scenario and your exact implementation. I’m going to talk about what we’ve found, but please don’t use this to justify a big hardware spend: race your own horses.&lt;/p&gt;&lt;p&gt;All of this work would be purely&amp;nbsp; academic if it didn’t help us. So we’ve spent a lot of time and effort comparing performance using our captured data between the CPU versions (v1 and v2) and the GPU version, using a range of devices. For local development purposes &lt;em&gt;today&lt;/em&gt;, a GTX 980 is more than sufficient.&lt;/p&gt;&lt;p&gt;So without further ado: some numbers. Looking &lt;strong&gt;only&lt;/strong&gt; at the “hard” queries (the easy queries aren’t a bottleneck), the 72-core high-end server that I showed in the task-manager screenshot above managed and average of 259.9 requests per second for the CPU version – 3.8ms per request, which is pretty respectable for non-trivial filters against 12 million records. The GPU version, however, managed nearly double that (507.7 requests per second) &lt;em&gt;&lt;strong&gt;just on my development GTX 980&lt;/strong&gt;&lt;/em&gt;. This is not “high end” in GPU terms. There are server devices like the K40, K80 (two K40s back-to-back with a shared power limit), and the M40 which are quite a bit more powerful, and in my testing the maximum throughput went up linearly as you added more GPUs. Given that a C4130 has space for 4 GPUs, that could give you 4 M40 devices or 8 K80 devices (two devices per package), to get a massive speedup&amp;nbsp; compared to CPU&amp;nbsp; limits.&lt;/p&gt;&lt;p&gt;And also note: the &lt;a href="http://www.geforce.com/hardware/10series/geforce-gtx-1080"&gt;GTX 1080&lt;/a&gt; (desktop) and &lt;a href="http://www.nvidia.com/object/tesla-p100.html"&gt;P100&lt;/a&gt; (server) around the corner boasting the next generation of architecture and hopefully another huge jump in performance (I haven’t got my grubby hands on those yet).&lt;/p&gt;&lt;p&gt;But: from what we’ve seen, we’re keen to push to the next level and get some of this kit deployed into production.&lt;/p&gt;&lt;h2&gt;My curiosity on CUDA is piqued; are you&amp;nbsp; done? what next?&lt;/h2&gt;&lt;p&gt;No, you don’t shut me up that easily! Next time, I’m going to be talking about things like grid-stride loops, async and CUDA streams, memory transfers, and showing how we can get access to all this goodness from my preferred every-day language: C#. I’ll also be walking you through some code that you can clone and play with to see it in action, rather than as text.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/u5Obr0tSJYM" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/8769924495263250964" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/8769924495263250964" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/u5Obr0tSJYM/how-i-found-cuda-or-rewriting-tag_9.html" title="How I found CUDA, or: Rewriting the Tag Engine–part 2" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh3.googleusercontent.com/-R057tySJEYk/VzCybjgvOMI/AAAAAAAAD1c/WrW1V2eUQuM/s72-c/image_thumb%25255B1%25255D.png?imgmax=800" height="72" width="72" /><feedburner:origLink>http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag_9.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-7377448965895200997</id><published>2016-05-09T03:54:00.001-07:00</published><updated>2016-05-11T01:15:51.154-07:00</updated><title type="text">How I found CUDA, or: Rewriting the Tag Engine–part 1</title><content type="html">&lt;p&gt;(&lt;a href="http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag_9.html"&gt;part 2&lt;/a&gt;, &lt;a href="http://blog.marcgravell.com/2016/05/cudagetting-started-in-net.html"&gt;part 3&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;This post is largely an introduction to set context so that the following few articles make sense… there won’t be any code here, but: well, take it or leave it :)&lt;/p&gt; &lt;h2&gt;The Context – what is the tag engine?&lt;/h2&gt; &lt;p&gt;At Stack Overflow / Stack Exchange, a lot of our pages relate to “tags” (topics). As an obvious example, a lot of users browse questions in specific technologies looking for new things to answer, or have feeds / email notifications configured for specific tags. Other users might be interested in all the newest questions, but absolutely never want to see another question that reminds them of their last job (by the way, if your job sucks, &lt;a href="http://stackoverflow.com/jobs"&gt;you should fix that&lt;/a&gt; – life is too short to be miserable). We also&amp;nbsp; do things like showing “related tags” – essentially the counts of the &lt;em&gt;intersections&lt;/em&gt; between technologies of what you’re looking at and other questions we know about. &lt;/p&gt; &lt;p&gt;All of this needs a non-trivial amount of processing and memory. Back in the day, it was sufficient to use our RDBMS for that (&lt;a href="http://blog.marcgravell.com/2014_04_01_archive.html"&gt;via some hacks that in turn left us some technical debt&lt;/a&gt;, but that is long gone now), but as we grew that simply wasn’t going to work. So after investigating a few options, out popped the “tag engine” – basically some bespoke code with a small set of jobs that we could run out-of-process to the main web-servers (so they don’t have to reload everything when we deploy / recycle).&lt;/p&gt; &lt;h2&gt;So… life was good?&lt;/h2&gt; &lt;p&gt;All was well. Sure, &lt;a href="http://blog.marcgravell.com/2011/10/assault-by-gc.html"&gt;we had to fight a few things like GC&lt;/a&gt;, but… it worked. But as we grew, that code base started to become more and more of a limiting factor. It is nearly 5 years old now, and we’ve grown a lot in that time, and our needs have changed a lot in that time. The tag engine was never really “designed” so much as … “grew”. We gradually hacked in features we needed, and tweaked bits, and more or less it kept working. It was a bit ugly, but it wasn’t actually a problem. We’re pragmatists: we fix problems. We don’t fix things that aren’t problems.&lt;/p&gt; &lt;p&gt;Lately, it has been moving more and more from the “not a problem” camp to “problem”, so yay, more things to do. Performance was a key part of this, stemming in part from data volume, part from design choices - an overhaul was overdue.&lt;/p&gt; &lt;h2&gt;Starting to think about GPUs&lt;/h2&gt; &lt;p&gt;Around this time, I happened to see an email conversation from Daniel Egloff at &lt;a href="http://www.quantalea.com/"&gt;QuantAlea&lt;/a&gt;, and it made me think about how much of the tag-engine might be suitable for GPU work. After a brief exchange with Daniel, I was satisfied that I wasn’t totally crazy. As a brief aside: QuantAlea were great and seemed keen to help us work on the problem. I think their tools show real promise, but &lt;strong&gt;&lt;em&gt;for us&lt;/em&gt;&lt;/strong&gt; we made the decision to keep everything in-house and do it ourselves. This is in part because our scenario is relatively simple. What I’m saying here is: they were really helpful, and if you’re interested in CUDA you might want to think about them, but: we didn’t go that way ourselves.&lt;/p&gt; &lt;h2&gt;So what the hell is the tag engine doing? Why do you even need that level of crazy?&lt;/h2&gt; &lt;p&gt;The interesting thing (to me) about the tag engine is that there are two very different scenarios. About half the queries tend to be &lt;em&gt;embarrassingly trivial&lt;/em&gt;; and the other half are absurd. If you come at the tag-engine thinking “list the first 50 newest C# questions” – then: that’s the embarrassingly trivial side. This really doesn’t need a lot of work – just keep track of questions by tag, pre-sorted, and pick out the pages you need. Very fast, very simple. It just works. Although I’ll leave it as an exercise for the reader to think about how to generate the tag intersection cloud.&lt;/p&gt; &lt;p&gt;The real problem is the other half, which could be “page 200 (50 per page) of all 'java or .net or sql' questions, and everything that starts with ‘visual’, but never show me anything with php or fortran, and only show me questions with a score above 2 that were created after some date”. Lots of complex unions, restrictions, etc. You &lt;strong&gt;can&lt;/strong&gt; do this type of thing with general purpose indexing tools (Elasticsearch for example), but a: we want the maximum performance, and b: some of our data is not really amenable to such tools – for example, some of our sort-orders are highly time dependent, which is complex and awkward for such tools (and may require lots of re-sends). Equally, some of the data we want to get back out (including, but not limited to, the tag intersection cloud) are not easy to do.&lt;/p&gt; &lt;p&gt;It is this second category of queries where the tag-engine shines; essentially, what it has to do is to keep some pre-built indexes and then try to figure out how best to exploit those indexes to answer a complex query – or, worst case, to do the moral equivalent of a table scan, and &lt;em&gt;just walk the data&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;OK, there’s a tricky technical problem; how could CUDA help? What the hell even is CUDA?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/CUDA"&gt;CUDA&lt;/a&gt; is an offshoot from the gaming world. In their efforts to make games that have high frame rates at high visual quality, the graphics card (GPU) vendors came up with a different approach to processing. Rather than having a relatively small number of very fast general purpose CPUs that switch constantly between doing 200 things, they went with a higher number of “symmetric multiprocessors” – not individually as fast as regular CPUs, but able to do the same thing many many times in parallel (think: &lt;a href="https://en.wikipedia.org/wiki/SIMD"&gt;SIMD&lt;/a&gt; gone mad), and with access to a large number of math processors (ALUs). NVIDIA correctly reasoned that this type of technology might be awesome for many computing tasks, so CUDA was developed as a framework to enable general purpose computing on the GPU, aka GPGPU. Of course, the bus between the device and main memory isn’t as fast as direct CPU&amp;lt;-&amp;gt;RAM access (although they’re working hard on that problem), but: very powerful.&lt;/p&gt; &lt;p&gt;And as a result of this, GPU programming is very, very good at scenarios where you want to do a relatively simple and predictable operation many times, ideally where the data you need can be pushed onto the device up-front (and left there, updated only periodically), and where you only need to get back relatively small quantities of results. Doesn't this sound a lot like the second – harder - scenario we just described for the tag engine?&lt;/p&gt; &lt;h2&gt;So…&lt;/h2&gt; &lt;p&gt;That’s why we were intrigued with GPU programming on the tag-engine. The next post will be more technical, discussing the relative merits of CPU and GPU programming and how we might need to use different approaches to optimize each. And hopefully some code, yay! As a teaser though: it works great.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/z5CbfQ2U0LE" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7377448965895200997" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/7377448965895200997" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/z5CbfQ2U0LE/how-i-found-cuda-or-rewriting-tag.html" title="How I found CUDA, or: Rewriting the Tag Engine–part 1" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><feedburner:origLink>http://blog.marcgravell.com/2016/05/how-i-found-cuda-or-rewriting-tag.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-4077305399878046495</id><published>2016-05-01T08:28:00.001-07:00</published><updated>2016-05-01T08:41:27.131-07:00</updated><title type="text">Using the Windows Subsystem for Linux to simplify CUDA builds</title><content type="html">&lt;p&gt;Recently, Microsoft &lt;a href="https://blogs.windows.com/buildingapps/2016/03/30/run-bash-on-ubuntu-on-windows/"&gt;announced the Windows Sybsystem for Linux&lt;/a&gt;, aka Bash on Ubuntu on Windows. This is currently pre-release, and allows you to run linux tools &lt;em&gt;inside Windows&lt;/em&gt;. In particular, this opens up a whole range of development tools and options. First. a caveat: this stuff is early release and only currently available on the “Fast Ring” of insider builds. If it works: great. If it doesn’t work: oh well. If it melts your PC, stops it booting, or causes skynet to launch the missiles: that’s tough too.&lt;/p&gt; &lt;p&gt;Now, it just so happens that I’ve been playing a lot with CUDA lately – it turns out that it works quite nicely for our “tag engine” back-end service (and I shall no-doubt be blogging about my journey here very soon). If you’ve done any CUDA, you will know that there are two compilation options for compiling your CUDA kernel source-code into something that can run on your GPU – runtime (NVRTC) and build-time (NVCC). The runtime compiler is very convenient for fast iterations – edit a text file, reload your app – but: it doesn’t currently support one very important feature: dynamic parallelization. I’m hoping that CUDA 8.0 Toolkit (due some time around August) might fix this, but until then, it means that I need to use NVCC for my kernels, since I happen to need this feature.&lt;/p&gt; &lt;p&gt;The next thing you’ll know if you’ve done any CUDA work is that NVCC on Windows is &lt;em&gt;fussy&lt;/em&gt;. Not just a little fussy, but &lt;em&gt;really&lt;/em&gt; fussy. Not only does it require specific versions of Visual Studio (that are several major versions behind what I usually have installed), but it also requires your command-line C++ environment to be configured perfectly, and even then (as I found out the hard way), it might still turn around and laugh in your face. Possibly for reasons, possibly just for the giggles. I eventually gave up on making NVCC work in Windows, so I configured a Ubuntu VM in Hyper-V, added a SMB share, and used the VM to build my kernels. It worked, but it was unsatisfactory. So I thought: can I use the new Windows goodies to fix this? And would you believe it: yes… ish.&lt;/p&gt; &lt;p&gt;Scope: note that for the purposes of this article I’m not going to try to explain what CUDA kernels are, or how to use NVCC. If you don’t happen to know these things, it will suffice to know that NVCC is a tool you need, it is really awkward to get it working on Windows, and quite a bit easier to get it working on linux.&lt;/p&gt; &lt;p&gt;It turns out that actually this was pretty easy; first we need to get Bash working:&lt;/p&gt; &lt;p&gt;1. &lt;a href="http://www.cnet.com/uk/how-to/change-to-the-fast-ring-for-more-frequent-windows-10-preview-updates/"&gt;enable the fast ring updates&lt;/a&gt;&lt;br&gt;2. be patient for a day or so until it deigns to offer to let you install the new Windows build&lt;br&gt;3. install the new Windows build and once again learn the true meaning of patience&lt;br&gt;4. enable developer mode in Windows settings:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-cnZXX8VNUos/VyYgoHhHgEI/AAAAAAAADzw/R_VItMC3X7Y/s1600-h/image%25255B2%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-btV-3xiu-dQ/VyYgooUeyrI/AAAAAAAADz0/cYQ2VzpMCEk/image_thumb.png?imgmax=800" width="244" height="227"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;5. turn on the new system in Windows features:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-wPPw8p9aco8/VyYgoxJgdFI/AAAAAAAADz4/Nf1aq2QqdJM/s1600-h/image%25255B8%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-1NPMEwRtimA/VyYgpayPsVI/AAAAAAAADz8/w5u9NNTHZuA/image_thumb%25255B2%25255D.png?imgmax=800" width="244" height="214"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;6. find and run the new tool&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-bWddpcjUIvk/VyYgpmMKvZI/AAAAAAAAD0A/u3tlKoYdjeY/s1600-h/image%25255B5%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-zffnhStuHMY/VyYgqKyeWKI/AAAAAAAAD0E/K0-WhLtH0Pc/image_thumb%25255B1%25255D.png?imgmax=800" width="244" height="126"&gt;&lt;/a&gt;&lt;/p&gt;   &lt;p&gt;The first time you run this, it downloads some pieces and prompts for your new linux credentials. When it has finished installing itself, you get a Ubuntu 14.04 terminal:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-X16vALY1wsg/VyYgqYaamvI/AAAAAAAAD0I/32VpcIjhMoQ/s1600-h/image%25255B11%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-iZVtTkw7OZM/VyYgqkQd7uI/AAAAAAAAD0M/GvFbOhnNl3w/image_thumb%25255B3%25255D.png?imgmax=800" width="244" height="131"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Conveniently, it also mounts your Windows drives for you – so “C:” is “/mnt/c” etc.&lt;/p&gt; &lt;p&gt;Important note: ctrl-v to paste doesn’t currently work, but don’t worry: you don’t need to re-type lines – you can successfully paste via the window menu:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-BCNIqCy0dNU/VyYgrCVx_1I/AAAAAAAAD0Q/5XVC1jEDpmg/s1600-h/image%25255B23%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-po7r7F_c7dY/VyYgrTJClTI/AAAAAAAAD0U/LIbJjDuUt3s/image_thumb%25255B7%25255D.png?imgmax=800" width="244" height="173"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Right. So we have a vanilla (if minimal) Ubuntu installation. From my Hyper-V install, I know that NVCC on linux requires gcc 4.9, so &lt;a href="https://askubuntu.com/questions/466651/how-do-i-use-the-latest-gcc-on-ubuntu-14-04/581497#581497"&gt;we can follow a few lines from Ask Ubuntu to install this&lt;/a&gt;. We also need to install NVCC itself; I recommend not using the version from apt-get, as that is very old – &lt;a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/"&gt;I installed 7.5 using the notes on nvidia.com&lt;/a&gt;, which really &lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;just meant choosing my preferred install from here&lt;/a&gt; - I went for the network-enabled deb install:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-jMFj6uF2lLE/VyYgr-iHUjI/AAAAAAAAD0Y/xnlwuCiFG_I/s1600-h/image%25255B17%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; margin: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-SmNmb-LTd1Q/VyYgsZD5iKI/AAAAAAAAD0c/k9tL-zrKYsE/image_thumb%25255B5%25255D.png?imgmax=800" width="230" height="244"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And remember: once you have downloaded the file, you can access it under /mnt/c/ or similar. The website tells you the full commands to run, so it isn’t a challenge even if you don’t know much linux. The installer takes quite a while to download and complete, so again: patience. It is also worth noting that when installing this way, no symlink or similar is added to the path – so you can either do that yourself, or just use the tool in the installation location of “/usr/local/cuda-7.5/bin/nvcc”:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-tw-f551Jd0k/VyYgsj-ye3I/AAAAAAAAD0g/YSZnjbRe1wQ/s1600-h/image%25255B28%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-zr9A7WKXqFU/VyYgs9OcfZI/AAAAAAAAD0k/rDqspjgHfxs/image_thumb%25255B10%25255D.png?imgmax=800" width="302" height="124"&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt;   &lt;p&gt;After that, compiling to cubin, fatbin, or ptx should &lt;em&gt;just work&lt;/em&gt;. The actual commands aren’t very interesting &lt;a href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/"&gt;and are described in the documentation&lt;/a&gt;. I just wrap my NVCC commands in a bash file &lt;strong&gt;(remembering to ensure newlines are LF only)&lt;/strong&gt;, and run that. Here we can see no “fatbin” before the script, executing the script, and the “fatbin” existing after the script:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lh3.googleusercontent.com/-gQKgmkqNJkw/VyYgteuZJZI/AAAAAAAAD0o/x1fbQDQFwmI/s1600-h/image%25255B27%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="https://lh3.googleusercontent.com/-T8DRNjPIiXI/VyYgt7AfLcI/AAAAAAAAD0s/VpkYe2cIvhU/image_thumb%25255B9%25255D.png?imgmax=800" width="361" height="104"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So what have we done?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;we have installed the linux subsystem&lt;/li&gt; &lt;li&gt;we have installed some linux tools and utilities&lt;/li&gt; &lt;li&gt;we have used those tools as part of a development build process, accessing our windows file system&lt;/li&gt; &lt;li&gt;all without needing a separate VM&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;The fact that it involved NVCC and CUDA is largely incidental; the same tecnhiques could be used on many tools.&lt;/p&gt; &lt;h2&gt;Minor confessions&lt;/h2&gt; &lt;p&gt;1. Life doesn’t often run smoothly. I’m not much of a linux person, and it took me very little time to completely trash my linux install. Fortunately &lt;a href="http://superuser.com/questions/1064801/i-broke-the-windows-linux-subsystem-bash-on-ubuntu-on-windows-10-linux-help/1065628#1065628"&gt;the nuclear reset option exists&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;2. The fast ring installs are … temperamental. As it happens, it installed &lt;strong&gt;just fine&lt;/strong&gt; on my travel laptop (hence @TRAVEL in the screens), but it silently failed to install on my main development desktop, and has now disappeared from the update UI … so I can’t find any way to get it installed. Unfortunately, my laptop doesn’t have a CUDA-enabled GPU, so &lt;strong&gt;&lt;em&gt;actually&lt;/em&gt;&lt;/strong&gt; this hasn’t saved me anything – I still need to use my Hyper-V VM for real work. But: it was still fun to get it working!&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/J_j7uqk_aJ4" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/4077305399878046495" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/4077305399878046495" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/J_j7uqk_aJ4/using-windows-subsystem-for-linux-to.html" title="Using the Windows Subsystem for Linux to simplify CUDA builds" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lh3.googleusercontent.com/-btV-3xiu-dQ/VyYgooUeyrI/AAAAAAAADz0/cYQ2VzpMCEk/s72-c/image_thumb.png?imgmax=800" height="72" width="72" /><feedburner:origLink>http://blog.marcgravell.com/2016/05/using-windows-subsystem-for-linux-to.html</feedburner:origLink></entry><entry><id>tag:blogger.com,1999:blog-8184237816669520763.post-2610238215232195026</id><published>2015-11-24T08:16:00.001-08:00</published><updated>2015-11-25T00:06:37.010-08:00</updated><title type="text">The road to DNX–part 3</title><content type="html">&lt;p&gt;&lt;a href="http://blog.marcgravell.com/2015/11/the-road-to-dnx-part-1.html"&gt;In part 1&lt;/a&gt;, we&amp;nbsp; looked at an existing library that we wanted to move to core-clr; we covered the basics of the tools, and made the required changes &lt;em&gt;just &lt;/em&gt;to change to the project.json build approach, targeting the same frameworks.&lt;/p&gt; &lt;p&gt;&lt;a href="http://blog.marcgravell.com/2015/11/the-road-to-dnx-part-2.html"&gt;In part 2&lt;/a&gt;, we looks at “dnxcore50”, and how to port a library to support this new framework alongside existing .net frameworks. We looked at how to setup and debug tests. We then introduced&amp;nbsp; “dnx451”: the .net framework running inside DNX.&lt;/p&gt; &lt;p&gt;In part 3, we dive deeper still…&lt;/p&gt; &lt;h1&gt;Targeting Hell&lt;/h1&gt; &lt;p&gt;You could well be thinking that all these frameworks (dnx451, dnxcore40, net35, net40, etc) could start to become&amp;nbsp; tedious. And you’d be right! FastMember only targets a few, but as a library author, you may know the …. &lt;em&gt;joy&lt;/em&gt; … of targeting a much wider set of .net frameworks. Here’s the build tree for protobuf-net r668 (pre core-clr&amp;nbsp; conversion):&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-IEirh-XcBkE/VlSNGgNbFQI/AAAAAAAADq4/yTyTROdy8A0/s1600-h/image_thumb1%25255B2%25255D.png"&gt;&lt;img title="image_thumb1" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image_thumb1" src="http://lh3.googleusercontent.com/-B0crCAZjov4/VlSNHWcMOcI/AAAAAAAADrA/qdeIBOLIN6M/image_thumb1_thumb.png?imgmax=800" width="396" height="337"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These&amp;nbsp; are incredibly hard&amp;nbsp; to build&amp;nbsp; currently (often requiring per-platform tools). It is a mess. Adding more frameworks isn’t going to make our life any easier. However, many of these frameworks have huge intersections. Rather than having to explicitly target 20 similar frameworks, how about if we could just target an entire &lt;em&gt;flavor&lt;/em&gt; of similar APIs? That is what the &lt;a href="https://github.com/dotnet/corefx/blob/master/Documentation/project-docs/standard-platform.md"&gt;.NET Platform Standard&lt;/a&gt; (aka: netstandard) introduces. This is mainly targeting a lot of the newer frameworks, but then… it is probably about time I dropped support for Silverlight 3. With these new tools, we&amp;nbsp; can increase our target audience very quickly, without overly increasing our development burden.&lt;/p&gt; &lt;p&gt;Important: the names here are moving. At the current time (rc1), the &lt;em&gt;documentation&lt;/em&gt; talks about “netstandard1.4” etc; however, the &lt;em&gt;tools &lt;/em&gt;recognise “dotnet5.5” to mean the same thing. (&lt;b&gt;edited&lt;/b&gt;: I originally said dotnet5.4===netstandard1.4; I had an off-by-one error - the versions are 4.1 off from each-other!) Basically, the community made it clear that “dotnet” was too confusing for this purpose, so the architects wisely changed the nomenclature. So&amp;nbsp; “netstandard1.1” === “dotnet5.2” – savvy?&lt;/p&gt; &lt;p&gt;Great! So how do we do this? It is much easier than it sounds; we just change our project.json from “dnxcore50” to “dotnet5.4”:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-dpTVL71iH_w/VlSNHy4-X-I/AAAAAAAADrE/PRosv7XZ29E/s1600-h/image%25255B6%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-6pIWnGBYtKg/VlSNIYXcrKI/AAAAAAAADrM/uPLp8zMKISM/image_thumb%25255B2%25255D.png?imgmax=800" width="366" height="146"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;netstandard1.4 (dotnet5.5) is the richest variant – the intersection of DNX Core 5.0 and .NET Framework 4.6.*. We're going to target dotnet5.4. If you go backwards (1.3, 1.2, 1.1, etc) you can target&amp;nbsp; a wider audience, but using a narrower intersection of available APIs. netstandard1.1, for example, includes Windows Phone 8.0. There are various tables&amp;nbsp; on the .NET Platform Standard documentation that tell you what each name targets. Notice I added a “COREFX” define. That&amp;nbsp; is because the compiler is now including “DOTNET5_4” as a build symbol, not the more specific “DNXCORE50”. To avoid confusion (especially since I know it will change in the next tools drop), I’v&lt;a href="https://github.com/mgravell/fast-member/commit/44326df4ef60171832fe671ea8b29a22b49f4048"&gt;e changed my existing “#if DNXCORE50” to “#if COREFX”, for my convenience&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-QA5AF56Zv7g/VlSNIi02e_I/AAAAAAAADrU/o6ApT3DCSXI/s1600-h/image%25255B10%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-LLxLCcmtSfc/VlSNJBVeJKI/AAAAAAAADrc/S_EeHIv33ho/image_thumb%25255B4%25255D.png?imgmax=800" width="395" height="136"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We don’t have to stop with netstandard1.3, though; what I suggest library authors do is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;get it working on what they &lt;em&gt;actively want &lt;/em&gt;to support&lt;/li&gt; &lt;li&gt;then try wider (lower number) versions to see what they &lt;em&gt;can&lt;/em&gt; support&lt;/li&gt;&lt;/ol&gt; &lt;p&gt;For example, changing to 1.2 (dotnet5.3) only gives me 13 errors, many of them duplicates and trivial to fix:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-lqSucPaScDw/VlSNJp-D8OI/AAAAAAAADrk/qbLuTTnoxLM/s1600-h/image%25255B15%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-NiFDNqaJK0M/VlSNKYiB3HI/AAAAAAAADrw/hvVSGqHsmtY/image_thumb%25255B7%25255D.png?imgmax=800" width="496" height="276"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And interestingly, this is the &lt;strong&gt;same 13 errors&lt;/strong&gt; that I get for 1.1 (dotnet5.2). If&amp;nbsp; I try&amp;nbsp; targeting dotnet5.1, I lose a lot of&amp;nbsp; things that I absolutely depend on (TypeBuilder, etc), so perhaps draw the line at dotnet5.2; that is still a lot more potential users than 1.4. &lt;a href="https://github.com/mgravell/fast-member/commit/829c12ed6084172b7b3d77db068252727800d4a2"&gt;With some minimal changes&lt;/a&gt;, we can support dotnet5.2 (netstandard1.1); the surprising bits there are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the need to add a System.Threading dependency to get Monitor support (aka: the “lock” keyword)&lt;/li&gt; &lt;li&gt;the need to explicitly specify the System.Runtime version in the test project&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;We can&amp;nbsp; test this with “dnx&amp;nbsp; test” / “dnx perf”, in both core-clr and .net&amp;nbsp; under dnx, and it works fine. We don’t need the dnx451 specific build any more&lt;/p&gt; &lt;p&gt;Observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have seen issues with dotnet5.4 projects trying to consume libraries that expose dotnet5.2 builds; this might just be because of the in-progress tooling&lt;/li&gt; &lt;li&gt;At the moment, xunit targets dnxcore50, not dotnet*/netstandard* – so you’ll need to keep your test projects targeting dnxcore50 and dnx451 for&amp;nbsp; now; however, your &lt;strong&gt;library &lt;/strong&gt;code should be able to just target the .NET Platform Standard without dnx451 or dnxcore50:&lt;/li&gt;&lt;/ul&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-0_4P4au9QVY/VlSNKhHN37I/AAAAAAAADr0/PFm2W0nnDWo/s1600-h/image%25255B49%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-cyAY0RzvtFY/VlSNLQVJimI/AAAAAAAADr8/mzlGCHmtslg/image_thumb%25255B25%25255D.png?imgmax=800" width="633" height="143"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That’s pretty much the key bits of netstandard; it lets you target a wider audience without having a myriad of individual frameworks defined. But you can use this &lt;strong&gt;in combination with &lt;/strong&gt;more specific targets if you want to use specific features of a particular framework, when available.&lt;/p&gt; &lt;h1&gt;Packaging&lt;/h1&gt; &lt;p&gt;As this is aimed at library authors, I’m assuming&amp;nbsp; you have previously deployed to nuget, so you should be familiar with the hoops you need to jump through, and the maintenance overhead. When you think about it, our project.json already defines quite a few of the key things nuget&amp;nbsp; needs (dependencies, etc). The dnx tools, then, introduce a new way to package our libraries. What we need to do first is fill in &lt;a href="https://github.com/mgravell/fast-member/commit/eb26efbb51af2d165e697cec149e9f71f0ff153b"&gt;some extra fields&lt;/a&gt; (copying from an existing nuspec, typically):&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-GjVoNMltf5k/VlSNLi3l78I/AAAAAAAADsE/TAlxWjEn6Rw/s1600-h/image%25255B20%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-gV3MIU-tshs/VlSNMR98iGI/AAAAAAAADsM/sGv9QUpOMZ8/image_thumb%25255B10%25255D.png?imgmax=800" width="379" height="236"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now all we need to do is “dnu pack --configuration release”:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-MCdmenX1X2w/VlSNMqys4cI/AAAAAAAADsU/xX10xFrouh4/s1600-h/image%25255B28%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-CiGU20tFAvk/VlSNNZaQt3I/AAAAAAAADsg/jN7rPnjdVPA/image_thumb%25255B14%25255D.png?imgmax=800" width="540" height="178"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and … we’ve just built our nupkg (or two). Aside: does anyone else think “dnu pack” should default to release builds? Or is that just me? We can go in and see what it has created:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-6iano2dUH3k/VlSNN6YjwXI/AAAAAAAADsk/YHO2Q08FbAA/s1600-h/image%25255B32%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-ts5G5nLhIj8/VlSNOfHpnlI/AAAAAAAADss/Z4BZ-rQpwBc/image_thumb%25255B16%25255D.png?imgmax=800" width="410" height="171"&gt;&lt;/a&gt;&lt;/p&gt;   &lt;p&gt;The nupkg is the packed contents, but we can also see what it was targeting. Looking at the above, it occurs that I should probably go back and remove the explicit dnx451 build, deferring to dotnet5.2, but… meh. It’ll all change again in rc2 ;p&lt;/p&gt; &lt;p&gt;I wish there was a “dnu push” for uploading to nuget, but for now I’ll just use the manual upload:&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-gw-R3VZLuDs/VlSNOxk3PcI/AAAAAAAADs4/EHE48041uAs/s1600-h/image%25255B36%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-pZdcpVtZZvM/VlSNPbcUq-I/AAAAAAAADs8/xMFp3-eIx5Q/image_thumb%25255B18%25255D.png?imgmax=800" width="225" height="402"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The details are as expected, so: &lt;a href="https://www.nuget.org/packages/FastMember/1.1.0-beta1"&gt;library uploaded&lt;/a&gt;! (and repeated for the &lt;a href="https://www.nuget.org/packages/FastMember.Signed/1.1.0-beta1"&gt;strong-name version&lt;/a&gt;; don’t get me started on the “strong-name or don’t strong-name” debate; it makes me lose the will to live).&lt;/p&gt; &lt;p&gt;We have now built, tested, packaged and deployed our multi-targeting library that uses the .NET Platform Standard 1.1 and above, plus .Net 3.5 and .Net 4.0. Hoorah for us!&lt;/p&gt; &lt;p&gt;I should also note that Visual Studio also offers the ability to create packages; this is hidden in the project properties (this is manipulating the xproj file):&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-ORd5whZMwyM/VlSNP0Ub4pI/AAAAAAAADtE/xyOMSqx6uxs/s1600-h/image%25255B40%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-sfOq-iCDxno/VlSNQfDleRI/AAAAAAAADtM/ajUGNR3G1Ho/image_thumb%25255B20%25255D.png?imgmax=800" width="431" height="107"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and if you build this way, the outputs go into “artifacts” under the solution (not the project):&lt;/p&gt; &lt;p&gt;&lt;a href="http://lh3.googleusercontent.com/-1VFho-pgCoE/VlSNRCDfahI/AAAAAAAADtU/v-VGemq6s1E/s1600-h/image%25255B45%25255D.png"&gt;&lt;img title="image" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="image" src="http://lh3.googleusercontent.com/-n9uHJHsU_r4/VlSNR1j1uuI/AAAAAAAADtc/aVXdOcYZzEI/image_thumb%25255B23%25255D.png?imgmax=800" width="479" height="204"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Either way: we have our nupkg, ready to distribute.&lt;/p&gt; &lt;h1&gt;Common Problems&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;The feature&amp;nbsp; I want isn’t available in core-clr&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;First, search &lt;a href="https://github.com/dotnet/corefx"&gt;dotnet/corefx&lt;/a&gt;; it is, of course, entirely possible that it &lt;strong&gt;isn’t &lt;/strong&gt;supported, especially if you are doing WPF over WCF, or something obscure like … DataTable ;p Hopefully you find it tucked away in a different package; lots of things move.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The feature is there on github, but not on nuget&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;One thing you could do here is to try using the experimental feed. You can control&amp;nbsp; your package feeds using NuGet.config in your solution folder, &lt;a href="https://github.com/StackExchange/dapper-dot-net/blob/c15ca4ca9b152812e8d17f554da536b2511c932a/NuGet.Config"&gt;like in this example&lt;/a&gt;, which disregards whatever package feeds are defined globally, and uses the experimental feed and official nuget feed. You may need to explicitly specify a full release number (including the beta marker) if it is pre-release. If that still doesn’t work, you could perhaps enquire with the corefx team on why/when.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The third party package I want doesn’t support core-clr&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;If it is open source, you could always throw them a pull-request with the changes. That depends on a lot of factors, obviously. Best practice would be to communicate with the project owner first, and check for branches. If it is available in source but not on NuGet, you could build it locally, and (using the same trick as above) add a local package source – all you need to do is drop the nupkg in a folder on the file-system, and add the folder to the NuGet.config file. When the actual package gets released, remember to nuke any temporary packages from&amp;nbsp; %USERPROFILE%/.dnx/packages.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I don’t like having the csproj as well as the project.json&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Long term, we can probably nuke those csproj; they are handy to keep for now, though, to make it easy for people to build the solution (minus core-clr support).&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The feature I want isn’t available in my target framework / Platform Standard&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Sometimes, you’ll be able to work around it. Sometimes you’ll have to restrict what you can support to more forgiving configurations. However, sometimes there are cheeky workarounds. For example, RegexOptions.Compiled is not available on a lot of Platform Standard configurations, but it is there really. You can cheat by checking if the enum is defined &lt;em&gt;at runtime&lt;/em&gt;, and use it when available; &lt;a href="https://github.com/jeremymeng/StackExchange.Redis/blob/netcore/StackExchange.Redis/StackExchange/Redis/InternalRegexCompiledOption.cs"&gt;here’s a nice example of that&lt;/a&gt;. There are uglier things you can do, too, such as using reflection to see if types and methods are &lt;em&gt;actually&lt;/em&gt; available, even if they aren’t there in the declared API – you should try to minimize these things. As an example, &lt;a href="https://github.com/mgravell/protobuf-net"&gt;protobuf-net&lt;/a&gt; would really like to use &lt;a href="https://msdn.microsoft.com/en-us/library/system.runtime.serialization.formatterservices.getuninitializedobject"&gt;FormatterServices.GetUninitializedObject()&lt;/a&gt; when it is available. Just… be careful. This trick work on things like universal applications, but then: neither will &lt;em&gt;hardly any &lt;/em&gt;of what protobuf-net does, so that is a moot point.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I’m having a problem with the tooling&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;The various teams are very open to feedback. I confess that I sometimes struggle to know what should go to the corefx team vs the asp.net team (some of the boundaries are largely arbitrary and historical), but it’ll probably find a receptive ear.&lt;/p&gt;   &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;The core-clr project moves a lot of pieces, and a lot of things are still in flux. But: it is now stable enough that many library authors should be more than capable of porting their projects, and quite&amp;nbsp; possibly simplifying their build process at the same time.&lt;/p&gt; &lt;p&gt;Happy coding.&lt;/p&gt;  &lt;img src="http://feeds.feedburner.com/~r/CodeCodeAndMoreCode/~4/g8aHEt_Z6l4" height="1" width="1" alt=""/&gt;</content><link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2610238215232195026" /><link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8184237816669520763/posts/default/2610238215232195026" /><link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/CodeCodeAndMoreCode/~3/g8aHEt_Z6l4/the-road-to-dnxpart-3.html" title="The road to DNX–part 3" /><author><name>Marc Gravell</name><uri>http://www.blogger.com/profile/01023334706549710089</uri><email>noreply@blogger.com</email><gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="32" height="32" src="http://4.bp.blogspot.com/_feJtUp7IGuI/TA-NNRO9KsI/AAAAAAAAAU4/4a0UAeDCCtI/s1600-R/b4ae7f8b50a2c32287e456394570679e%3Fs%3D128%26d%3Didenticon%26r%3DPG" /></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://lh3.googleusercontent.com/-B0crCAZjov4/VlSNHWcMOcI/AAAAAAAADrA/qdeIBOLIN6M/s72-c/image_thumb1_thumb.png?imgmax=800" height="72" width="72" /><feedburner:origLink>http://blog.marcgravell.com/2015/11/the-road-to-dnxpart-3.html</feedburner:origLink></entry></feed>
