<?xml version="1.0" encoding="UTF-8"?><feed
	xmlns="http://www.w3.org/2005/Atom"
	xmlns:thr="http://purl.org/syndication/thread/1.0"
	xml:lang="en-US"
	xml:base="https://www.percona.com/blog/wp-atom.php"
	
xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
>
	<title type="text">Percona Database Performance Blog</title>
	<subtitle type="text"></subtitle>

	<updated>2019-08-01T19:16:58Z</updated>

	<link rel="alternate" type="text/html" href="https://www.percona.com/blog" />
	<id>https://www.percona.com/blog/feed/atom/</id>
	<link rel="self" type="application/atom+xml" href="https://www.percona.com/blog/feed/atom/" />

	<generator uri="https://wordpress.org/" version="5.2.2">WordPress</generator>
<icon>https://www.percona.com/blog/wp-content/uploads/2018/09/percona-32x32.png</icon>
	<entry>
		<author>
			<name>Matt Yonkovit</name>
						<uri>http://www.percona.com/</uri>
						</author>
		<title type="html"><![CDATA[Your Personal Data Just Leaked and It’s (Partly) Your Fault]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/08/01/your-personal-data-just-leaked-and-its-your-fault/" />
		<id>https://www.percona.com/blog/?p=59678</id>
		<updated>2019-08-01T18:27:46Z</updated>
		<published>2019-08-01T18:27:46Z</published>
		<category scheme="https://www.percona.com/blog" term="open source databases" />		<summary type="html"><![CDATA[<img width="200" height="141" src="https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-200x141.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="protect your database" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-200x141.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-300x211.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-1024x720.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-367x258.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Capital One dominated the headlines this week when they disclosed that they had become the latest hacking victim with a shared responsibility between them and their customers. Does this mean Amazon bears no responsibility in a breach like this? Not entirely (more on that later). Responsibility of the User One major issue is that the [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/08/01/your-personal-data-just-leaked-and-its-your-fault/"><![CDATA[<img width="200" height="141" src="https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-200x141.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="protect your database" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-200x141.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-300x211.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-1024x720.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database-367x258.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/08/protect-your-database.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p>Capital One dominated the headlines this week when they disclosed that they had become the latest hacking victim with a <a target="_blank" href="https://krebsonsecurity.com/2019/07/capital-one-data-theft-impacts-106m-people/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">massive data breach</a> this past March. Over 100 million people had social security numbers, bank account information, and other personal data exposed when a hacker got access to Capital One’s critical infrastructure running in AWS via a misconfigured firewall.</p>
<p><img class="aligncenter wp-image-59684 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Capitalone.png" alt="Capital one data breach" width="990" height="374" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Capitalone.png 990w, https://www.percona.com/blog/wp-content/uploads/2019/07/Capitalone-200x76.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Capitalone-300x113.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Capitalone-367x139.png 367w" sizes="(max-width: 990px) 100vw, 990px" /></p>
<p style="text-align: center;"><span style="font-size: 14px;"><em>Email tip alerting Capital One to the data breach.</em></span></p>
<p>Since the story broke, many have rushed to ask how Amazon could allow this to happen. However, what most don’t realize, is that Amazon does not promise to secure your full system, nor have they ever claimed to. Amazon is very specific about security being a <a target="_blank" href="https://aws.amazon.com/compliance/shared-responsibility-model/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">shared responsibility</a> between them and their customers. Does this mean Amazon bears no responsibility in a breach like this? Not entirely (more on that later).</p>
<h2>Responsibility of the User</h2>
<p>One major issue is that the speed at which technology has become accessible has outpaced the general ability of consumers to wield it properly. Over the last 6 months, there have been <em>hundreds</em> of data breaches (that we know of), and the most common reason for these ultimately turns out to be human error. It can be people who are underqualified using complex and powerful tools they don’t fully understand or simply haven’t enough experience implementing. Or, they are being encouraged to push the technology in ways it is not engineered to support. As you have often heard&#8230;</p>
<blockquote><p>&#8220;With great power comes great responsibility.“</p></blockquote>
<p><strong>But it’s not just underqualified users who misuse technology.</strong> Easy access to technology can give users a false sense of security, with the misperception that just because it is backed by a big name, then it must be tested and trusted and therefore fail-safe. However, every user must know their own, and the technologies&#8217;, limitations. They must be encouraged to ask for help. They must also realize that easy technology often comes at a price.</p>
<p>This happens over and over—easy access to complex technology has encouraged bad behavior, which when viewed from outside the bubble would be quite shocking. For example, would you ever print a list of your customers and leave it in a coffee shop? For most, this would be a clear violation of data safety. But, many people don’t hesitate to gather customer data spun up from a database or push data to cloud storage to view it somewhere else for deeper analysis. The justification for this action is always the same, “I am doing what I need to do to get the job done.” Employees crunch the numbers, analyze the data, and even get praise from their boss for a quick and timely analysis. But this data is now outside corporate oversite, and at risk.</p>
<h2>“Fully Managed” by Cloud Providers</h2>
<p>Cloud providers often use the term “Fully Managed” to market and position their solutions, but this can be harmful. People have different ideas on what being fully managed means, and it gives the customer a false sense that technology they don’t understand or systems they have no experience with will be managed in full by the provider.</p>
<p>This misrepresentation and misunderstanding leads the customer to overlook important details they would (or should) have taken note of if they were solely responsible for their systems. This also leads to a lack of client trust and lower customer satisfaction when expectations are not met.</p>
<p>Think about this—cloud services are designed to be mass-produced. Vendors are designing and building tools and infrastructure that, out of the box, have to meet the broadest baseline of demands. Therefore, when you are adding something designed for the cloud it is built for out-of-the-box simplicity and access for the masses. However, each application and workload is unique, so you will need to take something that is mass-produced and customize it to ensure your applications and data are secure, sage, and can scale.</p>
<p><strong>How can cloud providers like Amazon help mitigate this issue?</strong> There are several ways. They can give users tools to validate and confirm they have not unintentionally left something open. They can educate customers that their version of fully-managed does not equate to 100% hands-off management or no customer responsibility. Finally, they can review how tools are being used and adjust the defaults to make things more secure in response to issues that occur.</p>
<h2>You Got Yourself Into This… Get Yourself Out</h2>
<p>There is immense pressure on companies to cut costs, speed up delivery, and do more with less. This pressure has encouraged people to find different—sometimes better, sometimes worse—ways of solving complex problems with technology. Automation and X as a Service (XaaS) providers offer a convenient and easy answer to quickly solve issues. Unfortunately, many people set up an environment and then move onto the next thing, forgetting that applications are living, breathing entities that evolve and change over time and thus require maintenance. Reliance on 3rd-party services can become a double-edged sword that lulls you into a false sense of security. The rapid pace of innovation and external pressures, combined with easy, often misunderstood tools, combines to create an environment where mistakes and human error can bloom.</p>
<p>Human error is unavoidable, and we can only hope to influence brand marketing, so now what do you do? There are some key steps you can take to put your company in a position to avoid major catastrophes.</p>
<ul>
<li><strong>First, users need to trust but verify</strong>. Ask questions, understand the technology you are implementing, and learn the advantages and limitations to the systems you are integrating.</li>
<li><strong>Second, implement best practices and create a depth of defense.</strong> Preventing people from accessing systems with firewalls or access controls is a good start, but you should also plan for someone to get in both on purpose and by accident. Build and anticipate response scenarios to known threats as well as to random occurrences.</li>
<li><strong>Third, focus on the data.</strong> Simply getting into a system is not the real threat; access to user data is. Encrypt your data, then audit and look for any holes in how users access and disseminate the data. Set up alerts for any and all suspicious behavior, and build strong data policies to keep and maintain control in the long term.</li>
<li><strong>Finally, check all the doors.</strong> One of the main areas where outages and breaches occur is in untested and ancillary spaces that no one thought to look. In the case of Capital One, the data was in Amazon Simple Storage Service (S3) buckets.</li>
</ul>
<p>S3 is marketed as a platform that allows you to “store your data and secure it from unauthorized access”. But, how you manage access and how you configure and set up your system is your responsibility, not Amazon’s.</p>
<p>So what happens? Sometimes, services like S3 are not used just in primary applications, but for offline storage, backups, and transient storage. These secondary areas are typically not tested and secured the same way overall applications are. A great deal of time is spent making sure applications are bulletproof, but oftentimes secondary systems are overlooked.</p>
<p><strong>The gaming industry is a prime example</strong>. If a game launches and experiences issues, the game itself is not the issue but the integrated apps, such as leaderboards or matchmaking, that were left untested are determined to be the cause. It&#8217;s the same thing here—the app is tested and made secure, but the job to backup or load data in the app were not tested and are at risk.</p>
<p>Data breaches are becoming a common occurrence. However, with clear roles and responsibilities, better awareness, in-depth training practices, and a thorough examination of all your entry points, you can protect your business and your customer&#8217;s data from becoming an easy target.</p>
<h3>About Percona</h3>
<p>Percona delivers enterprise-class solutions for MySQL®, MariaDB®, MongoDB®, and PostgreSQL across traditional and cloud-based platforms, including Amazon AWS. As a leading provider of unbiased open source database solutions, Percona helps organizations easily, securely, and affordably maintain business agility, minimize risks, and stay competitive.</p>
<p>For more information, visit <a target="_blank" href="http://www.percona.com">www.percona.com</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/08/01/your-personal-data-just-leaked-and-its-your-fault/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/08/01/your-personal-data-just-leaked-and-its-your-fault/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Vinicius Grippa</name>
					</author>
		<title type="html"><![CDATA[MySQL 8.0.17 Clone Plugin: How to Create a Slave from Scratch]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/08/01/mysql-8-0-17-clone-plugin-how-to-create-a-slave-from-scratch/" />
		<id>https://www.percona.com/blog/?p=59651</id>
		<updated>2019-08-01T15:35:20Z</updated>
		<published>2019-08-01T15:24:35Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="113" src="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-200x113.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL 8.0.17 Clone Plugin" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-200x113.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-300x169.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1024x576.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-367x206.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />In this blog post, we will discuss a new feature &#8211; the MySQL 8.0.17 clone plugin. Here I will demonstrate how easy it is to use to create the “classic” replication, building the standby replica from scratch. The clone plugin permits cloning data locally or from a remote MySQL server instance. The cloned data is [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/08/01/mysql-8-0-17-clone-plugin-how-to-create-a-slave-from-scratch/"><![CDATA[<img width="200" height="113" src="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-200x113.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL 8.0.17 Clone Plugin" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-200x113.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-300x169.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1024x576.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-367x206.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59705" src="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1-200x144.jpg" alt="MySQL 8.0.17 Clone Plugin" width="200" height="144" srcset="https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1-200x144.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1-300x216.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1-367x264.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/08/MySQL-8.0.17-Clone-Plugin-1.jpg 550w" sizes="(max-width: 200px) 100vw, 200px" />In this blog post, we will discuss a new feature &#8211; the MySQL 8.0.17 <a target="_blank" href="https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-17.html" target="_blank" rel="&quot;noopener&quot; noopener noreferrer">clone plugin</a>. Here I will demonstrate how easy it is to use to create the “classic” replication, building the standby replica from scratch.</p>
<p>The clone plugin permits cloning data locally or from a remote MySQL server instance. The cloned data is a physical snapshot of data stored in <code class="literal">InnoDB</code>, and this means, for example, that the data can be used to create a standby replica.</p>
<p>Let&#8217;s go to the hands-on and see how it works.</p>
<h2>Installation &amp; validation process of the MySQL 8.0.17 clone plugin</h2>
<p>Installation is very easy and it works in the same as <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/install-plugin.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">installing other plugins</a>. Below is the command line to install the clone plugin:</p><pre class="crayon-plain-tag">master [localhost:45008] {msandbox} ((none)) &gt; INSTALL PLUGIN clone SONAME 'mysql_clone.so';
Query OK, 0 rows affected (0.00 sec)</pre><p>And how to check if the clone plugin is active:</p><pre class="crayon-plain-tag">master [localhost:45008] {msandbox} ((none)) &gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS 
WHERE PLUGIN_NAME LIKE 'clone';
+-------------+---------------+
| PLUGIN_NAME | PLUGIN_STATUS |
+-------------+---------------+
| clone | ACTIVE |
+-------------+---------------+
1 row in set (0.00 sec)</pre><p>Note that these steps need to be executed on the <strong>Donor</strong> (<em><a target="_blank" href="https://www.collinsdictionary.com/dictionary/english/aka" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">aka</a></em> master) and on the <strong>Recipient</strong> (<em>aka</em> slave if the clone is being used to create a replica).</p>
<p>After executing the installation, the plugin will be loaded automatically across restarts, so you don&#8217;t need to worry about this anymore.</p>
<p>Next, we will create the user with the necessary privilege on the <strong>Donor</strong>, so we can connect to the instance remotely to clone it.</p><pre class="crayon-plain-tag">master [localhost:45008] {msandbox} ((none)) &gt; create user clone_user@'%' identified by 'sekret';
Query OK, 0 rows affected (0.01 sec)

master [localhost:45008] {msandbox} ((none)) &gt; GRANT BACKUP_ADMIN ON *.* TO 'clone_user'@'%';
Query OK, 0 rows affected (0.00 sec)</pre><p>As a security measure, I recommend replacing the <strong>%</strong> for the <em>IP/hostname </em>or<em> network mask</em> of the <strong>Recipient</strong> so the connections will be accepted only by the future replica server.  Now, on the <strong>Recipient </strong>server, the clone user requires the <a target="_blank" class="link" href="https://dev.mysql.com/doc/refman/8.0/en/privileges-provided.html#priv_clone-admin"><code class="literal">CLONE_ADMIN</code></a> privilege for replacing recipient data, blocking DDL during the cloning operation and automatically restarting the server.</p><pre class="crayon-plain-tag">slave1 [localhost:45009] {msandbox} ((none)) &gt; create user clone_user@'localhost' identified by 'sekret'; 
Query OK, 0 rows affected (0.01 sec) 

slave1 [localhost:45009] {msandbox} ((none)) &gt; GRANT CLONE_ADMIN ON *.* TO 'clone_user'@'localhost'; 
Query OK, 0 rows affected (0.00 sec)</pre><p>Next, with the plugin installed and validated, and users created on both <strong>Donor</strong> and <strong>Recipient </strong>servers, let&#8217;s proceed to the cloning process.</p>
<h2>Cloning process</h2>
<p>As mentioned, the cloning process can be executed locally or remotely.  Also, it supports replication, which means that the cloning operation extracts and transfers replication coordinates from the donor and applies them on the recipient. It can be used for GTID or non-GTID replication.</p>
<p>So, to begin the cloning process, first, let&#8217;s make sure that there&#8217;s a valid donor. This is controlled by <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/clone-plugin-options-variables.html#sysvar_clone_valid_donor_list" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">clone_valid_donor_list</a> parameter. As it is a dynamic parameter, you can change it while the server is running. Using the <em>show variables</em> command will show if the parameter has a valid donor:</p><pre class="crayon-plain-tag">slave1 [localhost:45009] {msandbox} ((none)) &gt; SHOW VARIABLES LIKE 'clone_valid_donor_list';
+------------------------+-------+
| Variable_name | Value |
+------------------------+-------+
| clone_valid_donor_list | |
+------------------------+-------+
1 row in set (0.01 sec)</pre><p>In our case, we need to set it. So let&#8217;s change it:</p><pre class="crayon-plain-tag">slave1 [localhost:45009] {msandbox} ((none)) &gt; set global clone_valid_donor_list = '127.0.0.1:45008';
Query OK, 0 rows affected (0.00 sec)</pre><p>The next step is not mandatory, but using the default <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_log_error_verbosity" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">log_error_verbosity </a>the error log does not display much information about the cloning progress. So, for this example, I will adjust the verbosity to a higher level (on the <strong>Donor</strong> and the <strong>Recipient</strong>):</p><pre class="crayon-plain-tag">mysql &gt; set global log_error_verbosity=3;
Query OK, 0 rows affected (0.00 sec)</pre><p>Now, let&#8217;s start the cloning process on the <strong>Recipient</strong>:</p><pre class="crayon-plain-tag">slave1 [localhost:45009] {msandbox} ((none)) &gt; CLONE INSTANCE FROM clone_user@127.0.0.1:45008 identified by 'sekret';
Query OK, 0 rows affected (38.58 sec)</pre><p>It is possible to observe the cloning progress in the error log of both servers. Below is the output of the <strong>Donor</strong>:</p><pre class="crayon-plain-tag">2019-07-31T12:48:48.558231Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: Acquired backup lock.'
2019-07-31T12:48:48.558307Z 47 [Note] [MY-013457] [InnoDB] Clone Begin Master Task by clone_user@localhost
2019-07-31T12:48:48.876138Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_INIT: Storage Initialize.'
2019-07-31T12:48:48.876184Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_RES_COMPLETE.'
2019-07-31T12:48:53.996976Z 48 [Note] [MY-013458] [InnoDB] Clone set state change ACK: 1
2019-07-31T12:48:53.997046Z 48 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_ACK: Storage Ack.'
2019-07-31T12:48:53.997148Z 48 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_RES_COMPLETE.'
2019-07-31T12:48:54.096766Z 47 [Note] [MY-013458] [InnoDB] Clone Master received state change ACK
2019-07-31T12:48:54.096847Z 47 [Note] [MY-013458] [InnoDB] Clone State Change : Number of tasks = 1
2019-07-31T12:48:54.096873Z 47 [Note] [MY-013458] [InnoDB] Clone State BEGIN FILE COPY

...

2019-07-31T12:49:33.939968Z 47 [Note] [MY-013457] [InnoDB] Clone End Master Task ID: 0 Passed, code: 0:
2019-07-31T12:49:33.940016Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_EXIT: Storage End.'
2019-07-31T12:49:33.940115Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: COM_RES_COMPLETE.'
2019-07-31T12:49:33.940150Z 47 [Note] [MY-013273] [Clone] Plugin Clone reported: 'Server: Exiting clone protocol.'</pre><p>And the <strong>Recipient</strong>:</p><pre class="crayon-plain-tag">2019-07-31T12:48:48.521515Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Task Connect.'
2019-07-31T12:48:48.557855Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Master ACK Connect.'
2019-07-31T12:48:48.557923Z 8 [Note] [MY-013457] [InnoDB] Clone Apply Begin Master Version Check
2019-07-31T12:48:48.558474Z 8 [Note] [MY-013457] [InnoDB] Clone Apply Version End Master Task ID: 0 Passed, code: 0:
2019-07-31T12:48:48.558507Z 8 [Note] [MY-013457] [InnoDB] Clone Apply Begin Master Task
2019-07-31T12:48:48.558749Z 8 [Warning] [MY-013460] [InnoDB] Clone removing all user data for provisioning: Started
2019-07-31T12:48:48.558769Z 8 [Note] [MY-011977] [InnoDB] Clone Drop all user data
2019-07-31T12:48:48.863134Z 8 [Note] [MY-011977] [InnoDB] Clone: Fix Object count: 371 task: 0
2019-07-31T12:48:53.829493Z 8 [Note] [MY-011977] [InnoDB] Clone Drop User schemas
2019-07-31T12:48:53.829948Z 8 [Note] [MY-011977] [InnoDB] Clone: Fix Object count: 5 task: 0
2019-07-31T12:48:53.838939Z 8 [Note] [MY-011977] [InnoDB] Clone Drop User tablespaces
2019-07-31T12:48:53.839800Z 8 [Note] [MY-011977] [InnoDB] Clone: Fix Object count: 6 task: 0
2019-07-31T12:48:53.910728Z 8 [Note] [MY-011977] [InnoDB] Clone Drop: finished successfully
...
2019-07-31T12:49:33.836509Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Command COM_EXECUTE.'
2019-07-31T12:49:33.836998Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Master ACK COM_EXIT.'
2019-07-31T12:49:33.839498Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Master ACK Disconnect : abort: false.'
2019-07-31T12:49:33.851403Z 0 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Command COM_EXECUTE.'
2019-07-31T12:49:33.851796Z 0 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Task COM_EXIT.'
2019-07-31T12:49:33.852398Z 0 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Task Disconnect : abort: false.'
2019-07-31T12:49:33.852472Z 0 [Note] [MY-013457] [InnoDB] Clone Apply End Task ID: 1 Passed, code: 0:
2019-07-31T12:49:33.940156Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Task COM_EXIT.'
2019-07-31T12:49:33.940810Z 8 [Note] [MY-013272] [Clone] Plugin Clone reported: 'Client: Task Disconnect : abort: false.'
2019-07-31T12:49:33.944244Z 8 [Note] [MY-013457] [InnoDB] Clone Apply End Master Task ID: 0 Passed, code: 0:</pre><p>Note that the MySQL server on the <strong>Recipient</strong> will be restarted after the cloning process finishes. After this, the database is ready to be accessed and the final step is setting up the replica.</p>
<h2>The replica process</h2>
<p>Both binary log position (filename, offset) and GTID coordinates are extracted and transferred from the donor MySQL server instance.</p>
<p>The queries below can be executed on the cloned MySQL server instance to view the binary log position or the GTID of the last transaction that was applied:</p><pre class="crayon-plain-tag"># Binary log position
slave1 [localhost:45009] {msandbox} ((none)) &gt; SELECT BINLOG_FILE, BINLOG_POSITION FROM performance_schema.clone_status;
+------------------+-----------------+
| BINLOG_FILE | BINLOG_POSITION |
+------------------+-----------------+
| mysql-bin.000001 | 437242601 |
+------------------+-----------------+
1 row in set (0.01 sec)

# GTID
slave1 [localhost:45009] {msandbox} ((none)) &gt; SELECT @@GLOBAL.GTID_EXECUTED;
+----------------------------------------------+
| @@GLOBAL.GTID_EXECUTED |
+----------------------------------------------+
| 00045008-1111-1111-1111-111111111111:1-32968 |
+----------------------------------------------+
1 row in set (0.00 sec)</pre><p>With the information in hand, we need to execute the <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/clone-plugin-replication.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">CHANGE MASTER</a> command accordingly:</p><pre class="crayon-plain-tag">slave1 [localhost:45009] {msandbox} ((none)) &gt; CHANGE MASTER TO MASTER_HOST = '127.0.0.1', MASTER_PORT = 45008,MASTER_USER='root',MASTER_PASSWORD='msandbox',MASTER_AUTO_POSITION = 1;
Query OK, 0 rows affected, 2 warnings (0.02 sec)

slave1 [localhost:45009] {msandbox} ((none)) &gt; start slave;
Query OK, 0 rows affected (0.00 sec)</pre><p></p>
<h2>Limitations</h2>
<p>The clone plugin has some limitations and they are described <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/clone-plugin-limitations.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">here</a>. In my opinion, two major limitations will be faced by the community. First, it is the ability to clone only InnoDB tables. This means that  <a target="_blank" class="link" title="16.2 The MyISAM Storage Engine" href="https://dev.mysql.com/doc/refman/8.0/en/myisam-storage-engine.html"><code class="literal">MyISAM</code></a> and <a target="_blank" class="link" title="16.4 The CSV Storage Engine" href="https://dev.mysql.com/doc/refman/8.0/en/csv-storage-engine.html"><code class="literal">CSV</code></a> tables stored in any schema including the <code class="literal">sys</code> schema will be cloned as empty tables.</p>
<p>The other limitation is regarding DDL, including TRUNCATE TABLE, which is not permitted during a cloning operation. Concurrent DML, however, is permitted. If a DDL is running, then the clone operation will wait for the lock:</p><pre class="crayon-plain-tag">+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+
| Id | User            | Host            | db   | Command          | Time  | State                                                         | Info                             |
+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+
| 63 | clone_user      | localhost:34402 | NULL | clone            |     3 | Waiting for backup lock                                       | NULL                             |
+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+</pre><p>Otherwise, if the clone operation is running, the DDL will wait for the lock:</p><pre class="crayon-plain-tag">+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+
| Id | User            | Host            | db   | Command          | Time  | State                                                         | Info                             |
+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+
| 52 | msandbox        | localhost       | test | Query            |     5 | Waiting for backup lock                                       | alter table joinit engine=innodb |
| 60 | clone_user      | localhost:34280 | NULL | clone            |    15 | Receiving from client                                         | NULL                             |
| 61 | clone_user      | localhost:34282 | NULL | clone            |    15 | Receiving from client                                         | NULL                             |
| 62 | clone_user      | localhost:34284 | NULL | clone            |     6 | Receiving from client                                         | NULL                             |
+----+-----------------+-----------------+------+------------------+-------+---------------------------------------------------------------+----------------------------------+</pre><p></p>
<h2>Conclusion</h2>
<p>Creating replicas has become much easier with the help of the MySQL 8.0.17 clone plugin. This feature can be used using <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/clone-plugin-remote.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">SSL connections and with encrypted data</a> as well.  At the moment of publication of this blog post, the clone plugin can be used to set up not only asynchronous replicas but provisioning Group Replication members too. Personally, I believe that in the near future this feature will also be used for Galera clusters. This is my two cents for what the future holds.</p>
<h2>Useful Resources</h2>
<p>Finally, you can reach us through the social networks, our forum or access our material using the links presented below:</p>
<ul>
<li style="font-weight: 400;"><b>Blog: </b><a target="_blank" href="https://www.percona.com/blog/"><span style="font-weight: 400;">https://www.percona.com/blog/</span></a><b> </b></li>
<li style="font-weight: 400;"><b>Solution Briefs: </b><a target="_blank" href="https://www.percona.com/resources/solution-brief"><span style="font-weight: 400;">https://www.percona.com/resources/solution-brief</span></a><b> </b></li>
<li style="font-weight: 400;"><b>White Papers: </b><a target="_blank" href="https://www.percona.com/resources/white-papers"><span style="font-weight: 400;">https://www.percona.com/resources/white-papers</span></a><b> </b></li>
<li style="font-weight: 400;"><b>Ebooks: </b><a target="_blank" href="https://www.percona.com/resources/ebooks"><span style="font-weight: 400;">https://www.percona.com/resources/ebooks</span></a></li>
<li style="font-weight: 400;"><b>Technical Presentations archive: </b><a target="_blank" href="https://www.percona.com/resources/technical-presentations"><span style="font-weight: 400;">https://www.percona.com/resources/technical-presentations</span></a><b> </b></li>
<li style="font-weight: 400;"><b>Videos/Recorded Webinars: </b><span style="font-weight: 400;"><a target="_blank" href="https://www.percona.com/resources/videos">https://www.percona.com/resources/videos</a></span></li>
<li><strong>Forum</strong>: <a target="_blank" href="https://www.percona.com/forums/">https://www.percona.com/forums/</a></li>
<li style="font-weight: 400;"><b>Knowledge Base (Percona Subscriber exclusive content): </b><a target="_blank" href="https://customers.percona.com">https://customers.percona.com</a></li>
</ul>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/08/01/mysql-8-0-17-clone-plugin-how-to-create-a-slave-from-scratch/#comments" thr:count="8"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/08/01/mysql-8-0-17-clone-plugin-how-to-create-a-slave-from-scratch/feed/atom/" thr:count="8"/>
		<thr:total>8</thr:total>
			</entry>
		<entry>
		<author>
			<name>Jobin Augustine</name>
					</author>
		<title type="html"><![CDATA[PostgreSQL: Simple C extension Development for a Novice User (and Performance Advantages)]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/31/postgresql-simple-c-extension-development-for-a-novice-user/" />
		<id>https://www.percona.com/blog/?p=59025</id>
		<updated>2019-07-31T14:15:10Z</updated>
		<published>2019-07-31T14:15:10Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="PostegreSQL simple C extension development" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />One of the great features of PostgreSQL is its extendability. My colleague and senior PostgreSQL developer Ibar has blogged about developing an extension with much broader capabilities including callback functionality. But in this blog post, I am trying to address a complete novice user who has never tried but wants to develop a simple function [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/31/postgresql-simple-c-extension-development-for-a-novice-user/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="PostegreSQL simple C extension development" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostegreSQL-simple-C-extension-development.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59660" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-Simple-C-extension-200x127.png" alt="PostgreSQL Simple C extension" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-Simple-C-extension-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-Simple-C-extension.png 300w" sizes="(max-width: 200px) 100vw, 200px" />One of the great features of PostgreSQL is its extendability. My <span data-dobid="hdw">colleague and senior PostgreSQL developer Ibar has blogged about <a target="_blank" href="https://www.percona.com/blog/2019/04/05/writing-postgresql-extensions-is-fun-c-language/">developing an extension with much broader capabilities including callback functionality</a>. But in this blog post, I am trying to address a complete novice user who has never tried but wants to develop a simple function with business logic. Towards the end of the blog post, I want to show how lightweight the function is by doing simple benchmarking which is repeatable and should act as a strong justification for why end-users should do such development.</span></p>
<p>Generally, PostgreSQL and extension developers work on a PostgreSQL source build. For a novice user, that may not be required, instead, dev/devel packages provided for the Linux distro would be sufficient. Assuming that you have installed PostgreSQL already, the following steps can get you the additional development libraries required.</p>
<p><strong>On Ubuntu/Debian</strong></p><pre class="crayon-plain-tag">$ sudo apt install postgresql-server-dev-11</pre><p><strong>On RHEL/CentOS</strong></p><pre class="crayon-plain-tag">sudo yum install postgresql11-devel</pre><p>The next step is to add a PostgreSQL binary path to your environment, to ensure that pg_config is there in the path. In my Ubuntu laptop, this is how:</p><pre class="crayon-plain-tag">export PATH=/usr/lib/postgresql/11/bin:$PATH</pre><p>Above mentioned paths may vary according to the environment.</p>
<p>Please make sure that the pg_config is executing without specifying the path:</p><pre class="crayon-plain-tag">$ pg_config</pre><p>PostgreSQL installation provides a build infrastructure for extensions, called PGXS, so that simple extension modules can be built simply against an already-installed server. It automates common build rules for simple server extension modules.</p><pre class="crayon-plain-tag">$ pg_config --pgxs
/usr/lib/postgresql/11/lib/pgxs/src/makefiles/pgxs.mk</pre><p>Now let&#8217;s create a directory for development. I am going to develop a simple extension addme with a function addme to add 2 numbers.</p><pre class="crayon-plain-tag">$ mkdir addme</pre><p>Now we need to create a Makefile which builds the extension. Luckily, we can use all PGXS macros.</p><pre class="crayon-plain-tag">MODULES = addme
EXTENSION = addme
DATA = addme--0.0.1.sql
PG_CONFIG = pg_config
PGXS := $(shell $(PG_CONFIG) --pgxs)
include $(PGXS)</pre><p>MODULE specifies the shared object without file extension and EXTENSION specifies the name of the extension name. DATA defines the installation script. The reason for &#8211;0.0.1 specifying in the name is that I should match the version we specify in the control file.</p>
<p>Now we need a control file addme.control with the following content:</p><pre class="crayon-plain-tag">comment = 'Simple number add function'
default_version = '0.0.1'
relocatable = true
module_pathname = '$libdir/addme'</pre><p>And we can prepare our function in C which will add 2 integers:</p><pre class="crayon-plain-tag">#include "postgres.h"
#include "fmgr.h"

PG_MODULE_MAGIC;

PG_FUNCTION_INFO_V1(addme);

Datum
addme(PG_FUNCTION_ARGS)
{
int32 arg1 = PG_GETARG_INT32(0);
int32 arg2 = PG_GETARG_INT32(1);

PG_RETURN_INT32(arg1 + arg2);
}</pre><p>At this stage, we have only 3 files in the directory.</p><pre class="crayon-plain-tag">$ ls
addme.c addme.control Makefile</pre><p>Now we can make the file:</p><pre class="crayon-plain-tag">$ make</pre><p>For installing the extension, we need a SQL file with create function. This SQL file name should be the same as the one we specified in DATA parameter in the Makefile, which is addme&#8211;0.0.1.sql</p>
<p>Add the following content into this file:</p><pre class="crayon-plain-tag">CREATE OR REPLACE FUNCTION
addme(int,int) RETURNS int AS 'MODULE_PATHNAME','addme'
LANGUAGE C STRICT;</pre><p>And install the extension:</p><pre class="crayon-plain-tag">$ sudo make install</pre><p>Now we can proceed to create the extension and test it:</p><pre class="crayon-plain-tag">postgres=# create extension addme;
CREATE EXTENSION
postgres=# select addme(2,3);
addme
-------
5
(1 row)</pre><p>Just like any function, we can use it in queries against multiple tuples.</p><pre class="crayon-plain-tag">postgres=# select 7||'+'||g||'='||addme(7,g) from generate_series(1,10) as g;
?column?
----------
7+1=8
7+2=9
7+3=10
7+4=11
7+5=12
7+6=13
7+7=14
7+8=15
7+9=16
7+10=17
(10 rows)</pre><p></p>
<h2>Performance Benchmarking</h2>
<p>Now it is important to understand the performance characteristics calling a C function in extension. For comparison, we have two  options like:<br />
1. &#8216;+&#8217; operator provided by SQL like <pre class="crayon-plain-tag">select 1+2;</pre><br />
2. PLpgSQL function as below</p><pre class="crayon-plain-tag">CREATE FUNCTION addmepl(a integer, b integer)
 RETURNS integer
as $$ 
BEGIN
  return a+b;
END;
$$ LANGUAGE plpgsql;</pre><p>For this test/benchmark, I am going to call the function for <strong>1 million times!</strong></p>
<p><strong>SQL + operator</strong></p><pre class="crayon-plain-tag">time psql -c "select floor(random() * (100-1+1) + 1)::int+g from generate_series(1,1000000) as g" &gt; out.txt</pre><p><strong>C function call</strong></p><pre class="crayon-plain-tag">$ time psql -c "select addme(floor(random() * (100-1+1) + 1)::int,g) from generate_series(1,1000000) as g" &gt; out.txt</pre><p><strong>PL function call</strong></p><pre class="crayon-plain-tag">$ time psql -c "select addmepl(floor(random() * (100-1+1) + 1)::int,g) from generate_series(1,1000000) as g" &gt; out.txt</pre><p>I have performed the tests 6 times for each case and tabulated below.</p>
<p><strong>Test Run</strong><br />
<img class="size-full wp-image-59029 aligncenter" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-01-59.png" alt="" width="432 height=" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-01-59.png 432w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-01-59-200x116.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-01-59-300x174.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-01-59-367x213.png 367w" sizes="(max-width: 432px) 100vw, 432px" /><br />
<img class="size-full wp-image-59030 aligncenter" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-27-29.png" alt="" width="485" height="298" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-27-29.png 485w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-27-29-200x123.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-27-29-300x184.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Screenshot-from-2019-07-15-19-27-29-367x225.png 367w" sizes="(max-width: 485px) 100vw, 485px" /></p>
<p>As we can see, the performance of Built in &#8216;+&#8217; operator and the custom C function in the extension takes the least time with almost the same performance. But the PLpgSQL function call is slow and it shows considerable overhead. Hope this justifies why those functions, which are heavily used, need to be written as a native C extension.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/31/postgresql-simple-c-extension-development-for-a-novice-user/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/31/postgresql-simple-c-extension-development-for-a-novice-user/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>David Quilty</name>
					</author>
		<title type="html"><![CDATA[Blog Poll: Who&#8217;s Responsible for Securing the Data in your Databases?]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/31/blog-poll-whos-responsible-for-securing-the-data-in-your-databases/" />
		<id>https://www.percona.com/blog/?p=59653</id>
		<updated>2019-07-31T12:10:19Z</updated>
		<published>2019-07-31T12:10:19Z</published>
		<category scheme="https://www.percona.com/blog" term="Database Poll" /><category scheme="https://www.percona.com/blog" term="database poll" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="blog poll" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />Next up in line for the blog poll series is a question about database management. Last year, we asked you a few questions in a blog poll and we received a great amount of feedback. We wanted to follow up on those some of those same survey questions to see what may have changed over [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/31/blog-poll-whos-responsible-for-securing-the-data-in-your-databases/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="blog poll" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59656" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg" alt="blog poll" width="200" height="112" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Poll_-Who-manages-you-database_.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />Next up in line for the blog poll series is a question about database management. Last year, we asked you a few questions in a blog poll and we received a great amount of feedback. We wanted to follow up on those some of those same survey questions to see what may have changed over the last 12 months. So with that in mind, we&#8217;re hoping you can take a minute or so to answer the first survey question in this series: <strong>In your company, who is responsible for securing the data in your databases?</strong> DBAs? Security team? Cloud vendor? We would like to know!</p>
Note: There is a poll embedded within this post, please visit the site to participate in this post's poll.
<p>This poll will be up for one month and will be maintained over in the sidebar should you wish to come back at a later date and take part. We look forward to seeing your responses!</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/31/blog-poll-whos-responsible-for-securing-the-data-in-your-databases/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/31/blog-poll-whos-responsible-for-securing-the-data-in-your-databases/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Avinash Vallarapu</name>
					</author>
		<title type="html"><![CDATA[Using plpgsql_check to Find Compilation Errors and Profile Functions]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/30/using-plpgsql_check-to-find-compilation-errors-and-profile-functions/" />
		<id>https://www.percona.com/blog/?p=58721</id>
		<updated>2019-08-01T19:00:36Z</updated>
		<published>2019-07-30T18:02:26Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-200x133.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="plpgsql_check" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-200x133.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-1024x683.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-367x245.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" />There is always a need for profiling tools in databases for admins or developers. While it is easy to understand the point where an SQL is spending more time using [crayon-5d433ac36b52d289319291-i/] or [crayon-5d433ac36b533921405899-i/] in PostgreSQL, the same would not work for functions. Recently, Jobin has published a blog post where he demonstrated how plprofiler can [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/30/using-plpgsql_check-to-find-compilation-errors-and-profile-functions/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-200x133.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="plpgsql_check" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-200x133.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-1024x683.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1-367x245.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-1.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright wp-image-59640 size-thumbnail" src="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-200x127.png" alt="plpgsql_check" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/plpgsql_check.png 300w" sizes="(max-width: 200px) 100vw, 200px" />There is always a need for profiling tools in databases for admins or developers. While it is easy to understand the point where an SQL is spending more time using <a target="_blank" href="https://www.postgresql.org/docs/11/sql-explain.html"><pre class="crayon-plain-tag">EXPLAIN</pre></a> or <a target="_blank" href="https://www.postgresql.org/docs/11/sql-explain.html#ANALYZE"><pre class="crayon-plain-tag">EXPLAIN ANALYZE</pre></a> in PostgreSQL, the same would not work for functions. Recently, Jobin has published a blog post where he demonstrated how <a target="_blank" href="https://www.percona.com/blog/2019/02/13/plprofiler-getting-a-handy-tool-for-profiling-your-pl-pgsql-code/">plprofiler</a> can be useful in profiling functions. <pre class="crayon-plain-tag">plprofiler</pre> builds call graphs and creates flame graphs which make the report very easy to understand. Similarly, there is another interesting project called <pre class="crayon-plain-tag">plpgsql_check</pre> which can be used for a similar purpose as <pre class="crayon-plain-tag">plprofiler</pre>, while it also looks at code and points out compilation errors. Let us see all of that in action, in this blog post.</p>
<h2>Installing plpgsql-check</h2>
<p>You could use yum on RedHat/CentOS to install this extension from PGDG repository. Steps to perform source installation on Ubuntu/Debian are also mentioned in the following logs.</p>
<p><strong>On RedHat/CentOS</strong></p><pre class="crayon-plain-tag">$ sudo yum install plpgsql_check_11</pre><p><strong>On Ubuntu/Debian</strong></p><pre class="crayon-plain-tag">$ sudo apt-get install postgresql-server-dev-11 libicu-dev gcc make 
$ git clone https://github.com/okbob/plpgsql_check.git 
$ cd plpgsql_check/ 
$ make &amp;&amp; make install</pre><p></p>
<h5>Creating and enabling this extension</h5>
<p>There are 3 advantages of using <pre class="crayon-plain-tag">plpgsql_check</pre></p>
<ol>
<li>Checking for compilation errors in a function code</li>
<li>Finding dependencies in functions</li>
<li>Profiling functions</li>
</ol>
<p>When using plpgsql_check for the first 2 requirements, you may <strong>not</strong> need to add any entry to <pre class="crayon-plain-tag">shared_preload_libraries</pre>. However, if you need to use it for profiling functions (3), then you should add appropriate entries to <pre class="crayon-plain-tag">shared_preload_libraries</pre> so that it could load both <pre class="crayon-plain-tag">plpgsql</pre> and <pre class="crayon-plain-tag">plpgsql_check</pre>. Due to dependencies, <pre class="crayon-plain-tag">plpgsql</pre> must be before <pre class="crayon-plain-tag">plpgsql_check</pre> in the <pre class="crayon-plain-tag">shared_preload_libraries</pre> config as you see in the following example :</p><pre class="crayon-plain-tag">shared_preload_libraries = plpgsql, plpgsql_check</pre><p>Any change to <pre class="crayon-plain-tag">shared_preload_libraries</pre> requires a restart. You may see the following error when you do not have <pre class="crayon-plain-tag">plpgsql</pre> before <pre class="crayon-plain-tag">plpgsql_check</pre> in the <pre class="crayon-plain-tag">shared_preload_libraries</pre> config.</p><pre class="crayon-plain-tag">$ grep "shared_preload_libraries" $PGDATA/postgresql.auto.conf
shared_preload_libraries = 'pg_qualstats, pg_stat_statements, plpgsql_check'

$ pg_ctl -D /var/lib/pgsql/11/data restart -mf
waiting for server to shut down.... done
server stopped
waiting for server to start....2019-07-07 02:07:10.104 EDT [6423] 
FATAL: could not load library "/usr/pgsql-11/lib/plpgsql_check.so": /usr/pgsql-11/lib/plpgsql_check.so: undefined symbol: plpgsql_parser_setup
2019-07-07 02:07:10.104 EDT [6423] LOG: database system is shut down
stopped waiting
pg_ctl: could not start server
Examine the log output.

$ grep "shared_preload_libraries" $PGDATA/postgresql.auto.conf
shared_preload_libraries = 'pg_qualstats, pg_stat_statements, plpgsql, plpgsql_check'

$ pg_ctl -D /var/lib/pgsql/11/data start
.....
server started</pre><p>Once the PostgreSQL instance is started, create this extension in the database where you need to perform any of the previously discussed 3 tasks.</p><pre class="crayon-plain-tag">$ psql -d percona -c "CREATE EXTENSION plpgsql_check"
CREATE EXTENSION</pre><p></p>
<h3>Finding Compilation Errors</h3>
<p>As discussed earlier, this extension can help developers and admins determine compilation errors. But why is it needed? Let&#8217;s consider the following example where we see no errors while creating the function. By the way, I have taken this example from my <a target="_blank" href="https://www.percona.com/blog/2019/07/22/automatic-index-recommendations-in-postgresql-using-pg_qualstats-and-hypopg/">previous blog post</a> where I was talking about Automatic Index recommendations using <pre class="crayon-plain-tag">hypopg</pre> and <pre class="crayon-plain-tag">pg_qualstats</pre>. You might want to read that blog post to understand the logic of the following function.</p><pre class="crayon-plain-tag">percona=# CREATE OR REPLACE FUNCTION find_usable_indexes()
percona-# RETURNS VOID AS
percona-# $$
percona$# DECLARE
percona$#     l_queries     record;
percona$#     l_querytext     text;
percona$#     l_idx_def       text;
percona$#     l_bef_exp       text;
percona$#     l_after_exp     text;
percona$#     hypo_idx      record;
percona$#     l_attr        record;
percona$#     /* l_err       int; */
percona$# BEGIN
percona$#     CREATE TABLE IF NOT EXISTS public.idx_recommendations (queryid bigint, 
percona$#     query text, current_plan jsonb, recmnded_index text, hypo_plan jsonb);
percona$#     FOR l_queries IN
percona$#     SELECT t.relid, t.relname, t.queryid, t.attnames, t.attnums, 
percona$#     pg_qualstats_example_query(t.queryid) as query
percona$#       FROM 
percona$#         ( 
percona$#          SELECT qs.relid::regclass AS relname, qs.relid AS relid, qs.queryid, 
percona$#          string_agg(DISTINCT attnames.attnames,',') AS attnames, qs.attnums
percona$#          FROM pg_qualstats_all qs
percona$#          JOIN pg_qualstats q ON q.queryid = qs.queryid
percona$#          JOIN pg_stat_statements ps ON q.queryid = ps.queryid
percona$#          JOIN pg_amop amop ON amop.amopopr = qs.opno
percona$#          JOIN pg_am ON amop.amopmethod = pg_am.oid,
percona$#          LATERAL 
percona$#               ( 
percona$#                SELECT pg_attribute.attname AS attnames
percona$#                FROM pg_attribute
percona$#                JOIN unnest(qs.attnums) a(a) ON a.a = pg_attribute.attnum 
percona$#                AND pg_attribute.attrelid = qs.relid
percona$#                ORDER BY pg_attribute.attnum) attnames,     
percona$#          LATERAL unnest(qs.attnums) attnum(attnum)
percona$#                WHERE NOT 
percona$#                (
percona$#                 EXISTS 
percona$#                       ( 
percona$#                        SELECT 1
percona$#                        FROM pg_index i
percona$#                        WHERE i.indrelid = qs.relid AND 
percona$#                        (arraycontains((i.indkey::integer[])[0:array_length(qs.attnums, 1) - 1], 
percona$#                         qs.attnums::integer[]) OR arraycontains(qs.attnums::integer[], 
percona$#                         (i.indkey::integer[])[0:array_length(i.indkey, 1) + 1]) AND i.indisunique)))
percona$#                        GROUP BY qs.relid, qs.queryid, qs.qualnodeid, qs.attnums) t
percona$#                        GROUP BY t.relid, t.relname, t.queryid, t.attnames, t.attnums                   
percona$#     LOOP
percona$#         /* RAISE NOTICE '% : is queryid',l_queries.queryid; */
percona$#         execute 'explain (FORMAT JSON) '||l_queries.query INTO l_bef_exp;
percona$#         execute 'select hypopg_reset()';
percona$#         execute 'SELECT indexrelid,indexname FROM hypopg_create_index(''CREATE INDEX on '||l_queries.relname||'('||l_queries.attnames||')'')' INTO hypo_idx;      
percona$#         execute 'explain (FORMAT JSON) '||l_queries.query INTO l_after_exp;
percona$#         execute 'select hypopg_get_indexdef('||hypo_idx.indexrelid||')' INTO l_idx_def;
percona$#         INSERT INTO public.idx_recommendations (queryid,query,current_plan,recmnded_index,hypo_plan) 
percona$#         VALUES (l_queries.queryid,l_querytext,l_bef_exp::jsonb,l_idx_def,l_after_exp::jsonb);        
percona$#     END LOOP;    
percona$#         execute 'select hypopg_reset()';
percona$# END;
percona$# $$ LANGUAGE plpgsql;
CREATE FUNCTION</pre><p>From the above log, it has created the function with no errors. Unless we call the above function, we do not know if it has any compilation errors. Surprisingly, with this extension, we can use the <pre class="crayon-plain-tag">plpgsql_check_function_tb()</pre> function to learn whether there are any errors, without actually calling it.</p><pre class="crayon-plain-tag">percona=# SELECT functionid, lineno, statement, sqlstate, message, detail, hint, level, position, 
left (query,60),context FROM plpgsql_check_function_tb('find_usable_indexes()');
-[ RECORD 1 ]------------------------------------------------------------
functionid | find_usable_indexes
lineno     | 14
statement  | FOR over SELECT rows
sqlstate   | 42P01
message    | relation "pg_qualstats_all" does not exist
detail     | 
hint       | 
level      | error
position   | 306
left       | SELECT t.relid, t.relname, t.queryid, t.attnames, t.attnums,
context    |</pre><p>From the above log, it is clear that there is an error where a relation used in the function does not exist. But, if we are using dynamic SQLs that are assembled in runtime, false positives are possible, as you can see in the following example. So, you may avoid the functions using dynamic SQL&#8217;s or comment the code containing those SQLs before checking for other compilation errors.</p><pre class="crayon-plain-tag">percona=# select * from plpgsql_check_function_tb('find_usable_indexes()');
-[ RECORD 1 ]------------------------------------------------------------------------------
functionid | find_usable_indexes
lineno     | 50
statement  | EXECUTE
sqlstate   | 00000
message    | cannot determinate a result of dynamic SQL
detail     | There is a risk of related false alarms.
hint       | Don't use dynamic SQL and record type together, when you would check function.
level      | warning
position   | 
query      | 
context    | 
-[ RECORD 2 ]------------------------------------------------------------------------------
functionid | find_usable_indexes
lineno     | 52
statement  | EXECUTE
sqlstate   | 55000
message    | record "hypo_idx" is not assigned yet
detail     | The tuple structure of a not-yet-assigned record is indeterminate.
hint       | 
level      | error
position   | 
query      | 
context    | SQL statement "SELECT 'select hypopg_get_indexdef('||hypo_idx.indexrelid||')'"</pre><p></p>
<h3>Finding Dependencies</h3>
<p>This extension can be used to find dependent objects in a function. This way, it becomes easy for administrators to understand the objects being used in a function without spending a huge amount of time reading the code. The trick is to simply pass your function as a parameter to <pre class="crayon-plain-tag">plpgsql_show_dependency_tb()</pre> as you see in the following example.</p><pre class="crayon-plain-tag">percona=# select * from plpgsql_show_dependency_tb('find_usable_indexes()');
   type   |  oid  |   schema   |            name            |  params   
----------+-------+------------+----------------------------+-----------
 FUNCTION | 16547 | public     | pg_qualstats               | ()
 FUNCTION | 16545 | public     | pg_qualstats_example_query | (bigint)
 FUNCTION | 16588 | public     | pg_stat_statements         | (boolean)
 RELATION |  2601 | pg_catalog | pg_am                      | 
 RELATION |  2602 | pg_catalog | pg_amop                    | 
 RELATION |  1249 | pg_catalog | pg_attribute               | 
 RELATION |  1262 | pg_catalog | pg_database                | 
 RELATION |  2610 | pg_catalog | pg_index                   | 
 RELATION | 16480 | public     | idx_recommendations        | 
 RELATION | 16549 | public     | pg_qualstats               | 
 RELATION | 16559 | public     | pg_qualstats_all           | 
 RELATION | 16589 | public     | pg_stat_statements         | 
(12 rows)</pre><p></p>
<h4>Profiling Functions</h4>
<p>This is one of the very interesting features. Once you have added the appropriate entries to <pre class="crayon-plain-tag">shared_preload_libraries</pre> as discussed earlier, you could easily enable or disable profiling through a GUC: <pre class="crayon-plain-tag">plpgsql_check.profiler.</pre> This parameter can either be set globally or for only your session. Here&#8217;s an example to understand it better.</p><pre class="crayon-plain-tag">percona=# ALTER SYSTEM SET plpgsql_check.profiler TO 'ON';
ALTER SYSTEM
percona=# select pg_reload_conf();
 pg_reload_conf 
----------------
 t
(1 row)</pre><p>When you set it globally, all the functions running in the database are automatically profiled and stored. This may be fine in a development or a testing environment, but you could also enable profiling of functions called in a single session through a session-level setting as you see in the following example.</p><pre class="crayon-plain-tag">percona=# BEGIN;
BEGIN
percona=# SET plpgsql_check.profiler TO 'ON';
SET
percona=# select salary_update(1000);
 salary_update 
---------------
 
(1 row)

percona=# select lineno, avg_time, source from plpgsql_profiler_function_tb('salary_update(int)');
 lineno | avg_time |                                                    source                                                     
--------+----------+---------------------------------------------------------------------------------------------------------------
      1 |          | 
      2 |          | DECLARE 
      3 |          | l_abc record;
      4 |          | new_sal     INT;
      5 |    0.005 | BEGIN
      6 |   144.96 |   FOR l_abc IN EXECUTE 'SELECT emp_id, salary FROM employee where emp_id between 1 and 10000 and dept_id = 2'
      7 |          |   LOOP
      8 |      NaN |       new_sal := l_abc.salary + sal_raise;
      9 |      NaN |       UPDATE employee SET salary = new_sal WHERE emp_id = l_abc.emp_id;
     10 |          |   END LOOP;
     11 |          | END;
(11 rows)

--- Create an Index and check if it improves the execution time of FOR loop.

percona=# CREATE INDEX idx_1 ON employee (emp_id, dept_id);
CREATE INDEX
percona=# select salary_update(1000);
 salary_update 
---------------
 
(1 row)

percona=# select lineno, avg_time, source from plpgsql_profiler_function_tb('salary_update(int)');
 lineno | avg_time |                                                    source                                                     
--------+----------+---------------------------------------------------------------------------------------------------------------
      1 |          | 
      2 |          | DECLARE 
      3 |          | l_abc record;
      4 |          | new_sal     INT;
      5 |    0.007 | BEGIN
      6 |   73.074 |   FOR l_abc IN EXECUTE 'SELECT emp_id, salary FROM employee where emp_id between 1 and 10000 and dept_id = 2'
      7 |          |   LOOP
      8 |      NaN |       new_sal := l_abc.salary + sal_raise;
      9 |      NaN |       UPDATE employee SET salary = new_sal WHERE emp_id = l_abc.emp_id;
     10 |          |   END LOOP;
     11 |          | END;
(11 rows)

percona=# END;
COMMIT
percona=# show plpgsql_check.profiler;
 plpgsql_check.profiler 
------------------------
 on
(1 row)</pre><p>In the above log, I have opened a new transaction block and enabled the parameter <pre class="crayon-plain-tag">plpgsql_check.profiler</pre> only for that block. So any function that I have executed in that transaction is profiled, which can be extracted using <pre class="crayon-plain-tag">plpgsql_profiler_function_tb()</pre>. Once we have identified the area where the execution time is high, the immediate action is to tune that piece of code. After creating the index, I have called the function again. It has now performed faster than earlier.</p>
<h5>Conclusion</h5>
<p>Special thanks to Pavel <span class="s1">Stehule who has authored this extension and also to the <a target="_blank" href="https://github.com/okbob/plpgsql_check/graphs/contributors" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer">contributors</a> who have put this extension into a usable stage today. This is one of the simplest extensions that can be used to check for compilation errors and dependencies. While this can also be a handy profiling tool, a developer may find both <a target="_blank" href="https://github.com/pgcentral/plprofiler"><span class="lang:sh decode:true crayon-inline">plprofiler</span></a> or <pre class="crayon-plain-tag">plpgsql_check</pre> helpful for profiling as well.</span></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/30/using-plpgsql_check-to-find-compilation-errors-and-profile-functions/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/30/using-plpgsql_check-to-find-compilation-errors-and-profile-functions/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Akira Kurogane</name>
					</author>
		<title type="html"><![CDATA[Network (Transport) Encryption for MongoDB]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/30/network-transport-encryption-for-mongodb/" />
		<id>https://www.percona.com/blog/?p=59562</id>
		<updated>2019-07-30T18:05:18Z</updated>
		<published>2019-07-30T15:11:52Z</published>
		<category scheme="https://www.percona.com/blog" term="MongoDB" /><category scheme="https://www.percona.com/blog" term="Security" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Encryption for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-1024x682.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-367x244.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Why do I need Network encryption? In our previous blog post MongoDB Security vs. Five ‘Bad Guys’ there&#8217;s an overview of five main areas of security functions. Let&#8217;s say you&#8217;ve enabled #1 and #2 (Authentication, Authorization) and #4 (Storage encryption a.k.a. encryption-at-rest and Auditing) mentioned in the previous blog post. Only authenticated users will be [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/30/network-transport-encryption-for-mongodb/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Encryption for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-1024x682.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-367x244.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><h3><span style="font-weight: 400;"><img class="alignright size-thumbnail wp-image-59618" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg" alt="Encryption for MongoDB" width="200" height="133" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-200x133.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-1024x682.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB-367x244.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Encryption-for-MongoDB.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Why do I need Network encryption?</span></h3>
<p><span style="font-weight: 400;">In our previous blog post </span><a target="_blank" href="https://www.percona.com/blog/2019/07/12/mongodb-security-vs-five-bad-guys/"><span style="font-weight: 400;">MongoDB Security vs. Five ‘Bad Guys’</span></a><span style="font-weight: 400;"> there&#8217;s an overview of five main areas of security functions.</span></p>
<p><span style="font-weight: 400;">Let&#8217;s say you&#8217;ve enabled #1 and #2 (Authentication, Authorization) and #4 (Storage encryption a.k.a. encryption-at-rest and Auditing) mentioned in the previous blog post. Only </span>authenticated<span style="font-weight: 400;"> users will be connecting, and they will be only doing what they&#8217;re </span>authorized<span style="font-weight: 400;"> to. With storage encryption configured properly, the database data can&#8217;t be decrypted even if the server&#8217;s disk was stolen or accidentally given away.</span></p>
<p><span style="font-weight: 400;">You will have some pretty tight database servers indeed. However, consider the following movement of user data over the network:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Clients sending updates to the database (to a mongos, or mongod if unsharded).</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">A mongos or mongod sending query results back to a client.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Between replica set members as they replicate to each other.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">mongos nodes retrieving collection data from the shards before relaying it to the user.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Shards with each other if chunks are being moved between sharded collections</span></li>
</ul>
<p><span style="font-weight: 400;">As it moves, the user collection data is no longer within the database &#8216;fortress&#8217;. It&#8217;s riding in plain, unencrypted TCP packets. It can be grabbed from that with tcpdump etc. as shown here:</span></p><pre class="crayon-plain-tag">~$ #mongod primary is running on localhost:28051 is this example.
~$ #
~$ #In a different terminal I run: 
~$ #  mongo --port 28051 -u akira -p secret  --quiet --eval 'db.getSiblingDB("payments").TheImportantCollection.findOne()
~$ 
~$ sudo ngrep -d lo . 'port 28051'
interface: lo (127.0.0.0/255.0.0.0)
filter: ( port 28051 ) and ((ip || ip6) || (vlan &amp;&amp; (ip || ip6)))
match: .
####
...
...
T 127.0.0.1:51632 -&gt; 127.0.0.1:28051 [AP] #19
  ..........................find.....TheImportantCollection..filter.......lim
  it........?.singleBatch...lsid......id........-H..HN.n.`..}{..$clusterTime.
  X....clusterTime...../%9].signature.3....hash.........&gt;.9...(.j. ..H4. .key
  Id.....fK.]...$db.....payments..                                           
#
T 127.0.0.1:28051 -&gt; 127.0.0.1:51632 [AP] #20
  ....4................s....cursor......firstBatch......0......_id..........c
  ustomer.d....fn.....Smith..gn.....Ken..city.....Georgeville..street1.....1 
  Wishful St...postcode.....45031...order_ids.........id..........ns. ...paym
  ents.TheImportantCollection...ok........?.operationTime...../%9].$clusterTi
  me.X....clusterTime...../%9].signature.3....hash.........&gt;.9...(.j. ..H4. .
  keyId.....fK.]...                                                          
#
T 127.0.0.1:51632 -&gt; 127.0.0.1:28051 [AP] #21
  \....................G....endSessions.&amp;....0......id........-H..HN.n.`..}{.
  ..$db.....admin..                                                          
#
T 127.0.0.1:28051 -&gt; 127.0.0.1:51632 [AP] #22
  ....5.....................ok........?.operationTime...../%9].$clusterTime.X
  ....clusterTime...../%9].signature.3....hash.........&gt;.9...(.j. ..H4. .keyI
  d.....fK.]...                                                              
###^Cexit</pre><p><span style="font-weight: 400;">The key names and strings such as customer name and address are </span><b>visible at a glance</b><span style="font-weight: 400;">. This is proof that the TCP data isn&#8217;t encrypted. It is moving around in the plain. (You can use &#8220;</span><a target="_blank" href="https://docs.mongodb.com/manual/reference/program/mongoreplay/#use-monitor"><span style="font-weight: 400;">mongoreplay monitor</span></a><span style="font-weight: 400;">&#8221; if you want to see numeric and other non-ascii-string data in a fully human-readable way.)</span></p>
<p><span style="font-weight: 400;">(If you can unscramble the ascii soup above and see the whole BSON document in your head &#8211; great! But you failed the &#8220;I am not a robot&#8221; test so now you have to stop reading this web page.)</span></p>
<p><span style="font-weight: 400;">For comparison, this is what the same ngrep command prints when I change to using TLS in the client &lt;-&gt; database connection.</span></p><pre class="crayon-plain-tag">~$ #ngrep during: mongo --port 28051 --ssl --sslCAFile /data/tls_certs_etc/root.crt \
~$ #  --sslPEMKeyFile /data/tls_certs_etc/client.foo_app.pem -u akira -p secret --quiet \
~$ #  --eval 'db.getSiblingDB("payments").TheImportantCollection.findOne()'
~$ 
~$ sudo ngrep -d lo . 'port 28051'
interface: lo (127.0.0.0/255.0.0.0)
filter: ( port 28051 ) and ((ip || ip6) || (vlan &amp;&amp; (ip || ip6)))
match: .
####
...
...
T 127.0.0.1:51612 -&gt; 127.0.0.1:28051 [AP] #23
  .........5nYe.).I.M..H.T..j...r".4{.1\.....&gt;...N.Vm.........C..m.V....7.nP.
  f..Z37......}..c?...$.......edN..Qj....$....O[Zb...[...v.....&lt;s.T..m8..u.u3
  R.?....5;...$.F.h...]....@...uq....."..F.M(^.b.....cv.../............\.z..9
  hY........Bz.QEu...`z.W..O@...\.K..C.....N..=.......}.                     
#
T 127.0.0.1:28051 -&gt; 127.0.0.1:51612 [AP] #24
  .....*......4...p.t...G5!.Od...e}.......b.dt.\.xo..^0g.F'.;""..a.....L....#
  DXR.H..)..b.3`.y.vB{@...../..;lOn.k.$7R.]?.M.!H..BC.7........8..k..Rl2.._..
  .pa..-.u...t..;7T8s. z4...Q.....+.Y.\B.............B`.R.(.........~@f..^{.s
  .....\}.D[&amp;..?..m.j#jb.....*.a..`. J?".........Z...J.,....B6............M&gt;.
  ....J....N.H.).!:...B.g2...lua.....5......L9.?.a3....~.G..:...........VB..v
  ........E..[f.S."+...W...A...3...0.G5^.                                    
#
T 127.0.0.1:51612 -&gt; 127.0.0.1:28051 [AP] #25
  ....m..m.5...u...i.H|..L;...M..~#._.v.....7..e...7w.0.......[p..".E=...a.?.
  G{{TS&amp;.........s\..).U.vwV.t...t..2.%..                                    
#
T 127.0.0.1:28051 -&gt; 127.0.0.1:51612 [AP] #26
  .....?..u.*.j...^.LF]6...I.5..5...X.....?..IR(v.T..sX.`....&gt;..Vos.v...z.3d.
  .z.(d.DFs..j.SIA.d]x..s{7..{....n.[n{z.'e'...r............\..#.&lt;&lt;.Y.5.K....
  .....[......[6.....2......[w.5....H                                        
###^Cexit</pre><p>&nbsp;</p>
<p><span style="font-weight: 400;">No more plain data to see! The high-loss ascii format being printed by ngrep above can&#8217;t provide genuine satisfaction that this is perfectly encrypted binary data, but I hope I&#8217;ve demonstrated a quick, useful way to do a &#8216;sanity check&#8217; that you are using TLS and are not still sending data in the plain.</span></p>
<p><span style="font-weight: 400;">Note: I&#8217;ve used ngrep because I found it made the shortest example. If you prefer tcpdump you can capture the dump with tcpdump &lt;interface filter&gt; &lt;bpf filter&gt; -w &lt;dump file&gt;, then open with the Wireshark GUI or view it with tshark -r &lt;dump file&gt; -V on the command line. And for real satisfaction that the TLS traffic is cryptographically protected data, you can print the captured data in hexadecimal / binary format (as opposed to &#8216;ascii&#8217;) and run an entropy assessment on it.</span></p>
<h2><span style="font-weight: 400;">What&#8217;s the risk, really?</span></h2>
<p><span style="font-weight: 400;">It&#8217;s probably a very difficult task for a hypothetical spy who was targeting you 1-to-1 to find and occupy a place in your network where they can just read the TCP traffic as a man-in-the-middle. But wholesale network scanners, who don&#8217;t know or care who any target is beforehand, will find any place that happens to have a gate open on the day they were passing by.</span></p>
<p><span style="font-weight: 400;">The scrambled look of raw TCP data to the human eye is not a distraction to them as it is to you, the DBA or server or application programmer. They&#8217;ve already scripted the unpacking of all the protocols. I assume the technical problem for the blackhat hackers is more a big-data one (too much copied data to process). As an aside, I hypothesize that they are already pioneering a lot of edge-computing techniques.</span></p>
<p><span style="font-weight: 400;">It is true that data captured on the move between servers might be only a tiny fraction of the whole data. But if you are making backups by the dump method once a day, and the traffic between the database server and the backup store server is being listened to, then it would be the full database.</span></p>
<h2><span style="font-weight: 400;">How to enable MongoDB network encryption</span></h2>
<p><span style="font-weight: 400;">MongoDB traffic is not encrypted until you create a set of TLS/SSL certificates and keys, and apply them in the mongod and mongos configuration files of your entire cluster (or non-sharded replica set). If you are an experienced TLS/SSL admin, or you are a DBA who has been given a certificate and key set by security administrators elsewhere in your organization, then I think you will find enabling MongoDB&#8217;s TLS easy &#8211; just distribute the files, reference them in the </span><i><span style="font-weight: 400;">net.ssl.*</span></i><span style="font-weight: 400;"> options, and stop all the nodes and restart them. Gradually enabling without downtime takes longer but is still possible by using rolling restarts changing </span><i><span style="font-weight: 400;">net.ssl.mode</span></i><span style="font-weight: 400;"> from disabled -&gt; allowSSL -&gt; preferSSL -&gt; requireSSL (</span><a target="_blank" href="https://docs.mongodb.com/manual/tutorial/upgrade-cluster-to-ssl/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">doc link</span></a><span style="font-weight: 400;">) in each restart.</span></p>
<p><span style="font-weight: 400;">Conversely, if you are an experienced DBA and it will be your first time creating and distributing TLS certificates and keys, be prepared to spend some time learning about it first.</span></p>
<div class="porto-content-box featured-boxes wpb_content_element "><div class="featured-box  align-left" style="border-radius:3px;"><div class="box-content" style="border-radius:3px;border-top-color:orange;border-top-width:1px;">
<p><span style="font-weight: 400;">Possibly you thought you already enabled network encryption, at least for the internal TCP traffic between mongod and mongos nodes, when you created and deployed the </span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-internal-authentication/#internal-auth-keyfile"><span style="font-weight: 400;">keyfile</span></a><span style="font-weight: 400;"> for the </span><i><span style="font-weight: 400;">security.keyfile</span></i><span style="font-weight: 400;"> option.</span></p>
<p><span style="font-weight: 400;">Admittedly it does look something like an SSH private key, but it is not. The long string value you made by extracting from /dev/random per the tutorial is just a string, it&#8217;s not part of asymmetric cryptography key where one binary value can be mathematically confirmed to be have been encrypted or signed by another.</span></p>
<p><span style="font-weight: 400;">In short, it is a misunderstanding &#8211; it is not the </span><i><span style="font-weight: 400;">net.ssl.PEMKeyFile</span></i><span style="font-weight: 400;"> option or a key like it. The </span><i><span style="font-weight: 400;">security.keyfile</span></i><span style="font-weight: 400;"> file just holds a password (like a user password) that the nodes use to connect as the internal &#8220;__system&#8221; user to each other. Setting it hasn&#8217;t enabled TLS/SSL. For example from a secondary&#8217;s server, or some hypothetical network node between it and the primary, you could listen to every write op as it comes through the oplog replication process.</span></p>
<p><span style="font-weight: 400;">The first ngrep network sniffing example in this article was from a replicaset with </span><i><span style="font-weight: 400;">security.keyfile</span></i><span style="font-weight: 400;"> option set whilst </span><i><span style="font-weight: 400;">net.ssl.*</span></i><span style="font-weight: 400;"> options were unset.</span></p>
<p></div></div></div>
<p><span style="font-weight: 400;">The way the certificates and PEM key files are created varies according to the following choices:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Using an external certificate authority or making a new root certificate just for these MongoDB clusters</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">If you are using it just for the internal system authentication between mongod and mongos nodes, or if you are enabling TLS for clients too</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">How strict you will be making certificates (e.g. with host limitations)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Whether you need the ability to revoke certificates</span></li>
</ul>
<p><i><span style="font-weight: 400;">To repeat the first point in this section: if you have a security administration team who already know and control these public key infrastructure (PKI) components &#8211; ask them for help, in the interests of saving time and being more certain you&#8217;re getting certificates that conform with internal policy.</span></i></p>
<h2><span style="font-weight: 400;">Self-made test certificates</span></h2>
<p style="padding-left: 40px;"><span style="color: #ff0000;"><b><span style="text-decoration: underline;">Percona Security Team note</span>:</b> <i><span style="font-weight: 400;">This is not a best practice, even though it is in the documentation as a </span></i><span style="font-weight: 400;">tutorial</span><i><span style="font-weight: 400;">; we recommend you do not use this in production deployments</span></i><span style="font-weight: 400;">.</span></span></p>
<p><span style="font-weight: 400;">So you want to get hands-on with TLS configuration of MongoDB a.s.a.p.? You&#8217;ll need certificates and PEM key files. Having the patience to fully master certificate administration would be a virtue, but you are not that virtuous. So you are going to use the existing tutorials (links below) to create self-signed certificates.</span></p>
<p><span style="font-weight: 400;">The quickest way to create certificates is:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Make a new root certificate</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Generate server certificates (i.e. the ones the mongod and mongos nodes use for </span><i><span style="font-weight: 400;">net.ssl.PEMKeyFile</span></i><span style="font-weight: 400;">) from that root certificate</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Generate client certificates from the new root certificate too</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Skip setting CN / &#8220;subject&#8221; fields that limit the hosts or domains the client certificate can be used on</span></li>
</ul>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Self-sign those certificates</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Skip making revocation certificates</span></li>
</ul>
<p><span style="font-weight: 400;">The weakness in these certificates is:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">A man in the middle attack is possible (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-transport-encryption/#certificates"><span style="font-weight: 400;">MongoDB doc link</span></a><span style="font-weight: 400;">):</span><span style="font-weight: 400;"><br />
</span><i><span style="font-weight: 400;">&#8220;MongoDB can use any valid TLS/SSL certificate issued by a certificate authority or a self-signed certificate. If you use a self-signed certificate, although the communications channel will be encrypted, there will be no validation of server identity. Although such a situation will prevent eavesdropping on the connection, it leaves you vulnerable to a man-in-the-middle attack. Using a certificate signed by a trusted certificate authority will permit MongoDB drivers to verify the server’s identity.&#8221;</span></i></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">What will happen if someone gets a copy of one of them?</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">If they get the client or a server certificate they will be able to decrypt or spoof being a  SSL encrypting-and-decrypting network peer on the network edges to those nodes.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">When using self-signed certificates you distribute a copy of the root certificate with the server or client certificate to every mongod, mongos, and client app. I.e. it&#8217;s as likely to be misplaced or stolen as a single client or server certificate. With the root certificate spoofing can be done on any edge in the network.</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">You can&#8217;t revoke a stolen client or server certificate and cut them off from further access. You&#8217;re stuck with it. You&#8217;ll have to completely replace all the server-side and client certificates with cluster-wide downtime (at least for MongoDB &lt; 4.2).</span></li>
</ul>
</li>
</ul>
<h3><span style="font-weight: 400;">Examples on how to make self-signed certificates:</span></h3>
<ul>
<li><a target="_blank" href="https://gist.github.com/kevinadi/96090f6f9973ff8c2d019bbe0d9a0f70" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">This snippet</span></a><span style="font-weight: 400;"> from MongoDB&#8217;s Kevin Adistimba is the most concise I&#8217;ve seen.</span></li>
<li><a target="_blank" href="https://www.percona.com/blog/2018/05/31/mongodb-deploy-replica-set-with-transport-encryption-part-3/"><span style="font-weight: 400;">This replicaset setup tutorial</span></a><span style="font-weight: 400;"> for Percona&#8217;s Corrado Pandiani includes similar instructions with more mongodb context on the page.</span></li>
</ul>
<h2><span style="font-weight: 400;">Reference in the MongoDB docs:</span></h2>
<p><span style="font-weight: 400;">Various </span><a target="_blank" href="https://docs.mongodb.com/manual/tutorial/configure-ssl/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">configuration file examples</span></a></p>
<p><span style="font-weight: 400;">Three detailed appendix entries on how to make </span><a target="_blank" href="https://docs.mongodb.com/manual/appendix/security/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">OpenSSL Certificates for Testing</span></a><span style="font-weight: 400;">.</span></p>
<h2><span style="font-weight: 400;">Troubleshooting</span></h2>
<p><span style="font-weight: 400;">I like the brevity of the SSL Troubleshooting page in Gabriel Ciciliani&#8217;s </span><a target="_blank" href="https://www.percona.com/live/e18/sites/default/files/slides/MongoDB%20administration%20cool%20tips%20-%20FileId%20-%20160529.pdf"><span style="font-weight: 400;">MongoDB administration cool tips</span></a><span style="font-weight: 400;"> presentation from Percona Live Europe &#8217;18. Speaking from my own experience before enabling them in the MongoDB config it&#8217;s crucial to make sure the PEM files (both server and client ones) pass the &#8216;openssl verify&#8217; test command against the root / CA certificate they&#8217;re derived from. Absolutely, 100% do this before trying to use them in your mongodb config.</span></p>
<p><span style="font-weight: 400;">If &#8220;<code>openssl verify</code>&#8220;-confirmed certificates still create a mongodb replicaset or cluster that is unconnectable then add the <code>--sslAllowInvalidHostnames</code> option when connecting with the mongo shell, and/or </span><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/#net.ssl.allowInvalidHostnames" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">net.ssl.allowInvalidHostnames</span></a><span style="font-weight: 400;"> in mongod/mongos configuration. This is a differential diagnosis to see if the hostname requirements of the certificates are the only thing causing the SSL rules to reject the certificates.</span></p>
<p><span style="font-weight: 400;">If you find it takes <code>--sslAllowInvalidHostnames</code> to make it work it means the CN subject field and/or SAN fields in the certificate need to be edited until they match the hostnames and domains that the SSL lib identifies the hosts as. Don&#8217;t be tempted to just conveniently forget about it; disabling hostname verification is a gap that might be leveraged into a man-in-the-middle attack.</span></p>
<p><span style="font-weight: 400;">If you still are experiencing trouble my next step would be to check the mongod logs. You will find lines matching the grep expression &#8216;NETWORK .*SSL&#8217; in the log if there are rejections. (This might become &#8220;TLS&#8221; later.) E.g.</span></p>
<p style="padding-left: 40px;"><span style="font-weight: 400;"><code>2019-07-25T16:34:49.981+0900 I NETWORK  [conn11] Error receiving request from client: SSLHandshakeFailed: SSL peer certificate validation failed: self signed certificate in certificate chain. Ending connection from 127.0.0.1:33456 (connection id: 11)</code></span></p>
<p><span style="font-weight: 400;">You might also try grepping for <code>'[EW] NETWORK'</code> to look for all network errors and warnings.</span></p>
<p><span style="font-weight: 400;">For SSL there is no need to raise the logging verbosity to see errors and warnings. From what I can see in </span><a target="_blank" href="https://github.com/mongodb/mongo/blob/master/src/mongo/util/net/ssl_manager_openssl.cpp" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">ssl_manager_openssl.cpp</span></a><span style="font-weight: 400;"> those all come at the default log verbosity of 0. Only if you want to confirm normal, successful connections would I advise briefly </span><a target="_blank" href="https://docs.mongodb.com/manual/reference/parameters/#param.logComponentVerbosity" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">raising log verbosity in the config file</span></a><span style="font-weight: 400;"> to level 2 for the exact log &#8216;component&#8217; (in this case this is the &#8220;network&#8221;). (Don&#8217;t forget to turn it off soon after &#8211; forgetting you set log level 2 is a great way to fill your disk.) But for this topic the only thing I think log level 2 will add is &#8220;Accepted TLS connection from peer&#8221; </span><i><span style="font-weight: 400;">confirmations</span></i><span style="font-weight: 400;"> like the following. </span></p>
<p style="padding-left: 40px;"><span style="font-weight: 400;"><code>2019-07-25T16:29:41.779+0900 D NETWORK  [conn18] Accepted TLS connection from peer: emailAddress=akira.kurogane@nowhere.com,CN=svrA80v,OU=testmongocluster,O=MongoTestCorp,L=Tokyo,ST=Tokyo,C=JP</code></span></p>
<h2><span style="font-weight: 400;">Take a peek in the code</span></h2>
<p><span style="font-weight: 400;">Certificate acceptance rules are a big topic and I am not the author to cover it. But take a look at the SSLManagerOpenSSL::parseAndValidatePeerCertificate(&#8230;) function in </span><a target="_blank" href="https://github.com/mongodb/mongo/blob/master/src/mongo/util/net/ssl_manager_openssl.cpp" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">ssl_manager_openssl.cpp</span></a><span style="font-weight: 400;"> as a starting point if you&#8217;d like to be a bit more familiar with MongoDB&#8217;s application.</span></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/30/network-transport-encryption-for-mongodb/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/30/network-transport-encryption-for-mongodb/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Ibrar Ahmed</name>
					</author>
		<title type="html"><![CDATA[Parallelism in PostgreSQL]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/30/parallelism-in-postgresql/" />
		<id>https://www.percona.com/blog/?p=59225</id>
		<updated>2019-07-30T19:12:21Z</updated>
		<published>2019-07-30T13:31:52Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="136" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x136.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Parallelism in PostgreSQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x136.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-300x205.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-1024x698.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-367x250.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />PostgreSQL is one of the finest object-relational databases, and its architecture is process-based instead of thread-based. While almost all the current database systems utilize threads for parallelism, PostgreSQL&#8217;s process-based architecture was implemented prior to POSIX threads. PostgreSQL launches a process “postmaster” on startup, and after that spans new process whenever a new client connects to [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/30/parallelism-in-postgresql/"><![CDATA[<img width="200" height="136" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x136.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Parallelism in PostgreSQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x136.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-300x205.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-1024x698.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-367x250.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-weight: 400;"><img class="alignright wp-image-59606 size-thumbnail" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x127.png" alt="Parallelism in PostgreSQL" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Parallelisms-in-PostgreSQL.png 300w" sizes="(max-width: 200px) 100vw, 200px" />PostgreSQL is one of the finest object-relational databases, and its architecture is process-based instead of thread-based. While almost all the current database systems utilize threads for parallelism, PostgreSQL&#8217;s process-based architecture was implemented prior to POSIX threads. PostgreSQL launches a process “postmaster” on startup, and after that spans new process whenever a new client connects to the PostgreSQL. </span></p>
<p><span style="font-weight: 400;">Before version 10 there was no parallelism in a single connection. It is true that multiple queries from the different clients can have parallelism because of process architecture, but they couldn’t gain any performance benefit from one another. In other words, a single query runs serially and did not have parallelism. This is a huge limitation because a single query cannot utilize the multi-core. Parallelism in PostgreSQL was introduced from version 9.6. Parallelism, in a sense, is where a single process can have multiple threads to query the system and utilize the multicore in a system. This gives PostgreSQL intra-query parallelism. </span></p>
<p><span style="font-weight: 400;">Parallelism in PostgreSQL was implemented as part of multiple features which cover sequential scans, aggregates, and joins.</span></p>
<h2>Components of Parallelism in PostgreSQL</h2>
<p><span style="font-weight: 400;">There are three important components of parallelism in PostgreSQL. These are the process itself, gather, and workers. Without parallelism the process itself handles all the data, however, when planner decides that a query or part of it can be parallelized, it adds a Gather node within the parallelizable portion of the plan and makes a gather root node of that subtree.  </span><span style="font-weight: 400;">Query execution starts at the process (leader) level and all the serial parts of the plan are run by the leader. However, if parallelism is enabled and permissible for any part (or whole) of the query, then gather node with a set of workers is allocated for it. </span><span style="font-weight: 400;">Workers are the threads that run in parallel with part of the tree (partial-plan) that needs to be parallelized. The relation&#8217;s blocks are divided amongst threads such that the relation remains sequential. The number of threads is governed by settings as set in PostgreSQL’s configuration file. The workers coordinate/communicate using shared memory, and o</span><span style="font-weight: 400;">nce workers have completed their work, the results are passed on to the leader for accumulation.</span></p>
<p><img class="aligncenter size-full wp-image-59484" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Process-Worker-Seq-Scan-4.png" alt="" width="831" height="621" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Process-Worker-Seq-Scan-4.png 831w, https://www.percona.com/blog/wp-content/uploads/2019/07/Process-Worker-Seq-Scan-4-200x150.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Process-Worker-Seq-Scan-4-300x224.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Process-Worker-Seq-Scan-4-367x274.png 367w" sizes="(max-width: 831px) 100vw, 831px" /></p>
<h3>Parallel Sequential Scans</h3>
<p><span style="font-weight: 400;">In PostgreSQL 9.6, support for the parallel sequential scan was added. A sequential scan is a scan on a table in which a sequence of blocks is evaluated one after the other. This, by its very nature, allows parallelism. So this was a natural candidate for the first implementation of parallelism. </span><span style="font-weight: 400;">In this, the whole table is sequentially scanned in multiple worker threads. Here is the simple query where we query the pgbench_accounts table rows (</span><b>63165</b><span style="font-weight: 400;">) which has </span><b>1500000000</b><span style="font-weight: 400;"> tuples. The total execution time is </span><b>4343080ms</b><span style="font-weight: 400;">. As there is no index defined, the sequential scan is used. The whole table is scanned in a single process with no thread. Therefore the single core of CPU is used regardless of how many cores are available. </span></p><pre class="crayon-plain-tag">db=# EXPLAIN ANALYZE SELECT * 
            FROM pgbench_accounts 
            WHERE abalance &gt; 0;
                             QUERY PLAN
----------------------------------------------------------------------
 Seq Scan on pgbench_accounts (cost=0.00..73708261.04 rows=1 width=97)
                (actual time=6868.238..4343052.233 rows=63165 loops=1)
   Filter: (abalance &gt; 0)
   Rows Removed by Filter: 1499936835
 Planning Time: 1.155 ms
 Execution Time: 4343080.557 ms
(5 rows)</pre><p><span style="font-weight: 400;">What if these</span><b> 1,500,000,000</b><span style="font-weight: 400;"> rows scanned parallel using “10” workers within a process? It will reduce the execution time drastically. </span></p><pre class="crayon-plain-tag">db=# EXPLAIN ANALYZE select * from pgbench_accounts where abalance &gt; 0;
                             QUERY PLAN                                                                   
---------------------------------------------------------------------- 
Gather  (cost=1000.00..45010087.20 rows=1 width=97) 
        (actual time=14356.160..1628287.828 rows=63165 loops=1)
   Workers Planned: 10
   Workers Launched: 10
   -&gt;  Parallel Seq Scan on pgbench_accounts  
              (cost=0.00..45009087.10 rows=1 width=97)
              (actual time=43694.076..1628068.096 rows=5742 loops=11)
   Filter: (abalance &gt; 0)
   Rows Removed by Filter: 136357894
Planning Time: 37.714 ms
Execution Time: 1628295.442 ms
(8 rows)</pre><p><span style="font-weight: 400;">Now the total execution time is </span><b>1628295ms; </b>this is a <span style="font-weight: 400;">266% improvement while using 10 workers thread used to scan. </span></p>
<p>&nbsp;</p>
<p><img class="aligncenter size-large wp-image-59635" src="https://www.percona.com/blog/wp-content/uploads/2019/07/WxQ-4-1024x342.png" alt="" width="900" height="301" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/WxQ-4-1024x342.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxQ-4-200x67.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxQ-4-300x100.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxQ-4-367x122.png 367w" sizes="(max-width: 900px) 100vw, 900px" /></p>
<p><strong>Query used for the Benchmark</strong>:  <span class="s1">SELECT * FROM pgbench_accounts WHERE abalance &gt; 0;</span></p>
<p><strong>Size of Table</strong>: <span class="s1">426GB</span></p>
<p><strong>Total Rows in Table: </strong>1500000000</p>
<p><strong>The system used for the Benchmark</strong>:</p>
<p class="p1"><span class="s1"><strong>    CPU</strong>: 2 Intel(R) Xeon(R) CPU E5-2643 v2 @ 3.50GHz</span></p>
<p><strong>    RAM</strong>: 256GB DDR3 1600</p>
<p><strong>    DISK</strong>: ST3000NM0033</p>
<p><span style="font-weight: 400;">The above graph clearly shows how parallelism improves performance for a sequential scan. When a single worker is added, the performance understandably degrades as no parallelism is gained, but the creation of an additional gather node and a single work adds overhead. However, with more than one worker thread, the performance improves significantly. Also, it is important to note that performance doesn’t increase in a linear or exponential fashion. It improves gradually until the addition of more workers will not give any performance boost; sort of like approaching a horizontal asymptote. This benchmark was performed on a 64-core machine, and it is clear that having more than 10 workers will not give any significant performance boost.</span></p>
<h3>Parallel Aggregates</h3>
<p><span style="font-weight: 400;">In databases, calculating aggregates are very expensive operations. When evaluated in a single process, these take a reasonably long time. In PostgreSQL 9.6, the ability to calculate these in parallel was added by simply dividing these in chunks (a divide and conquer strategy). This allowed multiple workers to calculate the part of aggregate before the final value(s) based on these calculations was calculated by the leader. More technically speaking, PartialAggregate nodes are added to a plan tree, and each PartialAggregate node takes the output from one worker. These outputs are then emitted to a FinalizeAggregate node that combines the aggregates from multiple (all) PartialAggregate nodes. So effectively, the parallel partial plan includes a FinalizeAggregate node at the root and a Gather node which will have PartialAggregate nodes as children.</span></p>
<p>&nbsp;</p>
<p><img class="aligncenter size-full wp-image-59487" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Aggr.png" alt="" width="851" height="321" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Aggr.png 851w, https://www.percona.com/blog/wp-content/uploads/2019/07/Aggr-200x75.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Aggr-300x113.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Aggr-367x138.png 367w" sizes="(max-width: 851px) 100vw, 851px" /></p><pre class="crayon-plain-tag">db=# EXPLAIN ANALYZE SELECT count(*) from pgbench_accounts;
                               QUERY PLAN                                                                   
----------------------------------------------------------------------
 Aggregate  (cost=73708261.04..73708261.05 rows=1 width=8) 
            (actual time=2025408.357..2025408.358 rows=1 loops=1)
   -&gt;  Seq Scan on pgbench_accounts  (cost=0.00..67330666.83 rows=2551037683 width=0) 
                                     (actual time=8.162..1963979.618 rows=1500000000 loops=1)
 Planning Time: 54.295 ms
 Execution Time: 2025419.744 ms
(4 rows)</pre><p><span style="font-weight: 400;">Following is an example of a plan when an aggregate is to be evaluated in parallel.  You can clearly see performance improvement here.</span></p><pre class="crayon-plain-tag">db=# EXPLAIN ANALYZE SELECT count(*) from pgbench_accounts;
                           QUERY PLAN                                                                 
---------------------------------------------------------------------- 
Finalize Aggregate  (cost=45010088.14..45010088.15 rows=1 width=8)
                 (actual time=1737802.625..1737802.625 rows=1 loops=1)
   -&gt;  Gather  (cost=45010087.10..45010088.11 rows=10 width=8) 
               (actual time=1737791.426..1737808.572 rows=11 loops=1)
         Workers Planned: 10
         Workers Launched: 10
         -&gt;  Partial Aggregate  
             (cost=45009087.10..45009087.11 rows=1 width=8) 
             (actual time=1737752.333..1737752.334 rows=1 loops=11)
             -&gt;  Parallel Seq Scan on pgbench_accounts
                (cost=0.00..44371327.68 rows=255103768 width=0)
              (actual time=7.037..1731083.005 rows=136363636 loops=11)
 Planning Time: 46.031 ms
 Execution Time: 1737817.346 ms
(8 rows)</pre><p><span style="font-weight: 400;">With parallel aggregates, in this particular case, we get a performance boost of just over 16% as the execution time of </span><b>2025419.744</b><span style="font-weight: 400;"> is reduced to </span><b>1737817.346</b><span style="font-weight: 400;"> when 10 parallel workers are involved</span><b>.  </b></p>
<p><img class="aligncenter size-large wp-image-59634" src="https://www.percona.com/blog/wp-content/uploads/2019/07/WxA-4-1024x316.png" alt="" width="900" height="278" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/WxA-4-1024x316.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxA-4-200x62.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxA-4-300x93.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/WxA-4-367x113.png 367w" sizes="(max-width: 900px) 100vw, 900px" /></p>
<p><strong>Query used for the Benchmark</strong>:  <span class="s1">SELECT <strong>count(*)</strong> FROM pgbench_accounts WHERE abalance &gt; 0;</span></p>
<p><strong>Size of Table</strong>: <span class="s1">426GB</span></p>
<p><strong>Total Rows in Table: </strong>1500000000</p>
<p><strong>The system used for the Benchmark</strong>:</p>
<p class="p1"><span class="s1"><strong>    CPU</strong>: 2 Intel(R) Xeon(R) CPU E5-2643 v2 @ 3.50GHz</span></p>
<p><strong>    RAM</strong>: 256GB DDR3 1600</p>
<p><strong>    DISK</strong>: ST3000NM0033</p>
<h3>Parallel Index (B-Tree) Scans</h3>
<p><span style="font-weight: 400;">The parallel support for B-Tree index means index pages are scanned in parallel. The B-Tree index is one of the most used indexes in PostgreSQL. In a parallel version of B-Tree, a worker scans the B-Tree and when it reaches its leaf node, it then scans the block and triggers the blocked waiting worker to scan the next block. </span></p>
<p><span style="font-weight: 400;">Confused? Let&#8217;s look at an example of this. Suppose we have a table foo with id and name columns, with 18 rows of data. We create an index on the id column of table foo. A system column CTID is attached with each row of table which identifies the physical location of the row. There are two values in the CTID column: the block number and the offset. </span></p><pre class="crayon-plain-tag">postgres=# <strong>SELECT</strong> ctid, id <strong>FROM</strong> foo;
  ctid  | id  
--------+-----
 (0,55) | 200
 (0,56) | 300
 (0,57) | 210
 (0,58) | 220
 (0,59) | 230
 (0,60) | 203
 (0,61) | 204
 (0,62) | 300
 (0,63) | 301
 (0,64) | 302
 (0,65) | 301
 (0,66) | 302
 (1,31) | 100
 (1,32) | 101
 (1,33) | 102
 (1,34) | 103
 (1,35) | 104
 (1,36) | 105
(18 rows)</pre><p><span style="font-weight: 400;">Let&#8217;s create the B-Tree index on that table’s id column.  </span></p><pre class="crayon-plain-tag">CREATE INDEX foo_idx ON foo(id)</pre><p><img class="aligncenter size-large wp-image-59489" src="https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2-1024x502.png" alt="" width="900" height="441" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2-1024x502.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2-200x98.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2-300x147.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2-367x180.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/BTree-2.png 1048w" sizes="(max-width: 900px) 100vw, 900px" /></p>
<p><span style="font-weight: 400;">Suppose we want to select values where id &lt;= 200 with 2 workers. Worker-0 will start from the root node and scan until the leaf node 200. It’ll handover the next block under node 105 to Worker-1, which is in a blocked and wait-state.  If there are other workers, blocks are divided into the workers. A similar pattern is repeated until the scan is completed.</span></p>
<p><b>Parallel Bitmap Scans</b></p>
<p><span style="font-weight: 400;">To parallelize a bitmap heap scan, we need to be able to divide blocks among workers in a way very similar to parallel sequential scan. To do that, a scan on one or more indexes is done and a bitmap indicating which blocks are to be visited is created. This is done by a leader process, i.e. this part of the scan is run sequentially. However, the parallelism kicks in when the identified blocks are passed to workers, the same way as in a parallel sequential scan.</span></p>
<p><b>Parallel Joins</b></p>
<p><span style="font-weight: 400;">Parallelism in the merge joins support is also one of the hottest features added in this release. In this, a table joins with other tables&#8217; inner loop hash or merge. In any case, there is no parallelism supported in the inner loop. The entire loop is scanned as a whole, and the parallelism occurs when each worker executes the inner loop as a whole. The results of each join sent to gather accumulate and produce the final results. </span></p>
<p><img class="aligncenter size-full wp-image-59491" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Joins.png" alt="" width="791" height="391" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Joins.png 791w, https://www.percona.com/blog/wp-content/uploads/2019/07/Joins-200x99.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Joins-300x148.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Joins-367x181.png 367w" sizes="(max-width: 791px) 100vw, 791px" /></p>
<h3><strong>Summary</strong></h3>
<p><span style="font-weight: 400;">It is obvious from what we’ve already discussed in this blog that parallelism gives significant performance boosts for some, slight gains for others, and may cause performance degradation in some cases. Ensure that parallel_setup_cost or parallel_tuple_cost are set up correctly to enable the query planner to choose a parallel plan. Even after setting low values for these GUIs, if a parallel plan is not produced, refer to the PostgreSQL documentation on parallelism for details. </span></p>
<p><span style="font-weight: 400;">For a parallel plan, you can get per-worker statistics for each plan node to understand how the load is distributed amongst workers. You can do that through EXPLAIN (ANALYZE, VERBOSE). </span><span style="font-weight: 400;">As with any other performance feature, there is no one rule that applies to all workloads. Parallelism should be carefully configured for whatever the need may be, and you must ensure that the probability of gaining performance is significantly higher than the probability of a drop in performance.</span></p>
<p><a target="_blank" href="https://news.ycombinator.com/item?id=20568134" target="_blank" rel="&quot;noopener&quot; noopener noreferrer"><em>Discuss on Hacker News</em></a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/30/parallelism-in-postgresql/#comments" thr:count="8"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/30/parallelism-in-postgresql/feed/atom/" thr:count="8"/>
		<thr:total>8</thr:total>
			</entry>
		<entry>
		<author>
			<name>David Bennett</name>
						<uri>http://www.percona.com/about-us/our-team/david-bennett</uri>
						</author>
		<title type="html"><![CDATA[Debian 10 &#8220;Buster&#8221; Packages Available for Percona Products]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/26/debian-10-buster-packages-available-for-percona-products/" />
		<id>https://www.percona.com/blog/?p=59428</id>
		<updated>2019-07-26T15:15:39Z</updated>
		<published>2019-07-26T13:31:30Z</published>
		<category scheme="https://www.percona.com/blog" term="Percona Software" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Debian 10 &quot;Buster&quot;" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />Debian 10 &#8220;Buster&#8221; packages have been released into the Percona packaging repositories.  Recent versions of Percona Server for MySQL, Percona XtraDB Cluster, Percona XtraBackup, Percona Server for MongoDB, Percona Toolkit, and the PMM Client can now be automatically installed via the Debian 10 apt command. Global Debian 10 &#8220;Buster&#8221; Setup: [crayon-5d433ac36e897567835383/] Percona Server 5.7, Percona [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/26/debian-10-buster-packages-available-for-percona-products/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Debian 10 &quot;Buster&quot;" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59531" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg" alt="Debian 10 &quot;Buster&quot;" width="200" height="112" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Debian.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />Debian 10 &#8220;Buster&#8221; packages have been released into the <a target="_blank" href="https://www.percona.com/doc/percona-repo-config/index.html">Percona packaging repositories</a>.  Recent versions of <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-server">Percona Server for MySQL</a>, <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-xtradb-cluster">Percona XtraDB Cluster</a>, <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-xtrabackup">Percona XtraBackup</a>, <a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Percona Server for MongoDB</a>, <a target="_blank" href="https://www.percona.com/software/database-tools/percona-toolkit">Percona Toolkit</a>, and the <a target="_blank" href="https://www.percona.com/software/database-tools/percona-monitoring-and-management">PMM Client</a> can now be automatically installed via the Debian 10 apt command.</p>
<h2>Global Debian 10 &#8220;Buster&#8221; Setup:</h2>
<p></p><pre class="crayon-plain-tag">$ sudo apt-get update
$ sudo apt-get install -y wget gnupg2 lsb-release
$ wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
$ sudo dpkg -i percona-release_latest.generic_all.deb</pre><p></p>
<h2>Percona Server 5.7, Percona XtraBackup 2.4, Percona Toolkit 3.0</h2>
<p></p><pre class="crayon-plain-tag">$ sudo percona-release setup ps57
$ sudo apt-get install -y percona-server-server-5.7 percona-server-client-5.7 percona-xtrabackup-24 percona-toolkit</pre><p></p>
<h2>Percona XtraDB Cluster 5.7, Percona XtraBackup 2.4, and Percona Toolkit:</h2>
<p></p><pre class="crayon-plain-tag">$ sudo percona-release setup pxc57
$ sudo apt-get install -y percona-xtradb-cluster-57 percona-toolkit</pre><p></p>
<h2>Percona Server 8.0, Percona XtraBackup 8.0 and Percona Toolkit 3.0:</h2>
<p></p><pre class="crayon-plain-tag">$ sudo percona-release setup ps80
$ sudo apt-get install -y percona-server-server percona-server-client percona-xtrabackup-80 percona-toolkit</pre><p></p>
<h2>Percona Server for MongoDB 4.0 with Percona Toolkit 3.0:</h2>
<p></p><pre class="crayon-plain-tag">$ sudo percona-release setup psmdb40
$ sudo apt-get install -y percona-server-mongodb percona-toolkit</pre><p></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/26/debian-10-buster-packages-available-for-percona-products/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/26/debian-10-buster-packages-available-for-percona-products/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Emily Ikuta</name>
					</author>
		<title type="html"><![CDATA[Upcoming Webinar 7/25: Enhancing MySQL Security]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/24/upcoming-webinar-enhancing-mysql-security/" />
		<id>https://www.percona.com/blog/?p=59447</id>
		<updated>2019-07-24T20:55:07Z</updated>
		<published>2019-07-24T14:51:30Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Technical Webinars" /><category scheme="https://www.percona.com/blog" term="webinar" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Enhancing MySQL Security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Join Percona Support Engineer Vinicius Grippa as he presents his talk “Enhancing MySQL Security” on Thursday, July 25th, 2019 at 6:00 AM PDT (UTC-7) / 9:00 AM EDT (UTC-4). Register Now Security is always a challenge when its comes to data. Moreso, regulations like GDPR add a whole new layer on top of it, with [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/24/upcoming-webinar-enhancing-mysql-security/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Enhancing MySQL Security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Enhancing-MySQL-Security.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p>Join Percona Support Engineer Vinicius Grippa as he presents his talk “Enhancing MySQL Security” on Thursday, July 25th, 2019 at 6:00 AM PDT (UTC-7) / 9:00 AM EDT (UTC-4).</p>
<p style="text-align: center;"><a target="_blank" class="btn btn-primary btn-lg" href="https://www.percona.com/resources/webinars/enhancing-mysql-security-0" rel="noopener">Register Now</a></p>
<div class="row">
<div class="col-sm-12">
<div class="field field-name-body field-type-text-with-summary field-label-hidden">
<div class="field-items">
<div class="field-item even">
<p>Security is always a challenge when its comes to data. Moreso, regulations like GDPR add a whole new layer on top of it, with rules more and more restrictive to access and manipulate data. Join us in this presentation to check security best practices as well as traditional and new features available for MySQL, including features coming with the new MySQL 8.</p>
<p>In this talk, DBAs and sysadmins will walk through the security features available on the OS and MySQL. These features include:</p>
<ul>
<li>SO security</li>
<li>SSL</li>
<li>ACL</li>
<li>TDE</li>
<li>Audit Plugin</li>
<li>MySQL 8 features (undo, redo, and binlog encryption)</li>
<li>New caching_sha2_password</li>
<li>Roles</li>
<li>Password Management</li>
<li>FIPS mode</li>
</ul>
<p>We will also share our experience of working with 2,000 support customers and help the audience to become familiar with all the security concepts and methods. Lastly, we&#8217;ll give you the necessary knowledge to apply this information to your environment.</p>
</div>
</div>
</div>
</div>
</div>
<p><a target="_blank" href="https://www.percona.com/resources/webinars/enhancing-mysql-security-0">If you can&#8217;t attend, sign up anyways we&#8217;ll send you the slides and recording afterward</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/24/upcoming-webinar-enhancing-mysql-security/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/24/upcoming-webinar-enhancing-mysql-security/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>marcos.albe</name>
						<uri>http://www.percona.com</uri>
						</author>
		<title type="html"><![CDATA[Connect to MySQL after hitting ERROR 1040: Too many connections]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/23/connect-to-mysql-after-hitting-error-1040-too-many-connections/" />
		<id>https://www.percona.com/blog/?p=58956</id>
		<updated>2019-08-01T19:10:03Z</updated>
		<published>2019-07-23T19:43:57Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="115" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="ERROR 1040" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-300x173.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-1024x591.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-367x212.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />ERROR 1040&#8230;again A pretty common topic in Support tickets is the rather infamous error: ERROR 1040: Too many connections. The issue is pretty self-explanatory: your application/users are trying to create more connections than the server allows, or in other words, the current number of connections exceeds the value of the max_connections variable. This situation on [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/23/connect-to-mysql-after-hitting-error-1040-too-many-connections/"><![CDATA[<img width="200" height="115" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="ERROR 1040" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-300x173.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-1024x591.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-367x212.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><h3><img class="alignright wp-image-59439 size-thumbnail" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg" alt="ERROR 1040: Too many connections" width="200" height="115" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-200x115.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-300x173.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-1024x591.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections-367x212.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ERROR-1040-Too-many-connections.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />ERROR 1040&#8230;again</h3>
<p>A pretty common topic in Support tickets is the rather infamous error: <code>ERROR 1040: Too many connections</code>. The issue is pretty self-explanatory: your application/users are trying to create more connections than the server allows, or in other words, the current number of connections exceeds the value of the <code>max_connections</code> variable.</p>
<p>This situation on its own is already a problem for your end-users, but when on top of that you are not able to access the server to diagnose and correct the root cause, then you have a really big problem; most times you will need to terminate the instance and restart it to recover.</p>
<h3>Root user can&#8217;t connect either! Why!?</h3>
<p>In a properly set up environment, a user with <code>SUPER</code> privilege will be able to access the instance and diagnose the error 1040 problem that is causing connection starvation, as explained in the manual:</p>
<blockquote><p>mysqld actually permits <code>max_connections</code> + 1 client connections. The extra connection is reserved for use by accounts that have the <code>SUPER</code> privilege. By granting the privilege to administrators and not to normal users (who should not need it), an administrator who also has the <code>PROCESS</code> privilege can connect to the server and use <code>SHOW PROCESSLIST</code> to diagnose problems even if the maximum number of unprivileged clients are connected.</p></blockquote>
<p>But we see <strong>lots</strong> of people who give <code>SUPER</code> privileges to their application or script users, either due to application requirements (dangerous!) or due to lack of knowledge regarding the consequences, but the case is then that the reserved connection is taken by a regular user, and your administrative user (usually <code>root</code>) won&#8217;t be able to connect.</p>
<h3>How to guarantee access to the instance</h3>
<p>Besides resorting to the <a target="_blank" href="https://www.percona.com/blog/2010/03/23/too-many-connections-no-problem/">well known GDB hack devised by Aurimas</a> long ago for Error 1040, there are now better solutions, <strong>but you need to enable them first.</strong></p>
<p>With Percona Server 5.5.29 and up, and with MySQL 8.0.14 and up, you can set up an extra port that allows a number of extra connections. These additional interfaces will not be used by your applications; they are only for your database administrators and monitoring/health-check agents (see note on this further below).</p>
<h4>Setting up in Percona Server</h4>
<p>Starting with Percona Server 5.5.29, you can simply add <a target="_blank" href="https://www.percona.com/doc/percona-server/5.5/performance/threadpool.html#extra_port"><code>extra_port</code></a> to your <code>my.cnf</code> and the next time you restart the port will become available and will listen on the same <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_bind_address">bind_address</a> as regular connections. If you don&#8217;t set the <code>extra_port</code> variable, no additional port will be available by default.</p>
<p>You can also define <code>extra_max_connections</code> which sets the number of connections this port will handle. The default value for this is 1.</p>
<p>For a quick demo, I have saturated connections on the regular users port of an instance where I already have set <code>extra_port</code> and <code>extra_max_connections</code> in the <code>my.cnf</code>:</p><pre class="crayon-plain-tag">~ egrep 'port|extra' my.sandbox.cnf
port                  = 45989
extra_port            = 45999
extra_max_connections = 10

# attempt to show some variables
~ mysql --host=127.0.0.1 --port=45989 --user=msandbox --password -e "SHOW GLOBAL VARIABLES WHERE Variable_name IN ('port', 'extra_port')"
ERROR 1040 (HY000): Too many connections

# now again, through the extra_port
~ mysql --host=127.0.0.1 --port=45999 --user=msandbox --password -e "SHOW GLOBAL VARIABLES WHERE Variable_name IN ('port', 'extra_port')"
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| extra_port    | 45999 |
| port          | 45989 |
+---------------+-------+</pre><p>Note that <strong>extra_port has been removed in Percona Server 8.0.14 and newer</strong> since MySQL Community has implemented admin_port which duplicates this functionality. So make sure to edit your my.cnf when upgrading to Percona Server 8.0.14 or newer if you already have extra_port defined there!</p>
<h4>Setting up in MySQL Community</h4>
<p>As mentioned, this requires MySQL 8.0.14 where <a target="_blank" href="https://dev.mysql.com/worklog/task/?id=12138" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer">WorkLog 12138</a> was implemented.</p>
<p>To enable the Admin Interface you have to define the <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_admin_address">admin_addres</a>, which must be a single and unique (no wildcards allowed) IPv4, IPv6, IPv4-mapped, or hostname on which the admin interface will listen. If this variable is not defined, then the interface is not enabled at all.</p>
<p>You can also define a port, but it&#8217;s not mandatory and it defaults to <code>33062</code>. So if that port is free then you don&#8217;t need to configure it. When defined, both variables should be placed under the <code>[mysqld]</code> section of your <code>my.cnf</code>.</p>
<p>Finally, you can also set <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_create_admin_listener_thread"><code>create_admin_listener_thread</code></a> (disabled by default) which will create a separate thread for incoming connection handling, which can be helpful in some situations.</p>
<p>Another difference is that Oracle&#8217;s documentation claims that:</p>
<blockquote><p><em>There is no limit on the number of administrative connections.</em></p></blockquote>
<p>(This is in contrast with our default of 1). I am not sure what this means, but I would be careful making sure you don&#8217;t accidentally establish 1,000,000 connections as they might not be limited but would still consume resources!</p>
<h3>Using it for monitoring and health-checks</h3>
<p>A very useful thing is that not only humans can use the extra interface/port during emergency cases where <code>max_connections</code> has been reached; it can also be used by your monitoring system and your proxy/load balancer/service discovery health-check probes.</p>
<p>Monitoring scripts can still pull data for your graphs to later understand why the connection pile up happened. And your health-check scripts could report the degraded state of the server, possibly with a particular code indicating connections are saturated but the server is responsive (meaning it could clear on its own, so it might be worth allowing a longer timeout to failover).</p>
<p>As a warning: make sure to establish only one single connection at a time for monitoring/health probes, to avoid filling up the extra_max_connections in Percona Server or to avoid creating one million threads in MySQL. In other words, your scripts should not connect again if the previous query/connection to the database is still ongoing.</p>
<p>And here is the same demo as before with MySQL:</p><pre class="crayon-plain-tag">~ grep admin_ my.sandbox.cnf
admin_address = 127.0.0.1
admin_port = 34888

# regular port
~ mysql --host=127.0.0.1 --port=35849 --user=msandbox --password -e "SHOW GLOBAL VARIABLES WHERE Variable_name IN ('port', 'admin_address', 'admin_port');"
Enter password:
ERROR 1040 (HY000): Too many connections

# admin interface and port
~ mysql --host=127.0.0.1 --port=34888 --user=msandbox --password -e "SHOW GLOBAL VARIABLES WHERE Variable_name IN ('port', 'admin_address', 'admin_port');"a
Enter password:
+---------------+-----------+
| Variable_name | Value     |
+---------------+-----------+
| admin_address | 127.0.0.1 |
| admin_port    | 34888     |
| port          | 35849     |
+---------------+-----------+</pre><p><strong>Note that for Percona Server 8.0.14 and newer, the process will be the same as for MySQL Community.</strong></p>
<h3>Help! I need to login but I don&#8217;t have an extra port!</h3>
<p>If this is the reason you are reading this post, then you can either follow the crazy GDB hack (no offense meant, Aurimas! Just seems risky :-D) or terminate the instance. The good part is that (most times) you can terminate the instance in a clean fashion by using <code>SIGTERM</code> (-15) instead of <code>SIGKILL</code> (-9). This will tell the server it should perform a clean shutdown, which will give threads a chance to exit gracefully. To do so simply run these:</p>
<p>1) Get PID</p><pre class="crayon-plain-tag">marcos.albe in ~/ pgrep -x mysqld;
650</pre><p>2) And then send SIGTERM to that PID:</p><pre class="crayon-plain-tag">marcos.albe in ~/ kill -15 650;</pre><p>3) You can now tail the error log to watch the shutdown happening; You should see a sequence like</p><pre class="crayon-plain-tag">2019-07-11T13:43:28.421244Z 0 [Note] Giving 0 client threads a chance to die gracefully
2019-07-11T13:43:28.521238Z 0 [Note] Shutting down slave threads
2019-07-11T13:43:28.521272Z 0 [Note] Forcefully disconnecting 0 remaining clients</pre><p>That signals the beginning of the shutdown sequence. Then you should wait for a line like the one below to appear, to know the shutdown is complete:</p><pre class="crayon-plain-tag">2019-07-11T13:43:31.292836Z 0 [Note] /opt/percona_server/5.7.26/bin/mysqld: Shutdown complete</pre><p></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/23/connect-to-mysql-after-hitting-error-1040-too-many-connections/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/23/connect-to-mysql-after-hitting-error-1040-too-many-connections/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>Stephen Thorn</name>
					</author>
		<title type="html"><![CDATA[PMM for MongoDB: Quick Start Guide]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/23/pmm-for-mongodb-start-guide/" />
		<id>https://www.percona.com/blog/?p=59052</id>
		<updated>2019-08-01T19:16:58Z</updated>
		<published>2019-07-23T13:00:32Z</published>
		<category scheme="https://www.percona.com/blog" term=".PMM QAN" /><category scheme="https://www.percona.com/blog" term="Database Monitoring" /><category scheme="https://www.percona.com/blog" term="MongoDB" /><category scheme="https://www.percona.com/blog" term="Percona Monitoring and Management" /><category scheme="https://www.percona.com/blog" term="PMM" />		<summary type="html"><![CDATA[<img width="200" height="130" src="https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-200x130.png" class="webfeedsFeaturedVisual wp-post-image" alt="PMM for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-200x130.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-300x195.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-367x238.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-560x367.png 560w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01.png 809w" sizes="(max-width: 200px) 100vw, 200px" />As a Solutions Engineer at Percona, one of my responsibilities is to support our customer-facing roles such as the sales and customer success teams, which affords me the opportunity to speak to many current and new customers who partner with Percona. I often find that many people are interested in Percona Monitoring and Management (PMM) [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/23/pmm-for-mongodb-start-guide/"><![CDATA[<img width="200" height="130" src="https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-200x130.png" class="webfeedsFeaturedVisual wp-post-image" alt="PMM for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-200x130.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-300x195.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-367x238.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01-560x367.png 560w, https://www.percona.com/blog/wp-content/uploads/2019/07/qan01.png 809w" sizes="(max-width: 200px) 100vw, 200px" /><p>As a Solutions Engineer at Percona, one of my responsibilities is to support our customer-facing roles such as the sales and customer success teams, which affords me the opportunity to speak to many current and new customers who partner with Percona. I often find that many people are interested in Percona Monitoring and Management (PMM) as a free and open-source monitoring solution due to its robust monitoring capabilities when compared to many SaaS-based monitoring solutions. They are interested in installing PMM for MongoDB for the first time and want a “quick start guide” with a brief overview to get their feet wet. I have included the commands to get started for both <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/index.html">PMM 1</a> and <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/2.x/index.html">PMM 2</a> (PMM2 is still in beta).</p>
<p><img class="alignnone wp-image-57371 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/05/qan01.png" alt="PMM for MongoDB" width="809" height="525" srcset="https://www.percona.com/blog/wp-content/uploads/2019/05/qan01.png 809w, https://www.percona.com/blog/wp-content/uploads/2019/05/qan01-200x130.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/05/qan01-300x195.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/05/qan01-367x238.png 367w" sizes="(max-width: 809px) 100vw, 809px" /></p>
<h2><strong>Overview and Architecture</strong></h2>
<p>PMM is an open-source platform for out-of-the-box management and monitoring of MySQL, MongoDB, and PostgreSQL performance, on-premise and in the cloud. It is developed by Percona in collaboration with experts in the field of managed database services, support, and consulting. PMM is built off of <a target="_blank" href="https://prometheus.io/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer">Prometheus</a>, a powerful open-source monitoring and alerting platform, and supports any other service that has an <strong>exporter</strong>. An exporter is an endpoint that collects data on the instance being monitored and is polled by Prometheus to collect metrics. For more information on how to use your own exporters, read the documentation <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/pmm-admin.html#pmm-pmm-admin-external-monitoring-service-adding">here</a>.</p>
<p>When deployed on-premises, the PMM platform is based on a client-server model that enables scalability. It includes the following modules:</p>
<ul>
<li><u>PMM Client</u>&#8211; installed on every database host that you want to monitor. It collects server metrics, general system metrics, and Query Analytics data for a complete performance overview.</li>
<li><u>PMM Server</u> &#8211; the central part of PMM that aggregates collected data and presents it in the form of tables, dashboards, and graphs in a web interface.</li>
</ul>
<p>PMM can also be deployed to support DBaaS instances for remoting monitoring. Instructions can be found <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/index.html">here</a>, under the <strong>Advanced</strong> section. The drawback of this approach is that you will not have visibility of host-level metrics (CPU, memory, and disk activity will not be captured nor displayed in PMM). There are currently 3 different deployment options:</p>
<ul>
<li><a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/deploy/server/docker.html">Running PMM Server via Docker</a></li>
<li><a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/deploy/server/virtual-appliance.html">Running PMM Server as a Virtual Appliance</a></li>
<li><a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/deploy/server/ami.html">Running PMM Server Using AWS Marketplace</a></li>
</ul>
<p>For a more detailed overview of the PMM Architecture please read the <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/architecture.html">Overview of PMM Architecture</a>.</p>
<h2><strong>Demonstration Environment</strong></h2>
<p>When deploying PMM in this example, I am making the following assumptions about the environment:</p>
<ul>
<li>MongoDB and the monitoring host are running on Debian based operating systems. (For information on installing as an RPM instead please read <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/deploy/index.html#deploy-pmm-server-installing" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.percona.com/doc/percona-monitoring-and-management/deploy/index.html%23deploy-pmm-server-installing&amp;source=gmail&amp;ust=1563469380766000&amp;usg=AFQjCNFbz-_fAlz6hnLJKLcret7YrOY52Q">Deploying Percona Monitoring and Management</a>.)</li>
<li>MongoDB is already installed and setup. The username and password for the MongoDB user are percona:percona.</li>
<li>The PMM server will be installed within a docker container on a dedicated host.</li>
</ul>
<h2><strong>Installing PMM Server</strong></h2>
<p>This process will consist of two steps:</p>
<ol>
<li>Create the docker container – docker will automatically pull the PMM Server image from the Percona docker repository.</li>
<li>Start (or run) the docker container – docker will bring up the PMM Server in the container</li>
</ol>
<h3><a target="_blank" name="_Toc9168526"></a><strong>Create the Docker Container</strong></h3>
<p>The code below illustrates the command for creating the docker container for PMM 1:</p><pre class="crayon-plain-tag">docker create \
  -v /opt/prometheus/data \
  -v /opt/consul-data \
  -v /var/lib/mysql \
  -v /var/lib/grafana \
  --name pmm-data \
  percona/pmm-server:1 /bin/true</pre><p>The code below illustrates the command for creating the docker container for PMM 2:</p><pre class="crayon-plain-tag">docker create -v /srv --name pmm-data-2-0-0-beta1 perconalab/pmm-server:2.0.0-beta1 /bin/true</pre><p>This is the expected output from the code:</p>
<p><img class="alignnone wp-image-59059" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture1.png" alt="" width="733" height="252" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture1.png 908w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture1-200x69.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture1-300x103.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture1-367x126.png 367w" sizes="(max-width: 733px) 100vw, 733px" /></p>
<p>Use the following command to start the PMM 1 docker container:</p><pre class="crayon-plain-tag">docker run -d \
   -p 80:80 \
   --volumes-from pmm-data \
   --name pmm-server \
   --restart always \
   percona/pmm-server:1</pre><p>Use the following command to start the PMM 2 docker container:</p><pre class="crayon-plain-tag">docker run -d -p 80:80 -p 443:443 --volumes-from pmm-data-2-0-0-beta1 --name pmm-server-2.0.0-beta1 --restart always perconalab/pmm-server:2.0.0-beta1</pre><p>This is the expected output from the code:</p>
<p><img class="alignnone wp-image-59061" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture2.png" alt="" width="737" height="172" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture2.png 908w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture2-200x47.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture2-300x70.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture2-367x86.png 367w" sizes="(max-width: 737px) 100vw, 737px" /></p>
<p>The PMM Server should now be installed! Yes, it <strong>IS</strong> that easy. In order to check that you can access PMM, navigate in a browser to the IP address of the monitoring host. If you are using PMM 2, the default username and password for viewing PMM is admin:admin. You should arrive at a page that looks like <a target="_blank" href="https://pmmdemo.percona.com">https://pmmdemo.percona.com.</a></p>
<h2><strong>Installing PMM Client for MongoDB</strong></h2>
<h3><strong>Setting up DB permissions</strong></h3>
<p>PMM Query Analytics for MongoDB requires the user of the <strong>mongodb_exporter</strong> to have the <em>clusterMonitor </em>role assigned for the <strong>admin</strong> database and the <em>read</em> role for the <strong>local</strong> database. If you do not have these set up already, please read <a target="_blank" href="https://www.percona.com/doc/percona-monitoring-and-management/conf-mongodb.html">Configuring MongoDB for Monitoring in PMM Query Analytics</a>.</p>
<h3><strong>Download the Percona repo package</strong></h3>
<p>We must first enable the Percona package repository on our MongoDB instance and install the PMM Client. We can run the following commands in order to accomplish this:</p><pre class="crayon-plain-tag">$ wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
$ sudo dpkg -i percona-release_latest.generic_all.deb
$ sudo apt-get update</pre><p>Since PMM 2 is still not GA, you’ll need to leverage our experimental release of the Percona repository. You’ll need to download and install the official <code>percona-release</code> package from Percona and use it to enable the Percona experimental component of the original repository. See <a target="_blank" class="external-link" href="https://www.percona.com/doc/percona-repo-config/percona-release.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;nofollow noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer" noopener noreferrer">percona-release official documentation</a> for further details on this new tool. The following commands can be used for PMM 2:</p><pre class="crayon-plain-tag">$ wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
$ sudo dpkg -i percona-release_latest.generic_all.deb
$ sudo percona-release disable all
$ sudo percona-release enable original experimental
$ sudo apt-get update</pre><p>Now that we have the MongoDB database server configured with the Percona software repository, we can download the agent software with the local package manager.  Enter the following command to automatically download and install the PMM Client package on the MongoDB server:</p><pre class="crayon-plain-tag">$ sudo apt-get install pmm-client</pre><p>To download and install the PMM 2 Client:</p><pre class="crayon-plain-tag">$ apt-get install pmm2-client</pre><p>Next, we will configure the PMM client by telling it where to find the PMM server.  Execute the following command to configure the PMM client:</p><pre class="crayon-plain-tag">$ sudo pmm-admin config --server=&lt;pmm_server_ip&gt;:80</pre><p>To configure the PMM 2 Client:</p><pre class="crayon-plain-tag">$ pmm-admin config --server-insecure-tls --server-url=https://&lt;pmm_server_ip&gt;:443</pre><p>You should get a similar output as below if it was successful:</p>
<p><img class="alignnone wp-image-59064" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture3.png" alt="" width="630" height="89" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture3.png 587w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture3-200x28.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture3-300x42.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture3-367x52.png 367w" sizes="(max-width: 630px) 100vw, 630px" /></p>
<p>Now we provide the PMM Client credentials necessary for monitoring the MongoDB database.  Execute the following command to start monitoring and communicating with the PMM server:</p><pre class="crayon-plain-tag">$ sudo pmm-admin add mongodb --uri mongodb://percona:percona@127.0.0.1:27017</pre><p>To start monitoring and communicating with the PMM 2 Server:</p><pre class="crayon-plain-tag">$ sudo pmm-admin add mongodb --use-profiler  --server-insecure-tls --username=percona  --password=percona --server-url=https://&lt;pmm_ip&gt;:443</pre><p>You should get a similar output as below if it was successful:</p>
<p><img class="alignnone size-full wp-image-59065" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture4.png" alt="" width="974" height="97" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Picture4.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture4-200x20.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture4-300x30.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Picture4-367x37.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p>Great! We have successfully installed PMM for MongoDB and are ready to take a look at the dashboards.</p>
<h2><strong>PMM for MongoDB Dashboards Overview</strong></h2>
<p>Navigate to the IP address of your monitoring host. http://&lt;<em>pmm_server_ip</em>&gt;.</p>
<p><u>PMM Home Dashboard</u> – The Home Dashboard for PMM gives an overview of your entire environment to include all the systems you have connected and configured for monitoring under PMM. It provides useful metrics such as CPU utilization, RAM availability, database connections, and uptime.</p>
<p><img class="alignnone wp-image-59067 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_HomeDashboard.png" alt="Percona Monitoring and Management Dashboard" width="974" height="616" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_HomeDashboard.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_HomeDashboard-200x126.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_HomeDashboard-300x190.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_HomeDashboard-367x232.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p><u>Cluster Summary</u> – it shows the statistics for the selected MongoDB cluster such as counts of sharded and un-sharded databases, shard and chunk statistics, and various mongos statistics.</p>
<p><img class="alignnone wp-image-59068 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ClusterSummary.png" alt="MongoDB Cluster Summary" width="974" height="516" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ClusterSummary.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ClusterSummary-200x106.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ClusterSummary-300x159.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ClusterSummary-367x194.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p><u>MongoDB Overview</u> &#8211; this provides basic information about MongoDB instances such as connections, command operations, and document operations.</p>
<p><img class="alignnone wp-image-59069 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_MongoDBOverview.png" alt="MongoDB Overview" width="974" height="381" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_MongoDBOverview.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_MongoDBOverview-200x78.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_MongoDBOverview-300x117.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_MongoDBOverview-367x144.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p><u>ReplSet</u> &#8211; provides information about replica sets and their members such as replication operations, replication lag, and member state uptime.</p>
<p><img class="alignnone wp-image-59070 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ReplSet.png" alt="ReplSet" width="974" height="445" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ReplSet.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ReplSet-200x91.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ReplSet-300x137.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_ReplSet-367x168.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p><u>WiredTiger/MMAPv1/In-Memory/RocksDB</u> &#8211; it contains metrics that describe the performance of the selected host storage engine.</p>
<p><img class="alignnone wp-image-59072 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_Engines.png" alt="WiredTiger/MMAPv1/In-Memory/RocksDB" width="974" height="468" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_Engines.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_Engines-200x96.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_Engines-300x144.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_Engines-367x176.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<p><u>Query Analytics</u> – this allows you to analyze database queries over periods of time. This can help you optimize database performance by ensuring queries are executed as expected and within the shortest amount of time. If you are having performance issues, this is a great place to see which queries may be the cause of your performance issues and get detailed metrics for them.</p>
<p><img class="alignnone wp-image-59073 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_QAN.png" alt="PMM Query Analytics" width="974" height="675" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_QAN.png 974w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_QAN-200x139.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_QAN-300x208.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PMM_MongoDB_QAN-367x254.png 367w" sizes="(max-width: 974px) 100vw, 974px" /></p>
<h2><strong>What Now?</strong></h2>
<p>Now that you have PMM for MongoDB up and running, I encourage you to explore in-depth more of the graphs and features. A few other MongoDB PMM blog posts which may be of interest:</p>
<ul>
<li><a target="_blank" href="https://www.percona.com/blog/2018/03/02/pmm-mongodb-basic-graphs-for-a-good-system-overview/">PMM for MongoDB: Basic Graphs for a Good System Overview</a></li>
<li><a target="_blank" href="https://www.percona.com/blog/2017/09/28/percona-monitoring-and-management-1-3-0-query-analytics-support-for-mongodb/">PMM Query Analytics Support for MongoDB</a></li>
<li><a target="_blank" href="https://www.percona.com/blog/2017/02/28/percona-monitoring-and-management-pmm-graphs-explained-mongodb-mmapv1/">PMM Graphs Explained: MongoDB MMAPv1</a></li>
<li><a target="_blank" href="https://www.percona.com/blog/2017/02/27/percona-monitoring-and-management-pmm-graphs-explained-wiredtiger-and-percona-memory-engine/">PMM Graphs Explained: WiredTiger and Percona Memory Engine</a></li>
<li><a target="_blank" href="https://www.percona.com/blog/2017/02/22/percona-monitoring-and-management-pmm-graphs-explained-mongodb-with-rocksdb/">PMM Graphs Explained: MongoDB with RocksDS</a></li>
<li><a target="_blank" href="https://www.percona.com/blog/2018/06/26/mongodb-explain-using-pmm-qan-for-mongodb-query-analytics/">MongoDB Explain – Using PMM-QAN for MongoDB Query Analytics</a></li>
</ul>
<p>If you run into issues during the install process, a good place to start is Peter Zaitsev’s blog post on <a target="_blank" href="https://www.percona.com/blog/2018/01/17/troubleshooting-percona-monitoring-and-management-pmm-metrics/">PMM Troubleshooting</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/23/pmm-for-mongodb-start-guide/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/23/pmm-for-mongodb-start-guide/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Dmitriy Kostiuk</name>
					</author>
		<title type="html"><![CDATA[Percona Monitoring and Management (PMM) 2 Beta 4 Is Now Available]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/22/percona-monitoring-and-management-pmm-2-beta-4-is-now-available/" />
		<id>https://www.percona.com/blog/?p=59136</id>
		<updated>2019-07-22T17:41:43Z</updated>
		<published>2019-07-22T17:33:17Z</published>
		<category scheme="https://www.percona.com/blog" term="Events and Announcements" /><category scheme="https://www.percona.com/blog" term="Percona Monitoring and Management" /><category scheme="https://www.percona.com/blog" term="PMM" />		<summary type="html"><![CDATA[<img width="150" height="100" src="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-150x100.png" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Monitoring and Management" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-150x100.png 150w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234.png 690w" sizes="(max-width: 150px) 100vw, 150px" />We are pleased to announce our 4th Beta release of PMM 2! PMM (Percona Monitoring and Management) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. With this release we&#8217;ve made the following improvements since our last public Beta at the end of May: Query Analytics PostgreSQL &#8211; Aggregate [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/22/percona-monitoring-and-management-pmm-2-beta-4-is-now-available/"><![CDATA[<img width="150" height="100" src="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-150x100.png" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Monitoring and Management" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-150x100.png 150w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234.png 690w" sizes="(max-width: 150px) 100vw, 150px" /><p><img class="alignright wp-image-45724 size-medium" src="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-300x200.png" alt="Percona Monitoring and Management" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234-150x100.png 150w, https://www.percona.com/blog/wp-content/uploads/2017/10/PMM-e1509564903234.png 690w" sizes="(max-width: 300px) 100vw, 300px" /></p>
<p>We are pleased to announce our <span class="inline-comment-marker" data-ref="580361d1-7685-4fa1-bdc4-282c4b5a97bc">4<sup>th</sup></span> Beta release of PMM 2! PMM (Percona Monitoring and Management) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. With this release we&#8217;ve made the following improvements since our last public Beta at the end of May:</p>
<ul>
<li><strong>Query Analytics</strong>
<ul>
<li><strong>PostgreSQL</strong> &#8211; Aggregate &amp; identify slow queries from pg_stat_statements data source</li>
<li><strong>Interface updates </strong>&#8211; label improvements, sparkline updates, tooltips</li>
</ul>
</li>
<li><strong>PMM Server Monitoring</strong>  &#8211; look for pmm-server in Dashboards and Query Analytics</li>
<li>ProxySQL monitoring &#8211; now available using <code>pmm-admin add proxysql</code></li>
<li><strong>Environment Overview Dashboard</strong> &#8211; Updated layout and colours &#8211; take a look!</li>
<li><strong>Debian 10 support</strong> we now have pmm2-client deb packages for Debian 10</li>
</ul>
<div class="confluence-information-macro-body">
<div class="confluence-information-macro-body">
<p>PMM 2 is still a work in progress &#8211; you may encounter some bugs and missing features. We are aware of a number of issues, but please report any and all that you find to <a target="_blank" class="external-link" href="https://jira.percona.com/browse/PMM" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;nofollow noopener noreferrer" noopener noreferrer">Percona&#8217;s JIRA</a>.</p>
<p><strong>This release is not recommended for production environments.  </strong><strong>PMM 2 is designed to be used as a new installation &#8211; please don&#8217;t try to upgrade your existing PMM 1 environment.</strong></p>
</div>
</div>
<h2 id="AnnouncingPMM2Betarelease-QueryAnalyticsDashboard"><span class="inplace-header">Query Analytics Dashboard Enhancements<br />
</span></h2>
<h3 id="AnnouncingPMM2Beta3release-QueryAnalyticsforPostgreSQL">Query Analytics for PostgreSQL</h3>
<p>We&#8217;re excited to provide Query Analytics for PostgreSQL in this release, where you can now visualize query activity for PostgreSQL. Monitoring PostgreSQL queries is achieved via the popular <code><a target="_blank" class="external-link" href="https://www.postgresql.org/docs/current/pgstatstatements.html" rel="nofollow">pg_stat_statements</a></code> extension.</p>
<h3><img class="alignnone size-full wp-image-59213" src="https://www.percona.com/blog/wp-content/uploads/2019/07/GIFMaker.org_H7NcMd.gif" alt="" width="800" height="326" /></h3>
<p>Query Analytics Interface Updates</p>
<p class="auto-cursor-target">We spent some time updating Sparklines logic to be more accurate:</p>
<p><img class="alignnone size-full wp-image-59145" src="https://www.percona.com/blog/wp-content/uploads/2019/07/sparkline-update.gif" alt="" width="800" height="231" /></p>
<p>We now wrap long labels, and shorten in some cases with a hint to show the full label name:</p>
<p><img class="alignnone size-full wp-image-59146" src="https://www.percona.com/blog/wp-content/uploads/2019/07/full-name.gif" alt="" width="800" height="306" /></p>
<div class="columnLayout single" data-layout="single">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<p>We&#8217;re incrementally improving our Query Analytics to better explain what we&#8217;re measuring with the addition of tooltips:</p>
<p><img class="alignnone size-full wp-image-59163" src="https://www.percona.com/blog/wp-content/uploads/2019/07/query-details-tooltip.gif" alt="" width="800" height="148" /></p>
</div>
</div>
</div>
<div class="columnLayout single" data-layout="single">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<h2 id="AnnouncingPMM2Beta3release-PMMServerMonitoring">PMM Server Monitoring</h2>
<p>To better understand resource utilization on your PMM Server host, we&#8217;ve added a pmm-server entry in the OS, PostgreSQL, Prometheus, and Query Analytics Dashboards:</p>
<h3 id="AnnouncingPMM2Beta3release-QueryAnalyticsforPostgreSQL-pmm-server">Query Analytics for PostgreSQL &#8211; pmm-server</h3>
<p>You can explore the queries that are executed by Query Analytics:</p>
<p><img class="alignnone size-full wp-image-59148" src="https://www.percona.com/blog/wp-content/uploads/2019/07/qan-pmm-managed.gif" alt="" width="800" height="443" /></p>
</div>
</div>
</div>
<h2 id="AnnouncingPMM2Beta2release-ProxySQLsupport">ProxySQL support</h2>
<p>You can now add ProxySQL instance to PMM and take advantage of the ProxySQL Overview Dashboard, which resides under the <strong>HA</strong> group in the system menu.</p>
<p><img class="alignnone size-full wp-image-59143" src="https://www.percona.com/blog/wp-content/uploads/2019/07/proxysql.png" alt="" width="790" height="625" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/proxysql.png 790w, https://www.percona.com/blog/wp-content/uploads/2019/07/proxysql-190x150.png 190w, https://www.percona.com/blog/wp-content/uploads/2019/07/proxysql-300x237.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/proxysql-367x290.png 367w" sizes="(max-width: 790px) 100vw, 790px" /></p>
<div class="columnLayout single" data-layout="single">
<div class="cell normal" data-type="normal">
<h2 class="innerCell">Simplified Environment Overview Dashboard</h2>
</div>
</div>
<p>We&#8217;ve categorized the dashboard into several sections. The first two sections show total information for the entire environment as well as Top and Min values:</p>
<table>
<tbody>
<tr>
<th><img class="alignnone wp-image-59154" src="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_topmost_sections-1024x511.png" alt="" width="399" height="199" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_topmost_sections-1024x511.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_topmost_sections-200x100.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_topmost_sections-300x150.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_topmost_sections-367x183.png 367w" sizes="(max-width: 399px) 100vw, 399px" /></th>
<th><img class="alignnone size-medium wp-image-59155" src="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom-300x225.png" alt="" width="300" height="225" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom-300x225.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom-200x150.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom-463x348.png 463w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom-367x276.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overview_left_bottom.png 935w" sizes="(max-width: 300px) 100vw, 300px" /></th>
</tr>
</tbody>
</table>
<p>We also display the label name and current value, and you can click each object in order to drill down for greater detail:</p>
<p><img class="alignnone wp-image-59156 size-medium" src="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overivew_right_bottom-300x221.png" alt="" width="300" height="221" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/env_overivew_right_bottom-300x221.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overivew_right_bottom-200x147.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overivew_right_bottom-367x270.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/env_overivew_right_bottom.png 908w" sizes="(max-width: 300px) 100vw, 300px" /></p>
<p>We&#8217;ve started collapsing some rows by default, in order to minimize the visual clutter. Opening each category automatically refreshes the dashboard:</p>
<p><img class="alignnone size-full wp-image-59160" src="https://www.percona.com/blog/wp-content/uploads/2019/07/env-anim.gif" alt="" width="800" height="625" /></p>
<h2>Installation and configuration</h2>
<p>The default PMM Server credentials are:</p>
<p><strong>username:</strong> admin<br />
<strong>password:</strong> admin</p>
<h3>Install PMM Server with docker</h3>
<p>The easiest way to install PMM Server is to deploy it with Docker. Running the PMM 2 Docker container with PMM Server can be done by the following commands (note the version tag of 2.0.0-beta4):</p><pre class="crayon-plain-tag">docker create -v /srv --name pmm-data-2-0-0-beta4 perconalab/pmm-server:2.0.0-beta4 /bin/true
docker run -d -p 80:80 -p 443:443 --volumes-from pmm-data-2-0-0-beta4 --name pmm-server-2.0.0-beta4 --restart always perconalab/pmm-server:2.0.0-beta4</pre><p></p>
<h3>Install PMM Client</h3>
<p>Since PMM 2 is still not GA, you&#8217;ll need to leverage our experimental release of the Percona repository. You&#8217;ll need to download and install the official <code>percona-release</code> package from Percona and use it to enable the Percona experimental component of the original repository. See <a target="_blank" class="external-link" href="https://www.percona.com/doc/percona-repo-config/percona-release.html" rel="nofollow">percona-release official documentation</a> for further details on this new tool.</p>
<p>Specific instructions for a Debian system are as follows:</p><pre class="crayon-plain-tag">wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb
sudo dpkg -i percona-release_latest.generic_all.deb</pre><p>Now enable the experimental repo:</p><pre class="crayon-plain-tag">sudo percona-release disable all
sudo percona-release enable original experimental</pre><p>Install <code>pmm2-client</code> package:</p><pre class="crayon-plain-tag">apt-get update
apt-get install pmm2-client</pre><p><strong>Users who have previously installed pmm2-client alpha version should remove the package and install a new one in order to update to beta1.</strong></p>
<p>Please note that leaving experimental repository enabled may affect further package installation operations with bleeding-edge software that may not be suitable for Production. You can revert by disabling experimental via the following commands:</p><pre class="crayon-plain-tag">sudo percona-release disable original experimental
sudo apt-get update</pre><p></p>
<h3>Configure PMM</h3>
<p>Once PMM Client is installed, run the <code>pmm-admin config</code> command with your PMM Server IP address to register your Node:</p><pre class="crayon-plain-tag"># pmm-admin config --server-insecure-tls --server-url=https://&lt;IP Address&gt;:443</pre><p>You should see the following:</p><pre class="crayon-plain-tag">Checking local pmm-agent status...
pmm-agent is running.
Registering pmm-agent on PMM Server...
Registered.
Configuration file /usr/local/percona/pmm-agent.yaml updated.
Reloading pmm-agent configuration...
Configuration reloaded.</pre><p></p>
<h3 id="AnnouncingPMM2Betarelease-AddingMySQLMetricsandQueryAnalytics">Adding MySQL Metrics and Query Analytics</h3>
<p>MySQL server can be added for the monitoring in its normal way. Here is a command which adds it using the PERFORMANCE_SCHEMA source:</p>
<div class="preformatted panel conf-macro output-block" data-hasbody="true" data-macro-name="noformat">
<div class="preformattedContent panelContent">
<pre class="crayon-plain-tag">pmm-admin add mysql <span class="blob-code-inner"><span class="x x-first x-last">--</span>query-source<span class="x x-first x-last">='</span>perfschema<span class="x x-first x-last">'</span></span> --username=pmm --password=pmm</pre>
</div>
</div>
<p>where <em>username</em> and <em>password</em> are credentials for the monitored MySQL access, which will be used locally on the database host.</p>
<p>The syntax to add MySQL services (Metrics and Query Analytics) using the Slow Log source is the following:</p>
<div class="preformatted panel conf-macro output-block" data-hasbody="true" data-macro-name="noformat">
<div class="preformattedContent panelContent">
<pre class="crayon-plain-tag">pmm-admin add mysql <span class="blob-code-inner"><span class="x x-first x-last">--</span>query-source<span class="x x-first x-last">='</span>slowlog<span class="x x-first x-last">'</span></span> --username=pmm --password=pmm</pre>
</div>
</div>
<p class="auto-cursor-target">When the server is added, you can check your MySQL dashboards and Query Analytics in order to view its performance information!</p>
<h3 id="AnnouncingPMM2Betarelease-AddingMongoDBMetricsandQueryAnalytics">Adding MongoDB Metrics and Query Analytics</h3>
<p>You can add MongoDB services (Metrics and Query Analytics) with a similar command:</p>
<div class="preformatted panel conf-macro output-block" data-hasbody="true" data-macro-name="noformat">
<div class="preformattedContent panelContent">
<pre class="crayon-plain-tag">pmm-admin add mongodb --use-profiler --username=pmm --password=pmm</pre>
</div>
</div>
<h3 id="AnnouncingPMM2Betarelease-AddingPostgreSQLmonitoringservice">Adding PostgreSQL monitoring service</h3>
<p>You can add PostgreSQL service as follows:</p>
<div class="preformatted panel conf-macro output-block" data-hasbody="true" data-macro-name="noformat">
<div class="preformattedContent panelContent">
<pre class="crayon-plain-tag">pmm-admin add postgresql --username=pmm --password=pmm</pre>
</div>
</div>
<p>You can then check your PostgreSQL Overview dashboard.</p>
<h3 id="AnnouncingPMM2Beta2release-AddProxySQLmonitoringservice">Add ProxySQL monitoring service</h3>
<p>You can add ProxySQL service as follows:</p>
<div class="preformatted panel conf-macro output-block" data-hasbody="true" data-macro-name="noformat">
<div class="preformattedContent panelContent">
<pre class="crayon-plain-tag">pmm-admin add proxysql --username=admin --password=admin</pre>
</div>
</div>
<p>You can then check your ProxySQL Overview dashboard.</p>
<h2 id="AnnouncingPMM2Beta2release-AboutPMM"><span class="inplace-header">About Percona Monitoring and Management</span></h2>
<p><a target="_blank" class="external-link" href="https://www.percona.com/doc/percona-monitoring-and-management/index.html" rel="nofollow">Percona Monitoring and Management</a> (PMM) is a free and open-source platform for managing and monitoring MySQL®, MongoDB®, and PostgreSQL® performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL®, MongoDB®, and PostgreSQL® servers to ensure that your data works as efficiently as possible.</p>
<p>Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using <a target="_blank" href="https://jira.percona.com/secure/Dashboard.jspa" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;nofollow noopener noreferrer" noopener noreferrer">our bug tracking system</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/22/percona-monitoring-and-management-pmm-2-beta-4-is-now-available/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/22/percona-monitoring-and-management-pmm-2-beta-4-is-now-available/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>Avinash Vallarapu</name>
					</author>
		<title type="html"><![CDATA[Automatic Index Recommendations in PostgreSQL using pg_qualstats and hypopg]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/22/automatic-index-recommendations-in-postgresql-using-pg_qualstats-and-hypopg/" />
		<id>https://www.percona.com/blog/?p=58654</id>
		<updated>2019-07-22T16:42:18Z</updated>
		<published>2019-07-22T16:13:59Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Automatic Index Recommendations" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />In my previous blog post, we have seen how the extension pg_qualstats is an extension developed by the POWA Team who have contributed to the very useful extension [crayon-5d433ac3707f6937641286-i/] in PostgreSQL. Let&#8217;s discuss this approach on Hacker News. ]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/22/automatic-index-recommendations-in-postgresql-using-pg_qualstats-and-hypopg/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Automatic Index Recommendations" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Automatic-Index-Recommendations.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-57141" src="https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-200x127.png" alt="PostgreSQL" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-300x190.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-367x233.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline.png 756w" sizes="(max-width: 200px) 100vw, 200px" />In my <a target="_blank" href="https://www.percona.com/blog/2019/06/21/hypothetical-indexes-in-postgresql/">previous blog post</a>, we have seen how the extension <a target="_blank" href="https://github.com/HypoPG/hypopg" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">hypopg</a> can be helpful in creating hypothetical indexes in PostgreSQL. If you have read it already, you know that it is very useful in verifying whether an Index can improve the performance of an SQL without having to create it in reality. Considering this, is there also a way to automatically suggest which indexes can improve the performance of some of your SQL&#8217;s? The answer is&#8230; YES!</p>
<p>In this blog post, we will see how we can get a direct recommendation to improve a specific query, as seen in the following snippet.</p><pre class="crayon-plain-tag">query                       |                 recmnded_index                 | percent_improvd 
---------------------------------------------------+------------------------------------------------+-----------------
 select * from foo.bar where id2 = $1 and id4 = $2 | CREATE INDEX ON foo.bar USING btree (id2, id4) |           99.96
 select * from foo.bar where id3 = $1              | CREATE INDEX ON foo.bar USING btree (id3)      |           99.93
(2 rows)</pre><p>Without any further delay, let&#8217;s discuss the extension <pre class="crayon-plain-tag">pg_qualstats</pre> which enables us to achieve this requirement for PostgreSQL versions 9.4 or later. Following that, we will take a look at a logic which could automatically suggest what indexes would be helpful for query optimizations &#8211; without much manual work.</p>
<p>All of the logic discussed in this blog post is reproducible, so please feel free to do so using the commands and try tuning your custom queries in test environments.</p>
<h3><strong>pg_qualstats</strong></h3>
<p><a target="_blank" href="https://github.com/powa-team/pg_qualstats" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">pg_qualstats</a> is an extension developed by the <a target="_blank" href="https://github.com/powa-team/pg_qualstats/graphs/contributors" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">POWA Team</a> to uncover the need for storing predicates (quals). It stores the predicates found in WHERE clauses and JOIN conditions. This helps us analyze a query execution and opens up the possibility of automatic query optimizations.</p>
<p>When we query <pre class="crayon-plain-tag">pg_stat_statements</pre> like extensions, we only see the prepared SQL or the SQL without any bind variable values. As you cannot perform an EXPLAIN (to see the execution plan of a Query) without the quals, the only option available is to see if that query was logged in the PostgreSQL log file and then identify the parameters passed to it. Or you can maybe use some arbitrary values, but this requires some manual intervention and time. But, when you create this extension: pg_qualstats, it stores queries along with the actual quals based on the sample rate (<pre class="crayon-plain-tag">pg_qualstats.sample_rate</pre>) specified.</p>
<h3><strong>Creating the extension: pg_qualstats</strong></h3>
<p>For RedHat/CentOS, we can install it using the packages available in the PGDG repository. Once you have added the PGDG repo, simply run the following command:</p><pre class="crayon-plain-tag"># yum install pg_qualstats11</pre><p>Similarly for Ubuntu/Debian:</p><pre class="crayon-plain-tag"># apt install postgresql-11-pg-qualstats</pre><p>Once installed, you must add <pre class="crayon-plain-tag">pg_qualstats</pre> to <pre class="crayon-plain-tag">shared_preload_libraries</pre>. This requires a restart. As I am also using <pre class="crayon-plain-tag">pg_stat_statements</pre> to get the <pre class="crayon-plain-tag">queryid</pre> associated with each query recorded by <pre class="crayon-plain-tag">pg_qualstats</pre>, I have the following setting in my <pre class="crayon-plain-tag">postgresql.conf</pre> file:</p><pre class="crayon-plain-tag">shared_preload_libraries = 'pg_stat_statements, pg_qualstats'</pre><p>Modifying the above parameter requires a restart of your PostgreSQL instance.</p>
<h3><strong>Some GUCs you should know</strong></h3>
<p>In order to start taking advantage of this extension, you may have to set some of the <pre class="crayon-plain-tag">GUCs</pre> (Grand Unified Configuration) in your PostgreSQL server. These can be set using <pre class="crayon-plain-tag">ALTER SYSTEM</pre> or by manually adding the associated entries in your <pre class="crayon-plain-tag">postgresql.conf</pre> or <pre class="crayon-plain-tag">postgresql.auto.conf</pre> files.</p>
<p><pre class="crayon-plain-tag">pg_qualstats.enabled</pre>: true or false (to enable or to disable pg_qualstats). ON by default.</p>
<p><pre class="crayon-plain-tag">pg_qualstats.track_constants</pre>: true or false (to enable tracking of each constant. False would reduce the number of entries to track predicates.)</p>
<p><pre class="crayon-plain-tag">pg_qualstats.max</pre>: The number of queries tracked. Defaults to 1000.</p>
<p><pre class="crayon-plain-tag">pg_qualstats.resolve_oids</pre>: Just store the oids or resolve them and store at query time. This takes additional space.</p>
<p><pre class="crayon-plain-tag">pg_qualstats.track_pg_catalog</pre>: Defaults to false. Whether or not the predicates of the objects in pg_catalog schema should be computed.</p>
<p><pre class="crayon-plain-tag">pg_qualstats.sample_rate</pre>: Default is -1. The fraction of queries to be sampled. -1 defaults to (1/max_connections). When set to 1, everything is sampled. Similarly when set to 0.1, one out of 10 queries are sampled.</p>
<h3><strong>Quals and Query Examples</strong></h3>
<p>In order to see this in action, let us use <a target="_blank" href="https://github.com/Percona-Lab/sysbench-tpcc">sysbench-tpcc</a> to generate some SQL traffic, and then see some of the details captured.</p>
<p>Before running <pre class="crayon-plain-tag">sysbench-tpcc</pre>, I have created all the required extensions as seen in the following log. In order to see the <pre class="crayon-plain-tag">queryid</pre> (same as the <pre class="crayon-plain-tag">queryid</pre> column of <pre class="crayon-plain-tag">pg_stat_statements</pre>) associated with each qual captured, it is important to have the extension: <pre class="crayon-plain-tag">pg_stat_statements</pre> created. Similarly, to create hypothetical indexes, we need to have the extension: <pre class="crayon-plain-tag">hypopg</pre> created.</p><pre class="crayon-plain-tag">percona=# CREATE EXTENSION hypopg;
CREATE EXTENSION
percona=# CREATE EXTENSION pg_stat_statements ;
CREATE EXTENSION
percona=# CREATE EXTENSION pg_qualstats;
CREATE EXTENSION
percona=# \dx
                                     List of installed extensions
        Name        | Version |   Schema   |                        Description                        
--------------------+---------+------------+-----------------------------------------------------------
 hypopg             | 1.1.3   | public     | Hypothetical indexes for PostgreSQL
 pg_qualstats       | 1.0.8   | public     | An extension collecting statistics about quals
 pg_stat_statements | 1.6     | public     | track execution statistics of all SQL statements executed
 plpgsql            | 1.0     | pg_catalog | PL/pgSQL procedural language
(4 rows)

percona=# show shared_preload_libraries ;
     shared_preload_libraries     
----------------------------------
 pg_stat_statements, pg_qualstats
(1 row)</pre><p>For this test, I have set  <pre class="crayon-plain-tag">pg_qualstats.sample_rate</pre> to 1. Setting this to 1 captured every qual. We can, of course, reduce the sampling to ensure not everything is captured.</p><pre class="crayon-plain-tag">$ psql -d percona -c "ALTER SYSTEM SET pg_qualstats.sample_rate TO 1" 
ALTER SYSTEM

$ psql -c "select pg_reload_conf()"
 pg_reload_conf 
----------------
 t
(1 row)</pre><p></p>
<h4><strong>Installing and running sysbench-tpcc on RedHat/CentOS</strong></h4>
<p></p><pre class="crayon-plain-tag">$ sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm
$ sudo yum install git sysbench
$ git clone https://github.com/Percona-Lab/sysbench-tpcc.git
$ cd sysbench-tpcc
$ ./tpcc.lua --pgsql-user=postgres --pgsql-port=5432 --pgsql-db=percona --time=10 --threads=2 --report-interval=1 --tables=2 --scale=2 --use_fk=0  --trx_level=RC --db-driver=pgsql prepare
$ ./tpcc.lua --pgsql-user=postgres --pgsql-port=5432 --pgsql-db=percona --time=10 --threads=2 --report-interval=1 --tables=2 --scale=2 --use_fk=0  --trx_level=RC --db-driver=pgsql run</pre><p>After running the benchmark for 10 seconds, we are now ready to query the view: <pre class="crayon-plain-tag">pg_qualstats_indexes</pre> that gives us a nice view of the columns on which there are no indexes.</p><pre class="crayon-plain-tag">percona=# select * from pg_qualstats_indexes;
    relid    |   attnames   |      possible_types      | execution_count 
-------------+--------------+--------------------------+-----------------
 customer2   | {c_id}       | {brin,btree,hash}        |               4
 customer2   | {c_last}     | {brin,btree,hash,spgist} |              33
 customer1   | {c_id}       | {brin,btree,hash}        |               7
 customer1   | {c_last}     | {brin,btree,hash,spgist} |               8
 orders2     | {o_c_id}     | {brin,btree,hash}        |               2
 order_line1 | {ol_o_id}    | {brin,btree}             |             208
 order_line2 | {ol_o_id}    | {brin,btree}             |             202
 order_line2 | {ol_o_id}    | {brin,btree,hash}        |              20
 stock1      | {s_quantity} | {brin,btree}             |             208
 stock2      | {s_quantity} | {brin,btree}             |             202
(10 rows)</pre><p></p>
<h2><strong>Automatic Index Recommendations</strong></h2>
<p>In the previous section, we have seen the columns (of tables used in some of the SQLs in WHERE condition) which do not have indexes on them. Indexes on these columns can be considered as recommended indexes. To make this more meaningful, I have written the following function to store the queries that are using these columns as predicates, along with their execution plans <pre class="crayon-plain-tag">before</pre> and <pre class="crayon-plain-tag">after</pre> creating the recommended index. Using this data, we can understand whether the recommended index is really helpful.</p><pre class="crayon-plain-tag">CREATE OR REPLACE FUNCTION find_usable_indexes()
RETURNS VOID AS
$$
DECLARE
    l_queries     record;
    l_querytext     text;
    l_idx_def       text;
    l_bef_exp       text;
    l_after_exp     text;
    hypo_idx      record;
    l_attr        record;
    /* l_err       int; */
BEGIN
    CREATE TABLE IF NOT EXISTS public.idx_recommendations (queryid bigint, 
    query text, current_plan jsonb, recmnded_index text, hypo_plan jsonb);
    FOR l_queries IN
    SELECT t.relid, t.relname, t.queryid, t.attnames, t.attnums, 
    pg_qualstats_example_query(t.queryid) as query
      FROM 
        ( 
         SELECT qs.relid::regclass AS relname, qs.relid AS relid, qs.queryid, 
         string_agg(DISTINCT attnames.attnames,',') AS attnames, qs.attnums
         FROM pg_qualstats_all qs
         JOIN pg_qualstats q ON q.queryid = qs.queryid
         JOIN pg_stat_statements ps ON q.queryid = ps.queryid
         JOIN pg_amop amop ON amop.amopopr = qs.opno
         JOIN pg_am ON amop.amopmethod = pg_am.oid,
         LATERAL 
              ( 
               SELECT pg_attribute.attname AS attnames
               FROM pg_attribute
               JOIN unnest(qs.attnums) a(a) ON a.a = pg_attribute.attnum 
               AND pg_attribute.attrelid = qs.relid
               ORDER BY pg_attribute.attnum) attnames,     
         LATERAL unnest(qs.attnums) attnum(attnum)
               WHERE NOT 
               (
                EXISTS 
                      ( 
                       SELECT 1
                       FROM pg_index i
                       WHERE i.indrelid = qs.relid AND 
                       (arraycontains((i.indkey::integer[])[0:array_length(qs.attnums, 1) - 1], 
                        qs.attnums::integer[]) OR arraycontains(qs.attnums::integer[], 
                        (i.indkey::integer[])[0:array_length(i.indkey, 1) + 1]) AND i.indisunique)))
                       GROUP BY qs.relid, qs.queryid, qs.qualnodeid, qs.attnums) t
                       GROUP BY t.relid, t.relname, t.queryid, t.attnames, t.attnums                   
    LOOP
        /* RAISE NOTICE '% : is queryid',l_queries.queryid; */
        execute 'explain (FORMAT JSON) '||l_queries.query INTO l_bef_exp;
        execute 'select hypopg_reset()';
        execute 'SELECT indexrelid,indexname FROM hypopg_create_index(''CREATE INDEX on '||l_queries.relname||'('||l_queries.attnames||')'')' INTO hypo_idx;      
        execute 'explain (FORMAT JSON) '||l_queries.query INTO l_after_exp;
        execute 'select hypopg_get_indexdef('||hypo_idx.indexrelid||')' INTO l_idx_def;
        INSERT INTO public.idx_recommendations (queryid,query,current_plan,recmnded_index,hypo_plan) 
        VALUES (l_queries.queryid,l_querytext,l_bef_exp::jsonb,l_idx_def,l_after_exp::jsonb);        
    END LOOP;    
        execute 'select hypopg_reset()';
END;
$$ LANGUAGE plpgsql;</pre><p></p>
<h4><strong>The above function uses the following logic:</strong></h4>
<ol>
<li>Create a Table with name: <pre class="crayon-plain-tag">public.idx_recommendations</pre> where the results are stored. It stores the queries on which the table and column names mentioned in the output of <pre class="crayon-plain-tag">pg_qualstats_indexes</pre> are used as predicates, along with their execution plan before and after creating the <pre class="crayon-plain-tag">hypothethical</pre> indexes.</li>
<li>Get the list of Queries (candidates for query tuning) along with their <pre class="crayon-plain-tag">queryid</pre> and the attributes on which an index is recommended for each query. The SQL in the above <pre class="crayon-plain-tag">FOR LOOP</pre> is built using a slight modification to the existing view: <pre class="crayon-plain-tag">pg_qualstats_indexes</pre>.<br />
<pre class="crayon-plain-tag">relid |   relname   |  queryid   |  attnames  | attnums |                   query                    
-------+-------------+------------+------------+---------+--------------------------------------------
 17725 | customer2   |  297872607 | c_id       | {1}     | UPDATE customer2 ....
 17725 | customer2   |  702831032 | c_id       | {1}     | UPDATE customer2 ....
 17725 | customer2   | 1509701064 | c_last     | {6}     | SELECT count(c_id) namecnt ....
 17725 | customer2   | 1539164311 | c_id       | {1}     | SELECT c_discount, c_last, c_credit, w_t
 17725 | customer2   | 1976730265 | c_last     | {6}     | SELECT c_id FROM customer2 ......
 17725 | customer2   | 2041891134 | c_id       | {1}     | SELECT c_first, c_middle, c_last, c_stre..
 17728 | customer1   |  850567043 | c_id       | {1}     | SELECT c_first, c_middle, c_last, c_stre..
 17728 | customer1   |  977223112 | c_last     | {6}     | SELECT count(c_id) namecnt ....
 17728 | customer1   | 2618115206 | c_id       | {1}     | SELECT c_discount, c_last, c_credit, w_t..
 17728 | customer1   | 2965820579 | c_last     | {6}     | SELECT c_id FROM customer1 ....
 17728 | customer1   | 3738483437 | c_id       | {1}     | UPDATE customer1 ....
 17752 | orders2     | 2217779140 | o_id       | {1}     | SELECT o_c_id ....
 17752 | orders2     | 2709941400 | o_id       | {1}     | UPDATE orders2 ....
 17762 | new_orders2 | 3012757930 | no_o_id    | {1}     | DELETE FROM new_orders2 ....
 17771 | order_line2 |  192474146 | ol_o_id    | {1}     | SELECT COUNT(DISTINCT (s_i_id)) ....
 17771 | order_line2 |  313117619 | ol_o_id    | {1}     | UPDATE order_line2 ....
 17771 | order_line2 | 2921207531 | ol_o_id    | {1}     | SELECT SUM(ol_amount) sm ....
 17782 | stock2      |  192474146 | s_quantity | {3}     | SELECT COUNT(DISTINCT (s_i_id)) ....
(18 rows)</pre>
</li>
<li>An example query with predicates can be obtained using the function : <pre class="crayon-plain-tag">pg_qualstats_example_query()</pre> provided by <pre class="crayon-plain-tag">pg_qualstats</pre>.<br />
<pre class="crayon-plain-tag">percona=# select pg_qualstats_example_query(297872607);
                          pg_qualstats_example_query                           
-------------------------------------------------------------------------------
 UPDATE customer2                                                             +
                         SET c_balance=-1576.000000, c_ytd_payment=1576.000000+
                       WHERE c_w_id = 2                                       +
                         AND c_d_id=9                                         +
                         AND c_id=900
(1 row)</pre>
</li>
<li>Run <pre class="crayon-plain-tag">EXPLAIN</pre> on the example query to store it in the table: public.idx_recommendations.</li>
<li>Use <pre class="crayon-plain-tag">hypopg</pre> to create a hypothetical index on the attributes mentioned as columns without an index on them.</li>
<li>Run EXPLAIN on the query again and use <pre class="crayon-plain-tag">hypopg_reset()</pre> to drop the hypothetical index created.</li>
</ol>
<p>When I have validated if any of the queries generated by <pre class="crayon-plain-tag">sysbench-tpcc</pre> need some tuning using indexing, it was not a surprise that none of those need any further indexing. Hence, for the purpose of a demo, I have created a table and ran a few queries as following.</p><pre class="crayon-plain-tag">percona=# CREATE TABLE foo.bar (id int, dept int, id2 int, id3 int, id4 int, id5 int,id6 int,id7 int,details text, zipcode int);
CREATE TABLE
percona=# INSERT INTO foo.bar SELECT (random() * 1000000)::int, (random() * 1000000)::int, (random() * 1000000)::int,
(random() * 1000000)::int,(random() * 1000000)::int,(random() * 1000000)::int,(random() * 1000000)::int,(random() * 1000000)::int,
md5(g::text), floor(random()* (20000-9999 + 1) + 9999) 
FROM generate_series(1,1*1e6) g;
INSERT 0 1000000
percona=# CREATE INDEX idx_btree_bar ON foo.bar (id, dept, id2,id3,id4,id5,id6,zipcode);
CREATE INDEX
percona=# select * from foo.bar where id2 = 1 and id4 = 3;
 id | dept | id2 | id3 | id4 | id5 | id6 | id7 | details | zipcode 
----+------+-----+-----+-----+-----+-----+-----+---------+---------
(0 rows)
percona=# select * from foo.bar where id3 = 3;
   id   |  dept  |  id2   | id3 |  id4   | id5  |  id6   |  id7   |             details              | zipcode 
--------+--------+--------+-----+--------+------+--------+--------+----------------------------------+---------
 876920 | 723557 | 670210 |   3 | 321924 | 9512 | 183549 | 756279 | 3e780bae1aacbebc10b1e06ca49d226e |   16364
(1 row)</pre><p>Now, let us check if we can find the indexes that could improve the SQL COST of execution in reality. As I have mentioned earlier, none of the queries run by <pre class="crayon-plain-tag">sysbench-tpcc</pre> needed any further improvement through indexing, so we see the improvement only for the 2 select statements I ran above.</p><pre class="crayon-plain-tag">percona=# select find_usable_indexes();
 find_usable_indexes 
---------------------
 
(1 row)

percona=# select queryid, current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost' as "cost_without_index", 
hypo_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost' as "cost_with_index", 
round((((current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric-(hypo_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric)*100/(current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric),2) as percent_improvd 
FROM idx_recommendations order by 4 desc;
  queryid   | cost_without_index | cost_with_index | percent_improvd 
------------+--------------------+-----------------+-----------------
  612880084 | 27346.00           | 8.07            |           99.97
 1669974955 | 24846.00           | 12.08           |           99.95
 1539164311 | 9.35               | 9.35            |            0.00
 1976730265 | 15.37              | 15.37           |            0.00
 2041891134 | 8.32               | 8.32            |            0.00
 2750481779 | 8.31               | 8.31            |            0.00
  850567043 | 8.32               | 8.32            |            0.00
  946492786 | 8.32               | 8.32            |            0.00
 2618115206 | 9.35               | 9.35            |            0.00
  297872607 | 8.31               | 8.31            |            0.00
 1170842645 | 8.31               | 8.31            |            0.00
 1210386388 | 8.31               | 8.31            |            0.00
 1101690903 | 39.52              | 39.52           |            0.00
 4203200359 | 39.51              | 39.51           |            0.00
  192474146 | 2289.00            | 2289.00         |            0.00
  192474146 | 2289.00            | 2289.00         |            0.00
 3738483437 | 8.31               | 8.31            |            0.00
 1509701064 | 15.39              | 15.39           |            0.00
(18 rows)

percona=# select b.query, a.recmnded_index,round((((a.current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric-(hypo_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric)*100/(a.current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric),2) as percent_improvd FROM idx_recommendations a JOIN pg_stat_statements b ON a.queryid = b.queryid WHERE round((((current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric-(hypo_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric)*100/(current_plan-&gt;0-&gt;'Plan'-&gt;&gt;'Total Cost')::numeric),2) &gt; 0 order by 3 desc ;
                       query                       |                 recmnded_index                 | percent_improvd 
---------------------------------------------------+------------------------------------------------+-----------------
 select * from foo.bar where id2 = $1 and id4 = $2 | CREATE INDEX ON foo.bar USING btree (id2, id4) |           99.96
 select * from foo.bar where id3 = $1              | CREATE INDEX ON foo.bar USING btree (id3)      |           99.93
(2 rows)</pre><p>As this function is storing the results into a table: <pre class="crayon-plain-tag">public.idx_recommendations</pre>, we can query that and see the hypothetical index that has improved the total cost of that query.</p><pre class="crayon-plain-tag">percona=# select * from idx_recommendations WHERE queryid IN (612880084,1669974955);
-[ RECORD 1 ]--+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
queryid        | 1669974955
query          | 
current_plan   | [{"Plan": {"Alias": "bar", "Filter": "(id3 = 3)", "Node Type": "Seq Scan", "Plan Rows": 2, "Plan Width": 69, "Total Cost": 24846.00, "Startup Cost": 0.00, "Relation Name": "bar", "Parallel Aware": false}}]
recmnded_index | CREATE INDEX ON foo.bar USING btree (id3)
hypo_plan      | [{"Plan": {"Alias": "bar", "Node Type": "Index Scan", "Plan Rows": 2, "Index Cond": "(id3 = 3)", "Index Name": "&lt;18208&gt;btree_foo_bar_id3", "Plan Width": 69, "Total Cost": 12.08, "Startup Cost": 0.05, "Relation Name": "bar", "Parallel Aware": false, "Scan Direction": "Forward"}}]
-[ RECORD 2 ]--+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
queryid        | 612880084
query          | 
current_plan   | [{"Plan": {"Alias": "bar", "Filter": "((id2 = 1) AND (id4 = 3))", "Node Type": "Seq Scan", "Plan Rows": 1, "Plan Width": 69, "Total Cost": 27346.00, "Startup Cost": 0.00, "Relation Name": "bar", "Parallel Aware": false}}]
recmnded_index | CREATE INDEX ON foo.bar USING btree (id2, id4)
hypo_plan      | [{"Plan": {"Alias": "bar", "Node Type": "Index Scan", "Plan Rows": 1, "Index Cond": "((id2 = 1) AND (id4 = 3))", "Index Name": "&lt;18207&gt;btree_foo_bar_id2_id4", "Plan Width": 69, "Total Cost": 8.07, "Startup Cost": 0.05, "Relation Name": "bar", "Parallel Aware": false, "Scan Direction": "Forward"}}]</pre><p></p>
<h4><strong>Conclusion</strong></h4>
<p>With this experiment, we see that we can use <pre class="crayon-plain-tag">hypopg</pre> and <pre class="crayon-plain-tag">pg_qualstats</pre>  to automate index recommendations. The automation logic is currently limited to B-Tree Indexes only. Though it has a very negligible impact on performance through some minimalistic resource consumption, it can be considered by developers while coding an application logic. Developers could easily enable sampling for each query and see what indexes can be used to improve which query, and then implement the changes in Production. The function logic I have created above is just an experiment in automatic index recommendations and you may re-use the same upon additional testing. Special thanks again to <a target="_blank" href="https://github.com/powa-team/pg_qualstats/graphs/contributors" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">POWA Team</a> who have contributed to the very useful extension <pre class="crayon-plain-tag">pg_qualstats</pre> in PostgreSQL.</p>
<p><em>Let&#8217;s discuss this approach on <a target="_blank" href="https://news.ycombinator.com/item?id=20499437" target="_blank" rel="&quot;noopener&quot; noopener noreferrer">Hacker News</a>. </em></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/22/automatic-index-recommendations-in-postgresql-using-pg_qualstats-and-hypopg/#comments" thr:count="3"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/22/automatic-index-recommendations-in-postgresql-using-pg_qualstats-and-hypopg/feed/atom/" thr:count="3"/>
		<thr:total>3</thr:total>
			</entry>
		<entry>
		<author>
			<name>Emily Ikuta</name>
					</author>
		<title type="html"><![CDATA[Upcoming Webinar 7/23: 10 Common Mistakes Java Developers Make when Writing SQL]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/19/webinar-common-mistakes-java-developers-make-when-writing-sql/" />
		<id>https://www.percona.com/blog/?p=58346</id>
		<updated>2019-07-24T16:29:05Z</updated>
		<published>2019-07-19T14:48:39Z</published>
		<category scheme="https://www.percona.com/blog" term="Java" /><category scheme="https://www.percona.com/blog" term="Technical Webinars" /><category scheme="https://www.percona.com/blog" term="webinar" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Mistakes Java Developers Make when Writing SQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Please join Percona’s Senior Support Engineer Charly Batista as he presents “10 Common Mistakes (Java) Developers Make when Writing SQL” on Tuesday, July 23rd, 2019 at 8:00 AM EDT (UTC-4). Register Now It&#8217;s easy for Java developers (and users of other OO languages) to mix object-oriented thinking and imperative thinking. But when it comes to [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/19/webinar-common-mistakes-java-developers-make-when-writing-sql/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Mistakes Java Developers Make when Writing SQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Mistakes-Java-Developers-Make-when-Writing-SQL.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p>Please join Percona’s Senior Support Engineer Charly Batista as he presents “<a target="_blank" href="https://www.percona.com/resources/webinars/10-common-mistakes-java-developers-make-when-writing-sql">10 Common Mistakes (Java) Developers Make when Writing SQL</a>” on Tuesday, July 23rd, 2019 at 8:00 AM EDT (UTC-4).</p>
<p style="text-align: center;"><a target="_blank" class="btn btn-primary btn-lg" href="https://www.percona.com/resources/webinars/10-common-mistakes-java-developers-make-when-writing-sql" rel="noopener">Register Now</a></p>
<p>It&#8217;s easy for Java developers (and users of other OO languages) to mix object-oriented thinking and imperative thinking. But when it comes to writing SQL the nightmare begins! Firstly, SQL is a declarative language and it has nothing to do with either OO or imperative thinking. It is relatively easy to express a condition in SQL but it is not so easy to express it optimally &#8211; and even worse to translate it to the OO paradigm. Secondly, they need to think in terms of set and relational algebra, even if unconsciously!</p>
<p>In this talk, we&#8217;ll see the most common mistakes that developers make in OO, especially Java, when writing SQL code, and how we can avoid them.</p>
<p><a target="_blank" href="https://www.percona.com/resources/webinars/10-common-mistakes-java-developers-make-when-writing-sql">If you can&#8217;t attend, sign up anyways we&#8217;ll send you the slides and recording afterward</a>.</p>
<h5 class="field-item even">Speakers:</h5>
<div class="field field-name-body field-type-text-with-summary field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="field field-name-field-presenter field-type-entityreference field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="pagecontent row">
<div class="img col-md-2">
<div class="field field-name-field-picture field-type-image field-label-hidden">
<div class="field-items">
<div class="field-item even"><img class="alignleft" src="https://www.percona.com/live/19/sites/default/files/speakers_pics/573042.jpg" alt="Charly Batista " width="141" height="141" /></div>
</div>
</div>
</div>
<div class="col-md-10">
<div class="member-name">
<h4 class="mb-none">Charly Batista<br />
Senior support engineer</h4>
</div>
<div class="biography">
<div class="field field-name-body field-type-text-with-summary field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="col-md-10">
<div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
<div class="col-lg-10 col-md-10 col-sm-10 col-xs-12">
<p>Charly worked as Java Architect for many years and using many different database technologies. He helped to design some of the features of the system used in the Brazilian Postal Service, the largest Java project in Latin America in that time. He also helped to design the database of the Brazilian REDESIM project, the system that is responsible for the municipalities taxation in Brazil. He now lives in China and works as Senior Engineer at Percona.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/v2.js"></script><br />
<script>
  hbspt.forms.create({
	portalId: "758664",
	formId: "c178618e-7332-4db5-997a-391300511058",
	sfdcCampaignId: "70116000000oZ9pAAE"
});
</script></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/19/webinar-common-mistakes-java-developers-make-when-writing-sql/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/19/webinar-common-mistakes-java-developers-make-when-writing-sql/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Alexey Stroganov</name>
						<uri>http://www.percona.com</uri>
						</author>
		<title type="html"><![CDATA[Assessing MySQL Performance Amongst AWS Options &#8211; Part Two]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/" />
		<id>https://www.percona.com/blog/?p=58829</id>
		<updated>2019-07-19T13:38:31Z</updated>
		<published>2019-07-19T13:10:44Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="107" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-200x107.png" class="webfeedsFeaturedVisual wp-post-image" alt="Compare Amazon RDS to Percona Server" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-1024x547.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server.png 1296w" sizes="(max-width: 200px) 100vw, 200px" />See part one of this series here.  This post is part two of my series &#8220;Assessing MySQL Performance Amongst AWS Options&#8221;, taking a look at how current Amazon RDS services – Amazon Aurora and Amazon RDS for MySQL – compare with Percona Server with InnoDB and RocksDB engines on EC2 instances. This time around, I [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/"><![CDATA[<img width="200" height="107" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-200x107.png" class="webfeedsFeaturedVisual wp-post-image" alt="Compare Amazon RDS to Percona Server" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-1024x547.png 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Compare-Amazon-RDS-to-Percona-Server.png 1296w" sizes="(max-width: 200px) 100vw, 200px" /><p><em>See part one of this series <a target="_blank" href="https://www.percona.com/blog/2019/07/17/assessing-mysql-performance-amongst-aws-options-part-one/">here</a>. </em></p>
<p>This post is part two of my series &#8220;Assessing MySQL Performance Amongst AWS Options&#8221;, taking a look at how current Amazon RDS services – Amazon Aurora and Amazon RDS for MySQL – compare with Percona Server with InnoDB and RocksDB engines on EC2 instances. This time around, I am reviewing the total cost of one test run for each database as well as seeing which databases are the most efficient.</p>
<p>First, a quick recap of the evaluation scenario:</p>
<h3>The benchmark scripts</h3>
<p>For these evaluations, we use the <a target="_blank" href="https://github.com/Percona-Lab/sysbench-tpcc/blob/master/tpcc.lua">sysbench/tpcc</a> LUA test with a scale factor of 500 warehouses/10 tables. This is the equivalent of 5000 warehouses of the official TPC-C benchmark.</p>
<h3>Amazon MySQL Environments</h3>
<p>These are the AWS MySQL environments under analysis:</p>
<ul>
<li>Amazon RDS Aurora</li>
<li>Amazon RDS for MySQL with the InnoDB storage engine</li>
<li>Percona Server for MySQL with the InnoDB storage engine on Amazon EC2</li>
<li>Percona Server for MySQL with the RocksDB storage engine on Amazon EC2</li>
</ul>
<h3>Technical Setup &#8211; Server</h3>
<p>These general notes apply across the board:</p>
<ul>
<li>AWS region us-east-1(N.Virginia) was used for all tests</li>
<li>Server and client instances were spawned in the same availability zone</li>
<li>All data for tests were prepared in advance, stored as snapshots, and restored before the test</li>
<li>Encryption was not used</li>
</ul>
<p>And we believe that these configuration notes allow for a fair comparison of the different technologies:</p>
<ul>
<li>AWS EBS optimization was enabled for EC2 instances</li>
<li>For RDS/Amazon Aurora only a primary DB instance was created and used</li>
<li>In the case of RDS/MySQL, a single AZ deployment was used for RDS/MySQL</li>
<li>EC2/Percona Server for MySQL tests were run with binary log enabled</li>
</ul>
<p>Finally, here are the individual server configurations per environment:</p>
<h4>Server test #1: <strong>Amazon RDS Aurora</strong></h4>
<ul>
<li>Database server: Aurora MySQL 5.7</li>
<li>DB instances: r5.large, r5.xlarge, r5.2xlarge, r5.4xlarge</li>
<li>volume: used ~450GB(&gt;15000 IOPS)</li>
</ul>
<h4>Server test #2: <strong>Amazon RDS for MySQL with InnoDB Storage Engine</strong></h4>
<ul>
<li>Database server: MySQL Server 5.7.25</li>
<li>RDS instances: db.m5.large, db.m5.xlarge, db.m5.2xlarge, db.m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 700GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h4>Server test #3: <strong>Percona Server for MySQL with InnoDB Storage Engine</strong></h4>
<ul>
<li>Database server: Percona Server 5.7.25</li>
<li>EC2 instances: m5.large, m5.xlarge, m5.2xlarge, m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 700GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h4>Server test #4: <strong>Percona Server for MySQL with RocksDB using LZ4 compression</strong></h4>
<ul>
<li>Database server: Percona Server 5.7.25</li>
<li>EC2 instances: m5.large, m5.xlarge, m5.2xlarge, m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 350GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h3>Technical Setup &#8211; Client</h3>
<p>Common to all tests, we used an EC2 instance: m5.xlarge. And now that we have established the setup, let&#8217;s take a look at what we found.</p>
<h2>Costs</h2>
<p>Now we are getting down to the $&#8217;s! First, let&#8217;s review the total cost of one test run for each database:</p>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3.png"><img class="aligncenter size-large wp-image-58545" src="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3.png" alt="" width="650" height="560" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3.png 650w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3-174x150.png 174w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3-300x258.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run.v3-367x316.png 367w" sizes="(max-width: 650px) 100vw, 650px" /></a></p>
<p>Sorting the costs of one test run in order from cheapest to most expensive we see this order emerge:</p>
<ol>
<li>EC2/gp2 carrying server tests #3 or #4 featuring Percona Server for MySQL [represents the LEAST cost in $&#8217;s]</li>
<li>RDS/gp2 carrying server test #2, RDS/MySQL</li>
<li>EC2/io1 carrying server tests #3 or #4</li>
<li>RDS/io1 carrying server test #2, RDS/MySQL</li>
<li>RDS/Aurora, server test #1  [GREATEST COST IN $&#8217;s]</li>
</ol>
<p>How does that translate to $&#8217;s? Let&#8217;s find out how the structure of these costs looks like for every database. Before we study that, though, there are some things to bear in mind:</p>
<ul>
<li>Our calculations include only server-side costs</li>
<li>Per instance, the price we used as a baseline was <a target="_blank" href="https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/">RESERVED INSTANCE STANDARD 1-YEAR TERM</a></li>
<li>For RDS/Amazon Aurora the values for volume size and amount of I/O requests represent real data obtained from CloudWatch metrics (VolumeBytesUsed for used volume space and VolumeReadIOPs+VolumeWriteIOPs for IOPs used) after the test run</li>
<li>In the case of Percona Server/RocksDB due to LZ4 compression, the database on disk is 5x smaller, so we used a half-sized io1 volume – 350GB vs 700GB for either Percona Server with InnoDB or RDS/MySQL. This still complies with the requirement for io1 volumes to deliver 50 IOPS per GB.</li>
<li>The duration set for the test run is 30 mins</li>
</ul>
<h3>Our total cost formulas</h3>
<p>These are the formulas we used in calculating these costs:</p>
<ul>
<li>EC2/gp2, EC2/io1, RDS/gp2, RDS/io1
<ul>
<li><strong>total cost</strong> = <strong>server instance size cost</strong> + <strong>allocated volume size cost</strong> + <strong>requested amount of IOPS cost</strong></li>
</ul>
</li>
<li>RDS/Amazon Aurora
<ul>
<li><strong>total cost</strong> = <strong>server instance size cost</strong> + <strong>allocated volume size cost</strong> + <strong>actually used amount of I/O cost</strong></li>
</ul>
</li>
</ul>
<h3>The results</h3>
<p>Here are our calculations in chart form, you can click on the chart to enlarge it on screen:</p>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run_detailed.v6.png"><img class="aligncenter size-large wp-image-58543" src="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_4.bar_.costs_1run_detailed.v6.png" alt="" width="650" height="800" /></a></p>
<p>One interesting observation here is that, as you can see from the costs structure chart, the most significant part of costs is IO provisioning – either the <strong>requested amount of IOPS </strong>(EC2/io1 or RDS/io1) or the <strong>actually used amount of IOPS </strong>(RDS/Aurora). In the former case, the cost is a function of time, and in the latter case, costs depend only on the amount of I/O requests actually issued.</p>
<p>Let&#8217;s check how these costs might look like if we provision EC2/io1, RDS/io1 volumes and RDS/aurora storage for one month. From the cost structure, it&#8217;s clear that in case of RDS/aurora 4xlarge &#8211; db instance performed 51M I/O requests for half an hour. So we effectively got <code>51000000 (I/O request) / 1800(seconds) ~= 28000 IOPs</code>.</p><pre class="crayon-plain-tag">EC2/io1:    (28000 (IOPS)      * 0.065(IOPs price)    * 24(hours)*30(days)/(24(hours)*30(days))   1820$
RDS/io1:    (28000 (IOPS)      *   0.1(IOPs price)    * 24(hours)*30(days)/(24(hours)*30(days))   2800$
RDS/aurora: 102M(I/O per hour) *   0.2(I/O req price) * 24(hours)*30(days)                       14688$</pre><p>In this way, IO provisioning of 28000 IOPS for EC2/io1 costs 8x less and for RDS/io1 costs 5x less. That means that to be cost-efficient, the throughput of RDS/Aurora should be at least 5x or even 8x better than that of EC2 or RDS with io1 volume.</p>
<p><strong>Conclusion:</strong> the IO provisioning factor should be taken into account during your planning of deployments with io1 volumes or RDS/aurora</p>
<h2>Efficiency</h2>
<p>Now it&#8217;s time to review which databases perform the most efficiently by analyzing their transaction/cost ratio:</p>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3.png"><img class="aligncenter size-large wp-image-58544" src="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3.png" alt="" width="650" height="560" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3.png 650w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3-174x150.png 174w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3-300x258.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_5.bar_.costs_1000trx.v3-367x316.png 367w" sizes="(max-width: 650px) 100vw, 650px" /></a></p>
<p>Below you can find the minimum and maximum prices for 1000 transactions for each of the database servers in our tests, again running from cheapest to most expensive in $ terms:</p>
<table>
<tbody>
<tr>
<th>Server</th>
<th>Min $&#8217;s per 1000 TX</th>
<th>Server Config</th>
<th>Min $&#8217;s per 1000 TX</th>
<th>Server Config</th>
</tr>
<tr>
<td>Server test #4 EC2#Percona Server/RocksDB</td>
<td>0.42</td>
<td>4xlarge/io1</td>
<td>1.93</td>
<td>large/io1</td>
</tr>
<tr>
<td>Server test #3 EC2#Percona Server/InnoDB</td>
<td>1.66</td>
<td>4xlarge/gp2</td>
<td>12.11</td>
<td>large/io1</td>
</tr>
<tr>
<td>Server test #2 RDS#MySQL/InnoDB</td>
<td>2.23</td>
<td>4xlarge/gp2</td>
<td>22.3</td>
<td>large/io1</td>
</tr>
<tr>
<td>Server test #1 RDS#Amazon Aurora</td>
<td>8.29</td>
<td>4xlarge</td>
<td>13.31</td>
<td>xlarge</td>
</tr>
</tbody>
</table>
<h2>Some concluding thoughts</h2>
<ul>
<li><strong>EC2#Percona Server/RocksDB</strong> offers the lowest price per 1000 transactions – $0.42 on m5.4xlarge instance with 350GB io1 volume/15000 IOPs</li>
<li><strong>RDS/MySQL</strong> looked to be the most expensive in this evaluation – $22.3 for 1000 transactions &#8211; db.m5.large with 700GB io1 volume/15000 IOPs</li>
<li>Lowest price for each database was obtained on 4xlarge instances, most expensive on large instances.</li>
<li>IO provisioning is a key factor that impacts run costs</li>
<li>For both EC2 and RDS gp2/5400GB (~16000 IOPS) is the cost wise choice</li>
<li>RDS/Aurora – the lowest price per 1000 transactions is $8.29, but that is 4x more expensive than the best price of 1000 transactions for RDS/MySQL, 5x more expensive than for EC2#Percona/InnoDB, and 20x more expensive than for EC2#Percona/RockDB. That means that despite the fact that Amazon Aurora shows very good throughput (actually the best among InnoDB-like engines), it may not be as cost-effective as other options.</li>
</ul>
<h2>One Final Note</h2>
<p>When estimating your expenses, you will need to keep in mind that each company is different in terms of what they offer, how they build and manage those offerings, and of course, their pricing structure and cost per transaction. For AWS, you do need to be aware of the expenses of building and managing those things yourself that AWS handles for you; i.e. built into their cost. We can see, however, that in these examples, MyRocks is definitely a cost-effective solution when comparing direct costs.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Fernando Laudares Camargos</name>
					</author>
		<title type="html"><![CDATA[Resolving MongoDB Stack Traces]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/18/resolving-mongodb-stack-trace/" />
		<id>https://www.percona.com/blog/?p=59110</id>
		<updated>2019-07-19T20:05:48Z</updated>
		<published>2019-07-18T14:29:42Z</published>
		<category scheme="https://www.percona.com/blog" term="MongoDB" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Stack Traces" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />When a MongoDB server crashes you will usually find what is called a &#8220;stack trace&#8221; in its log file. But what is it and what purpose does it have? Let&#8217;s simulate a simple crash so we can dig into it. Crashing a test server In a test setup with a freshly installed MongoDB server, we [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/18/resolving-mongodb-stack-trace/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Stack Traces" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59183" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg" alt="MongoDB Stack Traces" width="200" height="133" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Stack-Traces.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />When a MongoDB server crashes you will usually find what is called a &#8220;stack trace&#8221; in its log file. But what is it and what purpose does it have? Let&#8217;s simulate a simple crash so we can dig into it.</p>
<h2>Crashing a test server</h2>
<p>In a test setup with a freshly installed MongoDB server, we connect to it and create some test data:</p><pre class="crayon-plain-tag">$ mongo
MongoDB shell version v3.6.12
(...)
&gt; use test
switched to db test
&gt; db.albums.insert({ name: "The Wall" })
WriteResult({ "nInserted" : 1 })
&gt; db.albums.find()
{ "_id" : ObjectId("5d237cef9affce6d7e4e8345"), "name" : "The Wall" }</pre><p><span style="font-weight: 400;">On a separate connection to the server, we change the ownership of the MongoDB data files, so the </span><i><span style="font-weight: 400;">mongod</span></i><span style="font-weight: 400;"> user will no longer have access to them:</span></p><pre class="crayon-plain-tag">$ sudo chown root:root /var/lib/mongo/*</pre><p><span style="font-weight: 400;">Going back to the mongo session, we try to add a new record and it fails, as expected:</span></p><pre class="crayon-plain-tag">&gt; db.albums.insert({ name: "The Division Bell" })
2019-07-08T17:27:40.275+0000 E QUERY    [thread1] Error: error doing query: failed: network error while attempting to run command 'insert' on host '127.0.0.1:27017'  :
DB.prototype.runCommand@src/mongo/shell/db.js:168:1
DBCollection.prototype._dbCommand@src/mongo/shell/collection.js:173:1
Bulk/executeBatch@src/mongo/shell/bulk_api.js:903:22
Bulk/this.execute@src/mongo/shell/bulk_api.js:1154:21
DBCollection.prototype.insert@src/mongo/shell/collection.js:317:22
@(shell):1:1
2019-07-08T17:27:40.284+0000 I NETWORK  [thread1] trying reconnect to 127.0.0.1:27017 (127.0.0.1) failed
2019-07-08T17:27:40.284+0000 W NETWORK  [thread1] Failed to connect to 127.0.0.1:27017, in(checking socket for error after poll), reason: Connection refused
2019-07-08T17:27:40.284+0000 I NETWORK  [thread1] reconnect 127.0.0.1:27017 (127.0.0.1) failed failed</pre><p>Looking at the error log we confirm the server has crashed, leaving a stack trace (also called a &#8220;backtrace&#8221;) behind:</p><pre class="crayon-plain-tag">$ sudo cat /var/log/mongodb/mongod.log 
(...)
2019-07-08T17:27:39.666+0000 E STORAGE  [thread2] WiredTiger error (13) [1562606859:666004][24742:0x7f70a3501700], log-server: __directory_list_worker, 48: /home/vagrant/db/journal: directory-list: opendir: Permission denied
(...)
2019-07-08T17:27:39.666+0000 E STORAGE  [thread2] WiredTiger error (-31804) [1562606859:666313][24742:0x7f70a3501700], log-server: __wt_panic, 523: the process must exit and restart: WT_PANIC: WiredTiger library panic
(...)
----- BEGIN BACKTRACE -----
(...)
 mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x5618a3ab92c1]
 mongod(+0x22744D9) [0x5618a3ab84d9]
 mongod(+0x22749BD) [0x5618a3ab89bd]
 libpthread.so.0(+0xF6D0) [0x7f70a6cff6d0]
 libc.so.6(gsignal+0x37) [0x7f70a6959277]
 libc.so.6(abort+0x148) [0x7f70a695a968]
 mongod(_ZN5mongo32fassertFailedNoTraceWithLocationEiPKcj+0x0) [0x5618a21e064c]
 mongod(+0xA6D9EE) [0x5618a22b19ee]
 mongod(+0xADEEF1) [0x5618a2322ef1]
 mongod(__wt_err_func+0x90) [0x5618a217b742]
 mongod(__wt_panic+0x3F) [0x5618a217bb62]
 mongod(+0xB3DFB2) [0x5618a2381fb2]
 libpthread.so.0(+0x7E25) [0x7f70a6cf7e25]
 libc.so.6(clone+0x6D) [0x7f70a6a21bad]
-----  END BACKTRACE  -----
Aborted</pre><p>But what can we infer from these somewhat cryptic lines full of hexadecimal content?</p>
<h2>Inspecting the MongoDB stack trace</h2>
<p>In the bottom of the stack trace, we can see a list of function names and addresses. Note the resolution of most functions worked reasonably well in the example above; the <i>mongod</i> binary used by our test server is not <i>stripped</i> of symbols (if yours is you will need to install the respective <i>debugsymbols</i>/<i>debuginfo</i> package and use the <i>mongod</i> binary provided by it to resolve the stack trace):</p><pre class="crayon-plain-tag">$ file `which mongod`
/usr/bin/mongod: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=1d0fd59529274e06c35e6dc4c74e0ef08caf931c, not stripped</pre><p>This means we can actually extract them from the <i>mongod</i> binary with the help of a tool such as <i>nm</i>, from <a target="_blank" href="https://www.gnu.org/software/m68hc11/m68hc11_doc.html" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">GNU Development Tools</a>:</p><pre class="crayon-plain-tag">$ nm -n /usr/bin/mongod &gt; mongod.symbols</pre><p><span style="font-weight: 400;">Function names appear all mangled though:</span></p><pre class="crayon-plain-tag">$ tail mongod.symbols 
0000000003125f88 u _ZNSt9money_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE2idE
0000000003125f90 u _ZNSt10moneypunctIwLb1EE2idE
0000000003125f98 u _ZNSt10moneypunctIwLb0EE2idE
0000000003125fa0 b _ZZN9__gnu_cxx27__verbose_terminate_handlerEvE11terminating
0000000003125fc0 b _ZZN12_GLOBAL__N_112get_catalogsEvE10__catalogs
0000000003126008 b _ZGVZN12_GLOBAL__N_112get_catalogsEvE10__catalogs
0000000003126020 b _ZZN12_GLOBAL__N_112get_catalogsEvE10__catalogs
0000000003126068 b _ZGVZN12_GLOBAL__N_112get_catalogsEvE10__catalogs
0000000003126080 B __wt_process
00000000031260e8 A _end</pre><p>We can use another tool from that same toolkit, <i>c++filt</i>, to get them straightened, for example:</p><pre class="crayon-plain-tag">$ echo _ZNSt9money_getIwSt19istreambuf_iteratorIwSt11char_traitsIwEEE2idE | c++filt 
std::money_get&lt;wchar_t, std::istreambuf_iterator&lt;wchar_t, std::char_traits&lt;wchar_t&gt; &gt; &gt;::id</pre><p>In fact, we can process the whole stack trace with <i>c++filt</i> all at once…</p><pre class="crayon-plain-tag">$ cat &lt;&lt;EOT | c++filt
&gt; mongod(_ZN5mongo15printStackTraceERSo+0x41) [0x5618a3ab92c1]
&gt; mongod(+0x22744D9) [0x5618a3ab84d9]
&gt; mongod(+0x22749BD) [0x5618a3ab89bd]
&gt; libpthread.so.0(+0xF6D0) [0x7f70a6cff6d0]
&gt; libc.so.6(gsignal+0x37) [0x7f70a6959277]
&gt; libc.so.6(abort+0x148) [0x7f70a695a968]
&gt; mongod(_ZN5mongo32fassertFailedNoTraceWithLocationEiPKcj+0x0) [0x5618a21e064c]
&gt; mongod(+0xA6D9EE) [0x5618a22b19ee]
&gt; mongod(+0xADEEF1) [0x5618a2322ef1]
&gt; mongod(__wt_err_func+0x90) [0x5618a217b742]
&gt; mongod(__wt_panic+0x3F) [0x5618a217bb62]
&gt; mongod(+0xB3DFB2) [0x5618a2381fb2]
&gt; libpthread.so.0(+0x7E25) [0x7f70a6cf7e25]
&gt; libc.so.6(clone+0x6D) [0x7f70a6a21bad]
&gt; EOT</pre><p>… and get it fully demangled, with C++ function and method names easily recognizable now:</p><pre class="crayon-plain-tag">mongod(mongo::printStackTrace(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)+0x41) [0x5618a3ab92c1]
mongod(+0x22744D9) [0x5618a3ab84d9]
mongod(+0x22749BD) [0x5618a3ab89bd]
libpthread.so.0(+0xF6D0) [0x7f70a6cff6d0]
libc.so.6(gsignal+0x37) [0x7f70a6959277]
libc.so.6(abort+0x148) [0x7f70a695a968]
mongod(mongo::fassertFailedNoTraceWithLocation(int, char const*, unsigned int)+0x0) [0x5618a21e064c]
mongod(+0xA6D9EE) [0x5618a22b19ee]
mongod(+0xADEEF1) [0x5618a2322ef1]
mongod(__wt_err_func+0x90) [0x5618a217b742]
mongod(__wt_panic+0x3F) [0x5618a217bb62]
mongod(+0xB3DFB2) [0x5618a2381fb2]
libpthread.so.0(+0x7E25) [0x7f70a6cf7e25]
libc.so.6(clone+0x6D) [0x7f70a6a21bad]</pre><p>While easily reproducible, this was not a very interesting example: the change in the database files ownership caused WiredTiger to crash upon the insert without leaving much trace behind. Let&#8217;s have a look at another one.</p>
<h2>A more realistic example</h2>
<p>Despite being somewhat old, bug <a target="_blank" href="https://jira.mongodb.org/browse/SERVER-13751" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">SERVER-13751</a> (<i>mongod crash on geo nearSphere query</i>) provides a realistic yet easy to reproduce example of a simple routine that crashed MongoDB 2.6.0 (this bug is, in fact, a duplicate of <a target="_blank" href="https://jira.mongodb.org/browse/SERVER-13666" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">SERVER-13666</a>, but it provides a simpler test case). Here&#8217;s how to get to it.</p>
<p>1) First, we download these old binaries and start a MongoDB server:</p><pre class="crayon-plain-tag">$ wget http://downloads.mongodb.org/linux/mongodb-linux-x86_64-2.6.0.tgz
$ tar zxvf mongodb-linux-x86_64-2.6.0.tgz
$ cd mongodb-linux-x86_64-2.6.0/bin
$ mkdir /home/vagrant/db
$ ./mongod --dbpath /home/vagrant/db</pre><p>2) In a second terminal window, we connect to the MongoDB server we just started and run a more simplified version of the routine described in the bug, which consists of creating a 2dsphere index and querying for a point described with invalid coordinates:</p><pre class="crayon-plain-tag">$ cd mongodb-linux-x86_64-2.6.0/bin
$ ./mongo
&gt; db.places.ensureIndex({loc:"2dsphere"})
&gt; db.places.find({loc:{$nearSphere: [200.4905, 300.2646]}})</pre><p><span style="font-weight: 400;">Now when we look back at the first terminal we find the server has crashed, leaving the following stack trace:</span></p><pre class="crayon-plain-tag">./mongod(_ZN5mongo15printStackTraceERSo+0x21) [0x11bd301]
./mongod() [0x11bc6de]
/lib64/libc.so.6(+0x36340) [0x7f11cd866340]
/lib64/libc.so.6(gsignal+0x37) [0x7f11cd8662c7]
/lib64/libc.so.6(abort+0x148) [0x7f11cd8679b8]
./mongod(_ZN5mongo13fassertFailedEi+0x13a) [0x11421ea]
./mongod(_ZN15LogMessageFatalD1Ev+0x1d) [0x125d58d]
./mongod(_ZN5S2Cap13FromAxisAngleERK7Vector3IdERK7S1Angle+0x169) [0x1267699]
./mongod(_ZN5mongo11S2NearStage11nextAnnulusEv+0xd2) [0xabd142]
./mongod(_ZN5mongo11S2NearStage4workEPm+0x1fb) [0xabf2cb]
./mongod(_ZN5mongo12PlanExecutor7getNextEPNS_7BSONObjEPNS_7DiskLocE+0xef) [0xd66a7f]
./mongod(_ZN5mongo11newRunQueryERNS_7MessageERNS_12QueryMessageERNS_5CurOpES1_+0x958) [0xd4acf8]
./mongod() [0xb96382]
./mongod(_ZN5mongo16assembleResponseERNS_7MessageERNS_10DbResponseERKNS_11HostAndPortE+0x442) [0xb98962]
./mongod(_ZN5mongo16MyMessageHandler7processERNS_7MessageEPNS_21AbstractMessagingPortEPNS_9LastErrorE+0x9f) [0x76b76f]
./mongod(_ZN5mongo17PortMessageServer17handleIncomingMsgEPv+0x4fb) [0x117367b]
/lib64/libpthread.so.0(+0x7dd5) [0x7f11ce62bdd5]
/lib64/libc.so.6(clone+0x6d) [0x7f11cd92e02d]</pre><p>Processing  the stack trace with <i>c++filt</i> we get:</p><pre class="crayon-plain-tag">./mongod(mongo::printStackTrace(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)+0x21) [0x11bd301]
./mongod() [0x11bc6de]
/lib64/libc.so.6(+0x36340) [0x7f11cd866340]
/lib64/libc.so.6(gsignal+0x37) [0x7f11cd8662c7]
/lib64/libc.so.6(abort+0x148) [0x7f11cd8679b8]
./mongod(mongo::fassertFailed(int)+0x13a) [0x11421ea]
./mongod(LogMessageFatal::~LogMessageFatal()+0x1d) [0x125d58d]
./mongod(S2Cap::FromAxisAngle(Vector3&lt;double&gt; const&amp;, S1Angle const&amp;)+0x169) [0x1267699]
./mongod(mongo::S2NearStage::nextAnnulus()+0xd2) [0xabd142]
./mongod(mongo::S2NearStage::work(unsigned long*)+0x1fb) [0xabf2cb]
./mongod(mongo::PlanExecutor::getNext(mongo::BSONObj*, mongo::DiskLoc*)+0xef) [0xd66a7f]
./mongod(mongo::newRunQuery(mongo::Message&amp;, mongo::QueryMessage&amp;, mongo::CurOp&amp;, mongo::Message&amp;)+0x958) [0xd4acf8]
./mongod() [0xb96382]
./mongod(mongo::assembleResponse(mongo::Message&amp;, mongo::DbResponse&amp;, mongo::HostAndPort const&amp;)+0x442) [0xb98962]
./mongod(mongo::MyMessageHandler::process(mongo::Message&amp;, mongo::AbstractMessagingPort*, mongo::LastError*)+0x9f) [0x76b76f]
./mongod(mongo::PortMessageServer::handleIncomingMsg(void*)+0x4fb) [0x117367b]
/lib64/libpthread.so.0(+0x7dd5) [0x7f11ce62bdd5]
/lib64/libc.so.6(clone+0x6d) [0x7f11cd92e02d]</pre><p>This particular <i>mongod</i> binary is stripped of symbols:</p><pre class="crayon-plain-tag">$ file mongod
mongod: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.9, stripped</pre><p>So in order to resolve the stack trace, we need to first obtain one that is not:</p><pre class="crayon-plain-tag">$ wget http://downloads.mongodb.org/linux/mongodb-linux-x86_64-debugsymbols-2.6.0.tgz
$ tar zxvf mongodb-linux-x86_64-debugsymbols-2.6.0.tgz
$ cd mongodb-linux-x86_64-debugsymbols-2.6.0/bin</pre><p>We can now extract the actual function names from the addresses between the brackets using <a target="_blank" href="https://sourceware.org/binutils/docs/binutils/addr2line.html" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><i>addr2line</i></a> (option &#8220;f&#8221; provides the function name and if we also use &#8220;i&#8221; we get the preceding ones as well if the main one was inline; option &#8220;C&#8221; provides some extent of demangling, similar to <i>c++filt</i>):</p><pre class="crayon-plain-tag">$ addr2line -e mongod -ifC 0x1267699
S2Cap::FromAxisAngle(Vector3&lt;double&gt; const&amp;, S1Angle const&amp;)
/srv/10gen/mci-exec/mci/git@github.commongodb/mongo.git/mongodb-mongo-v2.6/src/third_party/s2/s2cap.cc:35</pre><p>One of the greatest values of working with Open Source software is being able to have a direct look at this exact piece of code, which translates to <a target="_blank" href="https://github.com/mongodb/mongo/blob/v2.6/src/third_party/s2/s2cap.cc#L35" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">https://github.com/mongodb/mongo/blob/v2.6/src/third_party/s2/s2cap.cc#L35</a> :</p><pre class="crayon-plain-tag">S2Cap S2Cap::FromAxisAngle(S2Point const&amp; axis, S1Angle const&amp; angle) {
  DCHECK(S2::IsUnitLength(axis));
  DCHECK_GE(angle.radians(), 0);
  return S2Cap(axis, GetHeightForAngle(angle.radians()));
}</pre><p>Note that the actual <a target="_blank" href="https://github.com/mongodb/mongo/commit/3286b208b30c43450354422e554070485f526e7b" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">fix</a> for this bug didn&#8217;t come from modifying this function, which is being re-used from a third-party (another beauty of working with Open Source!), but in making sure the arguments that are being passed to it (which compose the point&#8217;s coordinates) are validated beforehand.</p>
<h2>There is a home for bugs</h2>
<p>If you ever run into a MongoDB server crash, I hope this little set of instructions can serve as a reference in helping you make sense of the stack trace that will (hopefully) have been left behind. You can then search for bugs at <a target="_blank" href="https://jira.mongodb.org/" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">https://jira.mongodb.org</a> if you&#8217;re running a MongoDB server, or at <a target="_blank" href="https://jira.percona.com/projects/PSMDB" target="_blank" rel="&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">https://jira.percona.com/projects/PSMDB</a> if you&#8217;re running <a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Percona Server for MongoDB</a>. If you can&#8217;t find a bug that matches your crash, please consider filing a new one; providing a clear stack trace alongside the exact binary version you&#8217;re using is a must. If you are able to reproduce the problem at will and can provide a reproducible test case as well, like the ones we showed above, that will not only make the life of our developers easier, it also increases the likelihood of getting the bug fixed much, much faster.</p>
<p><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Learn more about Percona Server for MongoDB</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/18/resolving-mongodb-stack-trace/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/18/resolving-mongodb-stack-trace/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>Corrado Pandiani</name>
					</author>
		<title type="html"><![CDATA[MySQL: Disk Space Exhaustion for Implicit Temporary Tables]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/17/mysql-disk-space-exhaustion-for-implicit-temporary-tables/" />
		<id>https://www.percona.com/blog/?p=58968</id>
		<updated>2019-07-19T13:38:54Z</updated>
		<published>2019-07-17T15:42:34Z</published>
		<category scheme="https://www.percona.com/blog" term="Insight for DBAs" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona Server for MySQL" />		<summary type="html"><![CDATA[<img width="200" height="134" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Implicit Temporary Tables" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-1024x684.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />I was recently faced with a real issue about completely exhausting the disk space on MySQL. This was a serious issue because of the continuous outages of the service, as the customer had to constantly restart the server and wait for the next outage. What was happening? In this article, I&#8217;m going to explain it [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/17/mysql-disk-space-exhaustion-for-implicit-temporary-tables/"><![CDATA[<img width="200" height="134" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Implicit Temporary Tables" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-1024x684.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59002" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg" alt="Implicit Temporary Tables" width="200" height="134" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-200x134.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-1024x684.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Implicit-Temporary-Tables.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />I was recently faced with a real issue about completely exhausting the disk space on MySQL. This was a serious issue because of the continuous outages of the service, as the customer had to constantly restart the server and wait for the next outage.</p>
<p>What was happening? In this article, I&#8217;m going to explain it and propose solutions.</p>
<h2>Implicit temporary tables</h2>
<p>MySQL needs to create implicit temporary tables for solving some kinds of queries. The queries that require a sorting stage most of the time need to rely on a temporary table. For example, when you use <strong>GROUP BY</strong>, <strong>ORDER BY</strong> or <strong>DISTINCT.</strong>  Such queries are executed in two stages: the first is to gather the data and put them into a temporary table, the second is to execute the sorting on the temporary table.</p>
<p>A temporary table is also needed in case of some <strong>UNION</strong> statements evaluation, for <strong>VIEW </strong>that cannot use merge, for derived tables when using subqueries, for <strong>multiple-table UPDATE</strong>, and some other cases.</p>
<p>If the temporary table is small it can be created into the memory, otherwise, it&#8217;s created on the disk. Needless to say that an in-memory temporary table is faster. MySQL creates an in-memory table, and if it becomes too large it is converted to an on-disk table. The maximum size for in-memory temporary tables is defined by the <code class="literal">tmp_table_size</code> or <code class="literal">max_heap_table_size</code> value, whichever is smaller. The default size in MySQL 5.7 is 16MB. If you run queries on a large amount of data, or if you have not optimized queries, you can increase the variables. When setting the threshold, take into consideration the available RAM and the number of concurrent connections you have during your peaks. You cannot <span class="tlid-translation translation" lang="en"><span class="" title="">indefinitely </span></span>increase the variables, as at some point you&#8217;ll need to let MySQL use on-disk temporary tables.</p>
<p><em>Note: the temporary table is created on-disk if the tables involved have TEXT or BLOB columns, even if the size is less than the configured threshold.</em></p>
<h2>Temporary tables storage engine</h2>
<p>Until MySQL 5.6, all the on-disk temporary tables are created as MyISAM. The temporary table is created in-memory or on-disk, depending on the configuration, and it&#8217;s dropped immediately at the end of the query. From MySQL 5.7, they are created as InnoDB by default. Then you can rely on the advanced features.</p>
<p>The new default is the best option for the overall performance and should be used in the majority of the cases.</p>
<p>A new configuration variable is available to set the storage engine for the temporary tables:  <code class="literal">internal_tmp_disk_storage_engine</code>. The variable can be set to  <code class="literal">innodb</code> (default if not set) or <code class="literal">myisam</code>.</p>
<h2>The potential problem with InnoDB temporary tables</h2>
<p>Although using InnoDB is the best for performance, a new potential issue could arise. In some particular cases, you can have disk exhaustion and server outage.</p>
<p>As any other InnoDB table in the database, the temporary tables have their own tablespace file. The new file is in the data directory together with the general tablespace, with the name <code class="literal">ibtmp1</code>. It stores all the tmp tables. A tablespace file cannot be shrunk, and it grows constantly as long as you don&#8217;t run a manual OPTIMIZE TABLE. The <em>ibtmp1</em> makes no difference, as you cannot use OPTIMIZE. The only way to shrink <em>ibtmp1</em> size to zero is to restart the server.</p>
<p>Fortunately, even if the file cannot shrink, after the execution of a query the temporary table is automatically dropped and the space in the tablespace can be reused for another incoming query.</p>
<p>Let&#8217;s think now about the following case:</p>
<ul>
<li>you have non-optimized queries that require the creation of very large on-disk tmp tables</li>
<li>you have optimized queries, but they are creating very large on-disk tmp tables because you are doing in purpose computation on a very large dataset (statistics, analytics)</li>
<li>you have a lot of concurrent connections running the same queries with tmp table creation</li>
<li>you don&#8217;t have a lot of free space in your volume</li>
</ul>
<p>In such a situation it&#8217;s easy to understand that the file ibtmp1 size can increase considerably and the file can easily exhaust the free space. This was happening several times a day, and the server had to be restarted in order to completely shrink the <em>ibtmp1</em> tablespace.</p>
<p>It&#8217;s not mandatory that the concurrent queries are launched exactly at the same time. Since a query with a large temporary table will take several seconds or minutes to execute, it is sufficient to have queries launched at different times while the preceding ones are still running. Also, you have to consider that any connection creates its own temporary table, so the same exact query will create another exact copy of the same temporary table into the tablespace. Exhausting the disk space is very easy with non-shrinkable files!</p>
<p>So, what to do to avoid disk exhaustion and outages?</p>
<h2>The trivial solution: use a larger disk</h2>
<p>This is really trivial and can solve the problem, but it is not the optimal solution. In fact, it&#8217;s not so easy to figure out what your new disk size should be. You can guess by increasing the disk size step by step, which is quite easy to do if your environment is in the cloud or you have virtual appliances on a very large platform. But it&#8217;s not easy to do in on-premise environments.</p>
<p>But with this solution, you can risk having unneeded expenses, so keep that in mind.</p>
<p>You can also move the <em>ibtmp1</em> file on a dedicated large disk, by setting the following configuration variable:</p><pre class="crayon-plain-tag">[mysqld]
innodb_temp_data_file_path = ../../tmp/ibtmp1:12M:autoextend</pre><p>A MySQL restart is required.</p>
<p>Please note that the path has to be specified as relative to the data directory.</p>
<h2>Set an upper limit for ibtmp1 size</h2>
<p>For example:</p><pre class="crayon-plain-tag">[mysqld]
innodb_temp_data_file_path = ibtmp1:12M:autoextend:max:10G</pre><p>In this case, the file cannot grow more than 10GB. You can easily eliminate the outages, but this is a dangerous solution. When the data file reaches the maximum size, queries fail with an error indicating that the table is full. This is probably bad for your applications.</p>
<h2>Step back to MyISAM for on-disk temporary tables</h2>
<p>This solution seems to be <span class="tlid-translation translation" lang="en"><span class="" title="">counterintuitive but it</span></span> could be the best way to avoid the outages in a matter of seconds and is guaranteed to use all needed temporary tables.</p>
<p>You can set the following variable into <em>my.cnf</em>:</p><pre class="crayon-plain-tag">internal_tmp_disk_storage_engine = MYISAM</pre><p>Since the variable is dynamic you can set it also at runtime:</p><pre class="crayon-plain-tag">SET GLOBAL internal_tmp_disk_storage_engine = MYISAM;</pre><p>Stepping back to MyISAM, you will considerably decrease the possibility of completely filling your disk space. In fact, the temporary tables will be created into different files and immediately dropped at the end of the query. No more issues about a forever increasing file.</p>
<p>And while there is always the possibility to see the same issue, just in case you can run the queries at the exact same time or really very close. In my real-world case, this was the solution to avoid all the outages.</p>
<h2>Optimize your queries</h2>
<p>This is the most important thing to do. After stepping back the storage engine to MyISAM to mitigate the outage <span class="tlid-translation translation" lang="en"><span class="" title="">occurrences,</span></span> you have to absolutely take the time to analyze the queries.</p>
<p>The goal is to decrease the size of the on-disk temporary tables. It&#8217;s not the aim of this article to explain how to investigate the queries, but you can rely on the slow log, on a tool like <a target="_blank" href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html">pt-query-digest</a> and on EXPLAIN.</p>
<p>Some tips:</p>
<ul>
<li>create missing indexes on the tables</li>
<li>add more filters in the queries to gather less data, if you don&#8217;t really need it</li>
<li>rewrite queries to optimize the execution plan</li>
<li>if you have very large queries on purpose, you can use a queue manager in your applications to serialize their executions or to decrease the concurrency</li>
</ul>
<p>This will be the longest activity, but hopefully, after all the optimizations, you can return to set the temporary storage engine to InnoDB for better performance.</p>
<h2>Conclusion</h2>
<p>Sometimes the improvements can have unexpected side effects. The InnoDB storage engine for on-disk temporary tables is a good improvement, but in some particular cases, for example, if you have non-optimized queries and little free space, you can have outages because of &#8220;disk full&#8221; error. Stepping back the tmp storage engine to MyISAM is the fastest way to avoid outages, but the optimization of the queries is the more important thing to do as soon as possible in order to return to InnoDB. And yes, even a larger or dedicated disk may help. It&#8217;s a trivial suggestion, I know, but it can <span class="tlid-translation translation" lang="en"><span class="" title="">definitely</span></span> help a lot.</p>
<p>By the way, there&#8217;s a feature request about the issue: <a target="_blank" href="https://bugs.mysql.com/bug.php?id=82556" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer">https://bugs.mysql.com/bug.php?id=82556</a></p>
<p>Further readings:<br />
<a target="_blank" href="https://www.percona.com/blog/2007/01/19/tmp_table_size-and-max_heap_table_size/">https://www.percona.com/blog/2007/01/19/tmp_table_size-and-max_heap_table_size/</a><br />
<a target="_blank" href="https://www.percona.com/blog/2017/12/04/internal-temporary-tables-mysql-5-7/">https://www.percona.com/blog/2017/12/04/internal-temporary-tables-mysql-5-7/<br />
</a><a target="_blank" href="http://mysqlserverteam.com/mysql-5-7-innodb-intrinsic-tables/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer">http://mysqlserverteam.com/mysql-5-7-innodb-intrinsic-tables/</a><br />
<a target="_blank" href="https://dev.mysql.com/doc/refman/5.7/en/internal-temporary-tables.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer" noopener noreferrer">https://dev.mysql.com/doc/refman/5.7/en/internal-temporary-tables.html</a></p>
<p>&nbsp;</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/17/mysql-disk-space-exhaustion-for-implicit-temporary-tables/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/17/mysql-disk-space-exhaustion-for-implicit-temporary-tables/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>Alexey Stroganov</name>
						<uri>http://www.percona.com</uri>
						</author>
		<title type="html"><![CDATA[Assessing MySQL Performance Amongst AWS Options &#8211; Part One]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/17/assessing-mysql-performance-amongst-aws-options-part-one/" />
		<id>https://www.percona.com/blog/?p=58754</id>
		<updated>2019-07-19T13:12:36Z</updated>
		<published>2019-07-17T13:26:03Z</published>
		<category scheme="https://www.percona.com/blog" term="Amazon RDS" /><category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="114" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-200x114.png" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL Performance on AWS" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-200x114.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-300x171.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-367x210.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS.png 642w" sizes="(max-width: 200px) 100vw, 200px" />With such a wide range of options available for running MySQL based servers in Amazon cloud environments, how do you choose? There&#8217;s no doubt it&#8217;s a challenge. In this two-part series of blog posts, we&#8217;ll try to draw a fair and informative comparison based on well-established benchmark scenarios – at scale. In part one we [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/17/assessing-mysql-performance-amongst-aws-options-part-one/"><![CDATA[<img width="200" height="114" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-200x114.png" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL Performance on AWS" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-200x114.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-300x171.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS-367x210.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-Performance-on-AWS.png 642w" sizes="(max-width: 200px) 100vw, 200px" /><p>With such a wide range of options available for running MySQL based servers in Amazon cloud environments, how do you choose? There&#8217;s no doubt it&#8217;s a challenge. In this two-part series of blog posts, we&#8217;ll try to draw a fair and informative comparison based on well-established benchmark scenarios – at scale.</p>
<p>In part one we will discuss the performance of the current Amazon RDS services – Amazon Aurora and Amazon RDS for MySQL – and compare it with the performance of Percona Server with InnoDB and RocksDB engines. And in <a target="_blank" href="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/">part two</a> we will go over costs and efficiencies to look for. All this information is necessarily number-heavy, but hopefully, you&#8217;ll enjoy the experiments!</p>
<h2>Evaluation scenario</h2>
<h3>The benchmark scripts</h3>
<p>For these evaluations, we use the <a target="_blank" href="https://github.com/Percona-Lab/sysbench-tpcc/blob/master/tpcc.lua">sysbench/tpcc</a> LUA test with a scale factor of 500 warehouses/10 tables. This is the equivalent of 5000 warehouses of the official TPC-C benchmark.</p>
<h3>Amazon MySQL Environments</h3>
<p>These are the AWS MySQL environments under analysis:</p>
<ul>
<li>Amazon RDS Aurora</li>
<li>Amazon RDS for MySQL with the InnoDB storage engine</li>
<li>Percona Server for MySQL with the InnoDB storage engine on Amazon EC2</li>
<li>Percona Server for MySQL with the RocksDB storage engine on Amazon EC2</li>
</ul>
<h3>Technical Setup &#8211; Server</h3>
<p>These general notes apply across the board:</p>
<ul>
<li>AWS region us-east-1(N.Virginia) was used for all tests</li>
<li>Server and client instances were spawned in the same availability zone</li>
<li>All data for tests were prepared in advance, stored as snapshots, and restored before the test</li>
<li>Encryption was not used</li>
</ul>
<p>And we believe that these configuration notes allow for a fair comparison of the different technologies:</p>
<ul>
<li>AWS EBS optimization was enabled for EC2 instances</li>
<li>For RDS/Amazon Aurora only a primary DB instance was created and used</li>
<li>In the case of RDS/MySQL, a single AZ deployment was used for RDS/MySQL</li>
<li>EC2/Percona Server for MySQL tests were run with binary log enabled</li>
</ul>
<p>Finally, here are the individual server configurations per environment:</p>
<h4>Server test #1: <strong>Amazon RDS Aurora</strong></h4>
<ul>
<li>Database server: Aurora MySQL 5.7</li>
<li>DB instances: r5.large, r5.xlarge, r5.2xlarge, r5.4xlarge</li>
<li>volume: used ~450GB(&gt;15000 IOPS)</li>
</ul>
<h4>Server test #2: <strong>Amazon RDS for MySQL with InnoDB Storage Engine</strong></h4>
<ul>
<li>Database server: MySQL Server 5.7.25</li>
<li>RDS instances: db.m5.large, db.m5.xlarge, db.m5.2xlarge, db.m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 700GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h4>Server test #3: <strong>Percona Server for MySQL with InnoDB Storage Engine</strong></h4>
<ul>
<li>Database server: Percona Server 5.7.25</li>
<li>EC2 instances: m5.large, m5.xlarge, m5.2xlarge, m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 700GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h4>Server test #4: <strong>Percona Server for MySQL with RocksDB using LZ4 compression</strong></h4>
<ul>
<li>Database server: Percona Server 5.7.25</li>
<li>EC2 instances: m5.large, m5.xlarge, m5.2xlarge, m5.4xlarge</li>
<li>volumes(allocated space):
<ul>
<li>gp2: 5400GB(~16000 IOPs)</li>
<li>io1: 350GB(15000 IOPs)</li>
</ul>
</li>
</ul>
<h3>Technical Setup &#8211; Client</h3>
<p>Common to all tests, we used an EC2 instance: m5.xlarge</p>
<p>So, now that we have established the setup, let&#8217;s take a look at what we found.</p>
<h2>Disk space consumption for sysbench/tpcc 5000 warehouses</h2>
<p>Since disk space costs money, you&#8217;d want to see how the raw data occupied space in each environment. There&#8217;s a clear &#8220;winner&#8221; there: Percona Server for MySQL with RocksDB storage engine. Of course, there are lots more factors to consider. <a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size.png"><br />
</a><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size.png"><img class="aligncenter wp-image-58454 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size.png" alt="MySQL Performance Amongst AWS" width="600" height="250" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size.png 600w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size-200x83.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size-300x125.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_0.v2.db_size-367x153.png 367w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<h2><strong>The performance tests</strong></h2>
<p>For the performance tests, we used this set up as common to all:</p>
<ul>
<li>number of threads: 128</li>
<li>duration of each test run: 30 mins</li>
</ul>
<p>Then, per server, we worked through individual tests as follows:</p>
<h3>Server tests</h3>
<h4>Server test #1: RDS/Amazon Aurora</h4>
<ul>
<li>create cluster</li>
<li>restore cluster instance from db snapshot</li>
</ul>
<h4>Server test #2: RDS/MySQL</h4>
<ul>
<li>restore instance from db snapshot</li>
</ul>
<h4>Server tests #3 and #4: Percona Server for MySQL on EC2</h4>
<ul>
<li>create EC2 instance as server</li>
<li>create volume from snapshot and attach to server instance</li>
</ul>
<h3><strong>Client test</strong></h3>
<ul>
<li>create EC2 instance as client</li>
<li>start test</li>
</ul>
<h2>Throughput</h2>
<p>First of all, then, let&#8217;s review throughput results for every tested database. In case things aren&#8217;t complicated enough, at this stage we&#8217;re also taking into account whether io1 or gp2 volume type has an effect on the outcomes. AWS offers <a target="_blank" href="https://aws.amazon.com/ebs/features/">this comparison</a> of io1 and gp2.</p>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3.png"><img class="aligncenter wp-image-58519 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3.png" alt="MySQL Performance AWS Options" width="650" height="560" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3.png 650w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3-174x150.png 174w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3-300x258.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/aws.final_.16_3.bar_.tps_.v3-367x316.png 367w" sizes="(max-width: 650px) 100vw, 650px" /></a></p>
<h4>Server test #1: <strong>RDS/Amazon Aurora</strong></h4>
<p>Aurora scales almost linearly from large to 4xlarge, notably outperforms Percona Server/InnoDB and RDS/MySQL but still behind RocksDB more than 2x times</p>
<h4>Server test #2: <strong>RDS/MySQL</strong></h4>
<ul>
<li>for all types of instances, we saw almost no difference in results between io1 and gp2 volumes</li>
<li>in these tests, RDS/MySQL shows the lowest throughput from all databases</li>
</ul>
<h4>Server test #3: <strong>Percona Server for MySQL with InnoDB </strong></h4>
<ul>
<li>up to 4xlarge instance results for gp2 volume slightly better(~10%) than for io1. On 4xlarge instance where the size of instance allows having a large enough innodb buffer pool that from one side helps to reduce the amount of reads as a notable amount of data will be already cached and from other side that helps to improve writing/flushing. As and in case of RocksDB it looks like writes operations are much more efficient(better latency) on io1 volumes and that helps to get better results with io1 volumes.</li>
<li>overall results for Percona Server/InnoDB is on 10-15% better than for RDS/MySQL but notably lower than for RDS/Aurora (in 2x-2.5x times) and RocksDB(5x times)</li>
</ul>
<h4>Server test #4: <strong>Percona Server for MySQL with RocksDB</strong></h4>
<ul>
<li>large/xlarge &#8211; results are on par for gp2 and io1 volumes, starting from 2xlarge instance results with io1 volume continue scales linearly while gp2 stays almost on the same level</li>
<li>RockDB shows best results across all types of instances and outperforms Percona Server/InnoDB and RDS/MySQL in 5x times and RDS/Aurora 2-2.5 times</li>
</ul>
<p>In the <a target="_blank" href="https://www.percona.com/blog/2019/07/19/assessing-mysql-performance-amongst-aws-options-part-two/">next post in this series</a>, I will take a look at the total cost of one test run for each database, as well as review which databases perform most efficiently by analysis transaction cost ratio.</p>
<p><a target="_blank" href="https://news.ycombinator.com/item?id=20470557" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Discuss on HackerNews</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/17/assessing-mysql-performance-amongst-aws-options-part-one/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/17/assessing-mysql-performance-amongst-aws-options-part-one/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Emily Ikuta</name>
					</author>
		<title type="html"><![CDATA[Upcoming Webinar 7/18: Learn how to connect a MySQL database with Java]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/17/webinar-how-to-connect-a-mysql-database-with-java/" />
		<id>https://www.percona.com/blog/?p=58330</id>
		<updated>2019-07-17T13:20:00Z</updated>
		<published>2019-07-17T12:51:11Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Technical Webinars" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="connect a MySQL database with Java" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Please join Percona’s Service Delivery Manager Rodrigo Trindade as he presents “Troubleshooting Java Connections to MySQL” on Thursday, July 18th, 2019 at 10:00 AM PDT (UTC-7). Register Now This talk will explain the steps needed to make a connection from Java to MySQL work and highlight potential issues you might encounter. It will cover all [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/17/webinar-how-to-connect-a-mysql-database-with-java/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="connect a MySQL database with Java" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/connect-a-MySQL-database-with-Java.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p>Please join Percona’s Service Delivery Manager Rodrigo Trindade as he presents “<a target="_blank" href="https://www.percona.com/resources/webinars/troubleshooting-java-connection-mysql">Troubleshooting Java Connections to MySQL</a>” on Thursday, July 18th, 2019 at 10:00 AM PDT (UTC-7).</p>
<p style="text-align: center;"><a target="_blank" class="btn btn-primary btn-lg" href="https://www.percona.com/resources/webinars/troubleshooting-java-connection-mysql" rel="noopener">Register Now</a></p>
<p>This talk will explain the steps needed to make a connection from Java to MySQL work and highlight potential issues you might encounter. It will cover all components, installation, and configuration.</p>
<p><a target="_blank" href="https://www.percona.com/resources/webinars/troubleshooting-java-connection-mysql">If you can&#8217;t attend sign up anyways we&#8217;ll send you the slides and recording afterward</a>.</p>
<h5 class="field-item even">Speaker:</h5>
<div class="field field-name-body field-type-text-with-summary field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="field field-name-field-presenter field-type-entityreference field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="pagecontent row">
<div class="col-md-10">
<div class="member-name">
<h4 class="mb-none">Rodrigo Trindade<br />
Service Delivery Manager</h4>
</div>
<div class="biography">
<div class="field field-name-body field-type-text-with-summary field-label-hidden">
<div class="field-items">
<div class="field-item even">
<div class="col-md-10">Rodrigo has a Master in Computer Science degree by the Rio Grande do Sul Federal University (Brazil) as well as over 10 years of experience as a CS Professor. Started as Software Developer then moved to Support Engineering working for Netscape, Sun Microsystems, and Oracle. Joined Percona in 2018 as a Service Delivery Manager. Solaris, Java, and Weblogic certified.</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/17/webinar-how-to-connect-a-mysql-database-with-java/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/17/webinar-how-to-connect-a-mysql-database-with-java/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Dmitriy Kostiuk</name>
					</author>
		<title type="html"><![CDATA[Percona Kubernetes Operator for MongoDB 1.1.0 Is Now Available]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-mongodb-1-1-0-is-now-available/" />
		<id>https://www.percona.com/blog/?p=58624</id>
		<updated>2019-07-16T18:27:10Z</updated>
		<published>2019-07-16T18:09:11Z</published>
		<category scheme="https://www.percona.com/blog" term="Events and Announcements" /><category scheme="https://www.percona.com/blog" term="Kubernetes" /><category scheme="https://www.percona.com/blog" term="Percona Server for MongoDB" /><category scheme="https://www.percona.com/blog" term="Percona Software" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Kubernetes for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />We are glad to announce the 1.1.0 release of the Percona Kubernetes Operator for Percona Server for MongoDB. The Operator simplifies the deployment and management of the Percona Server for MongoDB in Kubernetes-based environments. It extends the Kubernetes API with a new custom resource for deploying, configuring and managing the application through the whole life [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-mongodb-1-1-0-is-now-available/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Kubernetes for MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2017/10/PerconaServerforMongoDB-Vertical.png"><img class="alignright size-thumbnail wp-image-59091" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg" alt="Kubernetes for MongoDB" width="200" height="112" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Kubernetes-for-MongoDB.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /></a>We are glad to announce the 1.1.0 release of the <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/index.html">Percona Kubernetes Operator for Percona Server for MongoDB.</a></p>
<p>The Operator simplifies the deployment and management of the <a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Percona Server for MongoDB</a> in Kubernetes-based environments. It extends the Kubernetes API with a new custom resource for deploying, configuring and managing the application through the whole life cycle.</p>
<p>The Operator source code is available <a target="_blank" href="https://github.com/percona/percona-server-mongodb-operator" rel="noopener">in our Github repository</a>. All of Percona’s software is open-source and free.</p>
<h2>New features and improvements</h2>
<ul>
<li>Now the Percona Kubernetes Operator <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/update.html">allows upgrading</a> Percona Server for MongoDB to newer versions, either in semi-automatic or in manual mode.</li>
<li>Also, two modes are implemented for updating the Percona Server for MongoDB <tt class="file docutils literal"><span class="pre">mongod.conf</span></tt> configuration file: in<em> automatic configuration update</em> mode Percona Server for MongoDB Pods are immediately re-created to populate changed options from the Operator YAML file, while in <em>manual mode</em> changes are held until Percona Server for MongoDB Pods are re-created manually.</li>
<li><a target="_blank" href="https://www.percona.com/doc/percona-server-for-mongodb/LATEST/data_at_rest_encryption.html">Percona Server for MongoDB data-at-rest encryption</a> is now supported by the Operator to ensure that encrypted data files cannot be decrypted by anyone except those with the decryption key.</li>
<li>A separate service account is now used by the Operator&#8217;s containers which need special privileges, and all other Pods run on default service account with limited permissions.</li>
<li><a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/users.html">User secrets</a> are now generated automatically if don&#8217;t exist: this feature especially helps reduce work in repeated development environment testing and reduces the chance of accidentally pushing predefined development passwords to production environments.</li>
<li>The Operator <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/TLS.html">is now able to generate TLS certificates itself</a> which removes the need in manual certificate generation.</li>
<li>The list of officially supported platforms now includes the <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/minikube.html">Minikube</a>, which provides an easy way to test the Operator locally on your own machine before deploying it on a cloud.</li>
<li>Also, <a target="_blank" href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> 1.14 and <a target="_blank" href="https://percona.github.io/percona-xtradb-cluster-operator/install/">OpenShift Platform</a> 4.1 are now supported.</li>
</ul>
<p><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Percona Server for MongoDB</a> is an enhanced, open source and highly-scalable database that is a fully-compatible, drop-in replacement for MongoDB Community Edition. It supports MongoDB® protocols and drivers. Percona Server for MongoDB extends MongoDB Community Edition functionality by including the Percona Memory Engine, as well as several enterprise-grade features. It requires no changes to MongoDB applications or code.</p>
<div>Help us improve our software quality by reporting any bugs you encounter using <a target="_blank" href="https://jira.percona.com/secure/Dashboard.jspa" target="_blank" rel="&quot;&quot;&quot;nofollow noopener noreferrer" noopener noreferrer">our bug tracking system</a>.</div>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-mongodb-1-1-0-is-now-available/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-mongodb-1-1-0-is-now-available/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Jobin Augustine</name>
					</author>
		<title type="html"><![CDATA[BRIN Index for PostgreSQL: Don&#8217;t Forget the Benefits]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/16/brin-index-for-postgresql-dont-forget-the-benefits/" />
		<id>https://www.percona.com/blog/?p=58868</id>
		<updated>2019-07-19T13:37:28Z</updated>
		<published>2019-07-16T16:30:22Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="BRIN Index for PostgreSQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />proposed by PostgreSQL contributor Alvaro Herrera. BRIN stands for “Block Range INdex”. A block range is a group of pages adjacent to each other, where summary information about all those pages is stored in Index.  For example, Datatypes like integers &#8211; dates where sort order is linear &#8211; can be stored as min and max [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/16/brin-index-for-postgresql-dont-forget-the-benefits/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="BRIN Index for PostgreSQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/BRIN-Index-for-PostgreSQL-1.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><a target="_blank" href="https://www.postgresql.org/docs/current/brin-intro.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><img class="alignright size-thumbnail wp-image-57141" src="https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-200x127.png" alt="PostgreSQL" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-300x190.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline-367x233.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/05/slonik_with_black_text_and_tagline.png 756w" sizes="(max-width: 200px) 100vw, 200px" />BRIN Index</a> was introduced in PostgreSQL 9.5, but many users postponed the usage of it in their design and development just because it was “new”. But now we understand that it has stood the test-of-time! It is time to reconsider BRIN if you have not done it yet. I often see users who forget there is a provision to select the type of Index by specifying USING clause when creating an index.</p>
<p>BRIN Index is a revolutionary idea in indexing first <a target="_blank" href="https://www.postgresql.org/message-id/20130614222805.GZ5491@eldon.alvh.no-ip.org" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">proposed by PostgreSQL contributor Alvaro Herrera</a>. BRIN stands for “Block Range INdex”. A block range is a group of pages adjacent to each other, where summary information about all those pages is stored in Index.  For example, Datatypes like integers &#8211; dates where sort order is linear &#8211; can be stored as min and max value in the range. Other database systems including Oracle announced similar features later. BRIN index often gives similar gains as Partitioning a table.</p>
<p>BRIN usage will return all the tuples in all the pages in the particular range. So the index is lossy and extra work is needed to further filter out records. So while one might say that is not good, there are a few advantages.</p>
<ol>
<li>Since only summary information about a range of pages is stored, BRIN indexes are usually very small compared to B-Tree indexes. So if we want to squeeze the working set of data to shared_buffer, this is a great help.</li>
<li>Lossiness of BRIN can be controlled by specifying pages per range (discussed in a later section)</li>
<li>Offloads the summarization work to vacuum or autovacuum. So the overhead of index maintenance on transaction / DML operation is minimal.</li>
</ol>
<h2>Putting BRIN into a test</h2>
<p>Let&#8217;s take a simple example to examine the benefits of BRIN index by creating a simple table.</p><pre class="crayon-plain-tag">postgres=# CREATE TABLE testtab (id int NOT NULL PRIMARY KEY,date TIMESTAMP NOT NULL, level INTEGER, msg TEXT);
CREATE TABLE</pre><p>Now let&#8217;s Insert some data into this table.</p><pre class="crayon-plain-tag">postgres=# INSERT INTO testtab (id, date, level, msg) SELECT g, CURRENT_TIMESTAMP + ( g || 'minute' ) :: interval, random() * 6, md5(g::text) FROM generate_series(1,8000000) as g;
INSERT 0 8000000</pre><p>Please note that values in id column and date columns keep increasing for new records, which is common for transaction records. This is an important property which BRIN make use off.</p>
<p>A query at this stage may have to do a full scan of the table and it will be quite expensive.</p><pre class="crayon-plain-tag">postgres=# explain analyze select * from public.testtab where date between '2019-08-08 14:40:47.974791' and '2019-08-08 14:50:47.974791';
                                                                          QUERY PLAN                                                                         
---------------------------------------------------------------------------------------------------------------------------------------------------------------
Gather  (cost=1000.00..133476.10 rows=11 width=49) (actual time=31.835..1766.409 rows=11 loops=1)
  Workers Planned: 2
  Workers Launched: 2
  -&gt; Parallel Seq Scan on testtab  (cost=0.00..132475.00 rows=5 width=49) (actual time=1162.029..1739.713 rows=4 loops=3)
        Filter: ((date &gt;= '2019-08-08 14:40:47.974791'::timestamp without time zone) AND (date &lt;= '2019-08-08 14:50:47.974791'::timestamp without time zone))
        Rows Removed by Filter: 2666663
Planning Time: 0.296 ms
Execution Time: 1766.454 ms
(8 rows)</pre><p>As we can see above, PostgreSQL employs 2 workers and does the scan in parallel, where it takes 1766.454 ms. In a nutshell, it is heavy on the system and takes a good amount of time.</p>
<p>As usual, our tendency is to create an index on the filtering column. (B-Tree by default)</p><pre class="crayon-plain-tag">postgres=# create index testtab_date_idx  on testtab(date);
CREATE INDEX</pre><p>Now let&#8217;s see how the previous SELECT query works:</p><pre class="crayon-plain-tag">postgres=# explain analyze select * from public.testtab where date between '2019-08-08 14:40:47.974791' and '2019-08-08 14:50:47.974791';
                                                                        QUERY PLAN                                                                        
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Index Scan using testtab_date_idx on testtab  (cost=0.43..8.65 rows=11 width=49) (actual time=1.088..1.090 rows=11 loops=1)
  Index Cond: ((date &gt;= '2019-08-08 14:40:47.974791'::timestamp without time zone) AND (date &lt;= '2019-08-08 14:50:47.974791'::timestamp without time zone))
Planning Time: 22.225 ms
Execution Time: 2.657 ms
(4 rows)</pre><p>Obviously, B-Tree is lossless, and it can give a tremendous boost to the performance and efficiency of SELECT queries, especially when Index is freshly created. But a B-Tree index has following side effects:</p>
<ol>
<li>There will be a reasonable penalty for DMLs</li>
<li>Index size can go big and consume a good amount of shared_buffers</li>
<li>The random page seeks, especially when index pages are not cached, can become expensive.</li>
</ol>
<p>It will be interesting to check the Index to table size ratio of this fresh index.</p><pre class="crayon-plain-tag">postgres=# \di+ testtab_date_idx;
List of relations
Schema | Name | Type | Owner | Table | Size | Description
--------+------------------+-------+----------+---------+--------+-------------
public | testtab_date_idx | index | postgres | testtab | 171 MB |
(1 row)</pre><p>So we have B-Tree index 171MB for a 1.3GB table. So Index to table size ratio is 0.13, for this is a fresh index.</p>
<p>But this index-table ratio can keep deteriorating over time as index undergoes continuous updates. Index ratio crossing 0.5 is common in many production environments. As the ratio becomes bad, the efficiency of the index goes bad and it starts occupying more shared buffers.</p>
<p>Things get much worse if the table grows to a bigger size (hundreds of GB or TB) as Index also grows in the same ratio. The impact of B-Tree index on DMLs is heavy &#8211; sometimes I measured up to 30% overhead especially for bulk loads.</p>
<p>Now let&#8217;s see what happens if we replace a B-Tree index with a BRIN index.</p><pre class="crayon-plain-tag">postgres=# create index testtab_date_brin_idx  on testtab using brin (date);
CREATE INDEX</pre><p>The very first observation is that the impact of Index on the same bulk insert operation is measured to be 3% which is within my noise/error limits. 30% overhead Vs 3% overhead.</p>
<p>If we consider the size of the newly created BRIN index:</p><pre class="crayon-plain-tag">postgres=# \di+ testtab_date_brin_idx;
                                List of relations
Schema |         Name          | Type  |  Owner   |  Table  | Size  | Description
--------+-----------------------+-------+----------+---------+-------+-------------
public | testtab_date_brin_idx | index | postgres | testtab | 64 kB |
(1 row)</pre><p>As we can see it is just 64kB! 171MB of B-Tree index Vs 64 kb of BRIN index.</p>
<p>So far BRIN wins my heart. Now its time to look at how much query performance improvement it can bring in.</p><pre class="crayon-plain-tag">postgres=# explain analyze select * from public.testtab where date between '2019-08-08 14:40:47.974791' and '2019-08-08 14:50:47.974791';
                                                                            QUERY PLAN                                                                           
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Bitmap Heap Scan on testtab  (cost=20.03..33406.84 rows=11 width=49) (actual time=62.762..87.471 rows=11 loops=1)
  Recheck Cond: ((date &gt;= '2019-08-08 14:40:47.974791'::timestamp without time zone) AND (date &lt;= '2019-08-08 14:50:47.974791'::timestamp without time zone))
  Rows Removed by Index Recheck: 12405
  Heap Blocks: lossy=128
  -&gt; Bitmap Index Scan on testtab_date_brin_idx  (cost=0.00..20.03 rows=12403 width=0) (actual time=1.498..1.498 rows=1280 loops=1)
        Index Cond: ((date &gt;= '2019-08-08 14:40:47.974791'::timestamp without time zone) AND (date &lt;= '2019-08-08 14:50:47.974791'::timestamp without time zone))
Planning Time: 0.272 ms
Execution Time: 87.703 ms
(8 rows)</pre><p>As I expected, it is not that efficient as a fully cached B-Tree index. However, performance improvement from 1766.454 ms to 87.703 ms means approximately 20 times better! That&#8217;s with a single worker consuming fewer resources. With just a 64kb Overhead, the cost-benefit of BRIN is very positive.</p>
<h2>Storage and maintenance</h2>
<p>As we saw the BRIN index uses much less space compared to normal B-Tree index, but the lossy index can reduce the effectiveness of Index. Luckily this can be adjusted using the storage parameter pages_per_range. BRIN stores entries for a range of pages in the corresponding table. The larger the range of pages, the smaller the index, and it gets lossier.</p><pre class="crayon-plain-tag">create index testtab_date_brin_idx on testtab using brin (date) with (pages_per_range = 32);</pre><p>As we already discussed, part of the index maintenance during DML is offloaded to vacuum. In fact, there are 2 cases.</p>
<ol>
<li>Pages in the table which are already summarized are updated</li>
<li>New pages which are not part of the summary.</li>
</ol>
<p>In the first case, summary in BRIN is updated straight away along with DML. But if new pages are not summarized already, it will be done by VACUUM or AUTOVACUUM.</p>
<p>There are 2 functions provided for this. A high-level, function call which can be executed against the index like:</p><pre class="crayon-plain-tag">postgres=# select brin_summarize_new_values('testtab_date_brin_idx'::regclass);
brin_summarize_new_values
---------------------------
2577
(1 row)</pre><p>This summarizes all ranges that are not currently summarized. The return value indicates the number of new page range summaries that were inserted into the index. This function operates on the top of a lower-level function brin_summarize_range which accepts the range of pages.</p><pre class="crayon-plain-tag">postgres=# SELECT brin_summarize_range('testtab_date_brin_idx', 10);
brin_summarize_range
----------------------
0
(1 row)</pre><p>A key point to note is that auto-summarization is off by default, which we can enable by storage parameter auto summarize:</p><pre class="crayon-plain-tag">postgres=# alter index testtab_date_brin_idx set (pages_per_range = 64,autosummarize = on);
ALTER INDEX</pre><p>In this case, automatic summarization will be executed by autovacuum as insertions occur.</p>
<h2>Limitation:</h2>
<p>BRIN indexes are efficient if the ordering of the key values follows the organization of blocks in the storage layer. In the simplest case, this could require the physical ordering of the table, which is often the creation order of the rows within it, to match the key&#8217;s order. Keys on generated sequence numbers or created data are best candidates for BRIN index.</p>
<p><a target="_blank" href="https://news.ycombinator.com/item?id=20478372" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Discuss on Hacker News</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/16/brin-index-for-postgresql-dont-forget-the-benefits/#comments" thr:count="6"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/16/brin-index-for-postgresql-dont-forget-the-benefits/feed/atom/" thr:count="6"/>
		<thr:total>6</thr:total>
			</entry>
		<entry>
		<author>
			<name>Tim Sharp</name>
						<uri>http://www.percona.com/blog</uri>
						</author>
		<title type="html"><![CDATA[Avoid Vendor Lock-in by Embracing Open Source]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/16/avoid-vendor-lock-in-by-embracing-open-source/" />
		<id>https://www.percona.com/blog/?p=58949</id>
		<updated>2019-07-16T14:02:29Z</updated>
		<published>2019-07-16T14:00:31Z</published>
		<category scheme="https://www.percona.com/blog" term="Percona Open Source Advance" />		<summary type="html"><![CDATA[<img width="200" height="87" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Open Source Advance" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-300x130.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1024x443.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1140x504.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-367x159.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance.jpg 1164w" sizes="(max-width: 200px) 100vw, 200px" />We are pleased to announce the launch of Percona Open Source Advance (POSA). POSA is a consulting-led migration solution which assesses your existing proprietary database architecture and helps you migrate to the most suitable free open source software available. Open source software migrations are increasingly mandated as strategic initiatives for companies but often require expert [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/16/avoid-vendor-lock-in-by-embracing-open-source/"><![CDATA[<img width="200" height="87" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Open Source Advance" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-300x130.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1024x443.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1140x504.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-367x159.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance.jpg 1164w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-58950" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg" alt="Percona Open Source Advance" width="200" height="87" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-200x87.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-300x130.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1024x443.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-1140x504.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance-367x159.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Open-Source-Advance.jpg 1164w" sizes="(max-width: 200px) 100vw, 200px" />We are pleased to announce the launch of <a target="_blank" href="https://www.percona.com/services/consulting/percona-open-source-advance"><strong>Percona Open Source Advance (POSA)</strong></a>. POSA is a consulting-led migration solution which assesses your existing proprietary database architecture and helps you migrate to the most suitable free open source software available.</p>
<p>Open source software migrations are increasingly mandated as strategic initiatives for companies but often require expert external advice and support. As a leader in open source software, Percona is ideally placed to deliver a solution which solves many common pain points and, most compelling, delivers a significant annual cost reduction.</p>
<p><a target="_blank" href="https://www.riministreet.com/documents/collateral/rimini-street-ebook-state-of-innovation-priorities-and-challenges.pdf" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">The 2018 Rimini Street Global IT Innovation Study</a> surveyed 900 CIOs, IT leaders, and financial decision-makers and found that 63% of respondents feel locked into their organization’s enterprise application software vendor relationship. Companies are unhappy for a variety of reasons:</p>
<ul>
<li>Support costs are often extortionate with few available alternatives</li>
<li>Annual license fees can be very expensive</li>
<li>They are locked-in to their vendor</li>
<li>Lack of flexibility as they are unable to embrace new systems or cloud-opportunities (other than those provided by their vendor)</li>
</ul>
<p>As a champion of unbiased open source software, Percona views open source software as a viable and effective means of securing high-quality, production-ready software without being locked into a license or paying fees.</p>
<p>In your journey to open source, Percona will act a trusted and unbiased advisor, working closely with you throughout the entire open source migration. POSA involves a deep-dive into your proprietary database architecture, comprehensive recommendations, a practical migration plan, and hands-on support and expertise during and after your migration.</p>
<p><strong>Please visit our <a target="_blank" href="https://www.percona.com/services/consulting/percona-open-source-advance">website</a> and <a target="_blank" href="https://www.percona.com/resources/solution-brief/percona-open-source-advance" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">download our new solution flyer</a> to find out more.</strong></p>
<p>Percona has extensive experience advising companies on the best way to configure, manage and run their databases. Contact us to discuss how you can stay at the forefront of technology innovation and benefit from the efficiencies and opportunities that open source software brings.</p>
<p>To discuss how Percona can help, please contact us at <a target="_blank" href="tel:1-888-316-9775" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">1-888-316-9775</a>, or <a target="_blank" href="tel:0-800-051-898" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">0-800-051-8984</a> in Europe, or click <a target="_blank" href="http://learn.percona.com/contact-me">here</a> for us to contact you directly.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/16/avoid-vendor-lock-in-by-embracing-open-source/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/16/avoid-vendor-lock-in-by-embracing-open-source/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Dmitriy Kostiuk</name>
					</author>
		<title type="html"><![CDATA[Percona Kubernetes Operator for Percona XtraDB Cluster 1.1.0 Now Available]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-percona-xtradb-cluster-1-1-0/" />
		<id>https://www.percona.com/blog/?p=58610</id>
		<updated>2019-07-16T13:44:44Z</updated>
		<published>2019-07-16T13:30:21Z</published>
		<category scheme="https://www.percona.com/blog" term="Events and Announcements" /><category scheme="https://www.percona.com/blog" term="Kubernetes" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona Software" /><category scheme="https://www.percona.com/blog" term="Percona XtraDB Cluster" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Kubernetes Operator for Percona XtraDB Cluster 1.1.0" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />We are glad to announce the 1.1.0 release of the  Percona Kubernetes Operator for Percona XtraDB Cluster. The Percona Kubernetes Operator for Percona XtraDB Cluster automates the lifecycle and provides a consistent Percona XtraDB Cluster instance. The Operator can be used to create a Percona XtraDB Cluster, or scale an existing Cluster, and contains the [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-percona-xtradb-cluster-1-1-0/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Kubernetes Operator for Percona XtraDB Cluster 1.1.0" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59089" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg" alt="Percona Kubernetes Operator for Percona XtraDB Cluster 1.1.0" width="200" height="112" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-300x168.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-1024x572.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0-367x205.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1.1.0.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />We are glad to announce the 1.1.0 release of the  <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html" rel="nofollow">Percona Kubernetes Operator for Percona XtraDB Cluster</a>.</p>
<p>The Percona Kubernetes Operator for Percona XtraDB Cluster automates the lifecycle and provides a consistent Percona XtraDB Cluster instance. The Operator can be used to create a Percona XtraDB Cluster, or scale an existing Cluster, and contains the necessary Kubernetes settings.</p>
<p>The Operator simplifies the deployment and management of the <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-xtradb-cluster">Percona XtraDB Cluster</a> in Kubernetes-based environments. It extends the Kubernetes API with a new custom resource for deploying, configuring and managing the application through the whole life cycle.</p>
<p>The Operator source code is available <a target="_blank" href="https://github.com/percona/percona-xtradb-cluster-operator" rel="noopener">in our Github repository</a>. All of Percona’s software is open-source and free.</p>
<h2>New features and improvements</h2>
<ul>
<li>Now the Percona Kubernetes Operator <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-pxc/update.html">allows upgrading</a> Percona XtraDB Cluster to newer versions, either in semi-automatic or in manual mode.</li>
<li>Also, two modes are implemented for updating the Percona XtraDB Cluster <tt class="file docutils literal"><span class="pre">my.cnf</span></tt> configuration file: in<em> automatic configuration update</em> mode Percona XtraDB Cluster Pods are immediately re-created to populate changed options from the Operator YAML file, while in <em>manual mode</em> changes are held until Percona XtraDB Cluster Pods are re-created manually.</li>
<li>A separate service account is now used by the Operator&#8217;s containers which need special privileges, and all other Pods run on default service account with limited permissions.</li>
<li><a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-pxc/users.html">User secrets</a> are now generated automatically if don&#8217;t exist: this feature especially helps reduce work in repeated development environment testing and reduces the chance of accidentally pushing predefined development passwords to production environments.</li>
<li>The Operator <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-pxc/TLS.html">is now able to generate TLS certificates itself</a> which removes the need in manual certificate generation.</li>
<li>The list of officially supported platforms now includes <a target="_blank" href="https://kubernetes.io/docs/setup/learning-environment/minikube/">Minikube</a>, which provides an easy way to test the Operator locally on your own machine before deploying it on a cloud.</li>
<li>Also, <a target="_blank" href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> 1.14 and <a target="_blank" href="https://percona.github.io/percona-xtradb-cluster-operator/install/">OpenShift Platform</a> 4.1 are now supported.</li>
</ul>
<p><a target="_blank" href="http://www.percona.com/doc/percona-xtradb-cluster/">Percona XtraDB Cluster</a> is an open source, cost-effective and robust clustering solution for businesses. It integrates Percona Server for MySQL with the Galera replication library to produce a highly-available and scalable MySQL® cluster complete with synchronous multi-master replication, zero data loss and automatic node provisioning using Percona XtraBackup.</p>
<p>Help us improve our software quality by reporting any bugs you encounter using <a target="_blank" href="https://jira.percona.com/secure/Dashboard.jspa" rel="nofollow noopener">our bug tracking system</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-percona-xtradb-cluster-1-1-0/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/16/percona-kubernetes-operator-for-percona-xtradb-cluster-1-1-0/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Lorraine Pocklington, Community Manager</name>
					</author>
		<title type="html"><![CDATA[Percona Live Europe 2019 Conference Updates]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-live-europe-2019-conference-updates/" />
		<id>https://www.percona.com/blog/?p=59018</id>
		<updated>2019-07-25T13:06:47Z</updated>
		<published>2019-07-16T07:43:08Z</published>
		<category scheme="https://www.percona.com/blog" term="Events and Announcements" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona Live" /><category scheme="https://www.percona.com/blog" term="Amsterdam" />		<summary type="html"><![CDATA[<img width="200" height="105" src="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />The Percona Live Open Source Database Conference, Europe 2019 will have a one week extension of the call for papers.  Our new close date will be Monday July 22, 2019.  Any and all topics related to open source database technologies are invited. This year&#8217;s conference takes a new approach, organizing tracks along these key business [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/16/percona-live-europe-2019-conference-updates/"><![CDATA[<img width="200" height="105" src="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p>The <a target="_blank" href="https://percona.com/live-info">Percona Live Open Source Database Conference, Europe 2019</a> will have a one week extension of <a target="_blank" href="https://www.percona.com/cfp">the call for papers</a>.  Our new close date will be <strong>Monday July 22, 2019</strong>.  Any and all topics related to open source database technologies are invited.</p>
<p>This year&#8217;s conference takes a new approach, organizing tracks along these key business themes:</p>
<ul>
<li>Performance &amp; Scalability</li>
<li>Public, Private, and Hybrid Clouds &amp; Everything In Between</li>
<li>Building Large, Scalable, &amp; Secure Database Deployments</li>
<li>Hot Topics, New Features, &amp; Trends You Should Know About</li>
<li>Monitor, Manage, &amp; Maintain Databases at Scale</li>
<li>How to Reduce Costs &amp; Complexity with Open Source Databases</li>
<li>Fascinating or important talks that don&#8217;t 100% fit in the other tracks</li>
</ul>
<p>But as always the conference will be rich in expertise about MySQL®, MongoDB®, MariaDB®, PostgreSQL, Kubernetes®, and everything else related to open source databases.</p>
<h2>Program Committee Announced</h2>
<p>Percona is pleased to announce the <a target="_blank" href="https://www.percona.com/live/amsterdam19/conference-committee">conference program committee</a>.  We are grateful for the participation of these senior technical experts from across the open source database world:</p>
<ul>
<li><strong>Laine Campbell,</strong> Facebook</li>
<li><strong>Colin Charles</strong>, Consultant</li>
<li><strong>Frédéric Descamps</strong>, Oracle</li>
<li><strong>Antonios Giannopoulos</strong>, Object Rocket</li>
<li><strong>Giuseppe Maxia</strong>, Software Explorer</li>
<li><strong>Valerie Parham-Thompson</strong>, Pythian</li>
<li><strong>Nicolai Plum</strong>, Booking.com</li>
</ul>
<p>Percona&#8217;s committee representatives are:</p>
<ul>
<li><strong>Jobin Augustine</strong>, Senior Support Engineer</li>
<li><strong>Vinicius Grippa</strong>, Senior Support Engineer</li>
<li><strong>Sveta Smirnova</strong>, Principal Support Escalation Specialist</li>
<li><strong>Alkin Tezuysal</strong>, Senior Technical Manager</li>
</ul>
<p>The conference program will be announced on a rolling basis in early August.</p>
<h2>Registration is OPEN!</h2>
<p><a target="_blank" href="https://www.percona.com/live-registration"><img class="alignright size-thumbnail wp-image-59039" src="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg" alt="" width="200" height="105" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/earlybird-social2.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /></a>Finally,  registration is now open!! Buying your ticket early will get you the best value by far at just €525 for all three days until August 9 . We are hosting the conference at a stunning new airport venue, the Amsterdam Hilton Schiphol, allowing easy access for international travelers but just 20 minutes by train from the historic heart of Amsterdam.  This new venue provides outstanding modern facilities with room for the conference to continue to grow. <a target="_blank" href="https://percona.com/live-info">Read more about the event</a> or simply&#8230;</p>
<h3><a target="_blank" href="https://www.percona.com/live-registration">Register here now</a></h3>
<p>&nbsp;</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/16/percona-live-europe-2019-conference-updates/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/16/percona-live-europe-2019-conference-updates/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Tibor Korocz</name>
					</author>
		<title type="html"><![CDATA[MySQL: The Impact of Transactions on Query Throughput]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/15/mysql-the-impact-of-transactions-on-query-throughput/" />
		<id>https://www.percona.com/blog/?p=58153</id>
		<updated>2019-07-19T20:06:54Z</updated>
		<published>2019-07-15T14:30:02Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="124" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-200x124.png" class="webfeedsFeaturedVisual wp-post-image" alt="Impact of Transactions on Query Throughput" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1.png 600w" sizes="(max-width: 200px) 100vw, 200px" />Recently I had a customer where every single query was running in a transaction, as well as even the simplest selects. Unfortunately, this is not unique and many connectors like Java love to do that. In their case, the Java connector changed autocommit=off for the connection itself at the beginning, and as these were permanent [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/15/mysql-the-impact-of-transactions-on-query-throughput/"><![CDATA[<img width="200" height="124" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-200x124.png" class="webfeedsFeaturedVisual wp-post-image" alt="Impact of Transactions on Query Throughput" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-1.png 600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-59033" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput-200x133.jpeg" alt=" Impact of Transactions on Query Throughput" width="200" height="133" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-The-Impact-of-Transactions-on-Query-Throughput.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Recently I had a customer where every single query was running in a transaction, as well as even the simplest selects. Unfortunately, this is not unique and many connectors like Java love to do that.</p>
<p>In their case, the Java connector changed <code>autocommit=off</code> for the connection itself at the beginning, and as these were permanent connections they never or almost never reconnected.</p>
<p>In the slow query log we could see after every select there was a commit. So why is this a problem?</p>
<h3>Test Case</h3>
<p>Like always, the best way to deal with a problem to test it. I have created two EC2 instances t3.xlarge with Ubuntu, one for application and one for the databases.  I have used <a target="_blank" href="https://github.com/akopytov/sysbench" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">sysbench</a> to run my tests.</p>
<p>I have created a table with 1 million records and was running simple primary key point selects against the database. I was using three threads and running only 1 select per transactions,  and every test was running for 60 seconds. I ran every test 3 times and took the average number of these three runs. These are the only MySQL variables I have changed:</p><pre class="crayon-plain-tag">innodb_buffer_pool_size=5G
innodb_adaptive_hash_index=off
query_cache_size=0
query_cache_type=0</pre><p>The dataset could fit in memory as there were no disk reads.</p>
<p>I have tested four cases:</p>
<ol>
<li>PK selects &#8211; without transaction (select only)</li>
<li>PK selects &#8211; inside a transaction (begin,select,commit)</li>
<li>PK selects &#8211; autocommit=off (select,commit)</li>
<li>PK selects &#8211; Read-Only transaction (START TRANSACTION READ ONLY, select, commit)</li>
</ol>
<h4>Disclaimer</h4>
<p>I was not trying to do a proper performance test to see the maximum performance on the server. I was only trying to demonstrate the impact on the executed query number if we are running every single query in a transaction in a limited time window.</p>
<h3>Test results on MySQL 5.6</h3>
<p>Because my customer was running on MySQL 5.6 I did my first tests on that version as well.</p>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1.png"><img class="size-full wp-image-58159 aligncenter" src="https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1.png" alt="" width="600" height="371" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1.png 600w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-5.6-1-367x227.png 367w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<p>Here we can see the average transactions per second (inside InnoDB everything is a transaction even if we do not start it explicitly). In the same time window, sysbench could run more than twice as many PK lookups without transactions than in transactions. You can see there are big differences here.</p>
<h3>But what does that mean? My Selects are slower?</h3>
<p>It depends on your point of view, but if you measure the whole transaction time, yes it takes more time, but if you measure the single select statement that should take the same amount of time. So your single selects are not going to be slower.</p>
<p>Let me try to oversimplify this. Let&#8217;s say your database server can run only 1 query per second. That means in 100 seconds it can run 100 queries. What if we are using transaction? In the first second the database server will run the <code>begin</code>, then it runs the <code><code>select</code></code> , and after that in the third second, it will run the <code>commit</code>.  So it will run 1 select in every three seconds. In 100 seconds 33 begin, 33 select, 33 commit. Your query time is not going to be higher but your throughput is going to be impacted.</p>
<p>If there is no real reason (example repeatable read) why your selects should run in transactions, I would recommend to avoid them, because you can save a lot of extra roundtrip between the application and the database server, some CPU time, and even begin commits very fast, but in large scale you can save time as well.</p>
<h3>What is a transaction?</h3>
<p><a target="_blank" href="https://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_transaction" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">From InnoDB manual:</a></p>
<blockquote><p>Transactions are atomic units of work that can be committed or rolled back. When a transaction makes multiple changes to the database, either all the changes succeed when the transaction is committed, or all the changes are undone when the transaction is rolled back.</p>
<p>Database transactions, as implemented by InnoDB, have properties that are collectively known by the acronym ACID, for atomicity, consistency, isolation, and durability.</p></blockquote>
<p>InnoDB associates every transaction with a <code>transaction ID</code> (TRX_ID field is part of every single row). That transaction ID is only needed for operations that might perform writes or locks like <code>Select ... for update</code>.  Also, this transaction ID is used to create a read view, a snapshot of the database, as this is part of the <a target="_blank" href="https://dev.mysql.com/doc/refman/8.0/en/innodb-multi-versioning.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">MVCC</a> mechanism. MVCC keeps information of the old pages until the transaction is running, an example is to be able to rollback. These pages also used to provide consistent reads. Even if this is just a single long-running select, because we started a transaction, InnoDB has to keep track of these changes. There are other internal processes happening when we explicitly start a transaction, which might be not necessary for a simple select query.</p>
<p>This is one of the reasons why they developed <a target="_blank" href="https://dev.mysql.com/doc/refman/5.6/en/innodb-performance-ro-txn.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Read-only transactions</a> to avoid these side effects.</p>
<p>When <code>autocommit</code> is enabled and we do not start transactions explicitly, the insert/delete/update operations are still handled inside MySQL like transactions, but InnoDB tries to detect non-locking select queries which should be handled like Read-only transactions.</p>
<p>I was curious if is there any difference/improvements between the versions so I re-ran my tests on multiple MySQL versions.</p>
<h3>Testing different versions</h3>
<p>I have tested:</p>
<ul>
<li>MySQL 5.6</li>
<li>Percona Server 5.6</li>
<li>MySQL 5.7</li>
<li>Percona Server 5.7</li>
<li>MySQL 8.0</li>
<li>Percona Server 8.0</li>
</ul>
<p><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions.png"><img class="size-full wp-image-58160 aligncenter" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions.png" alt="" width="600" height="371" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions.png 600w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Impact-of-Transactions-367x227.png 367w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<p>What we can see here? We can see that a big difference is still there in all versions, and actually, I was not expecting it to disappear because from the application point of view it still has to do the round trips to the database server.</p>
<p>We knew there is a regression in the newer version of MySQL regarding the performance of point lookups. <a target="_blank" href="https://smalldatum.blogspot.com/2014/10/low-concurrency-performance-for-point.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Mark Callaghan</a> and many other people already blogged about this, but as we can see the regression is quite big. I am already working on another blog post where I am trying to dig deeper on what is causing this degradation.</p>
<p>The newer version of MySQL servers are more optimized on high concurrent workloads, and they can handle more threads at the same time as the previous versions. There are many detailed <a target="_blank" href="http://dimitrik.free.fr/blog/archives/cat_mysql.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">blog posts</a> in this <a target="_blank" href="https://www.percona.com/blog/2016/05/17/mysql-5-7-read-write-benchmarks/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">area as well</a>.</p>
<p>We can also see, for these simple queries starting Read-only transactions, they are actually a lit bit slower than starting normal transactions. The difference is between 1-4% while in MySQL 8.0 it is around 1% slower. That is a very small difference but is continuous through all the tests.</p>
<h3>Conclusion</h3>
<p>If you are running every single select query in a transaction your throughput could be definitely lower than without transactions. If your application requires higher throughput, you should not use explicit transactions for every select query.</p>
<p><a target="_blank" href="https://www.percona.com/software/mysql-database/percona-server">Learn more about Percona Server for MySQL</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/15/mysql-the-impact-of-transactions-on-query-throughput/#comments" thr:count="5"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/15/mysql-the-impact-of-transactions-on-query-throughput/feed/atom/" thr:count="5"/>
		<thr:total>5</thr:total>
			</entry>
		<entry>
		<author>
			<name>Akira Kurogane</name>
					</author>
		<title type="html"><![CDATA[MongoDB Security vs. Five &#8216;Bad Guys&#8217;]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/12/mongodb-security-vs-five-bad-guys/" />
		<id>https://www.percona.com/blog/?p=58905</id>
		<updated>2019-07-19T20:07:39Z</updated>
		<published>2019-07-12T13:51:43Z</published>
		<category scheme="https://www.percona.com/blog" term="MongoDB" /><category scheme="https://www.percona.com/blog" term="Security" />		<summary type="html"><![CDATA[<img width="200" height="143" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-300x215.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-1024x734.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-367x263.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Most any commercially mature DBMS provides the following five ways to secure the data you keep inside it: Authentication of user connections (== Identity) Authorization (== DB command permissions) (a.k.a. Role-based access control) Network Encryption (a.k.a. Transport encryption) Storage Encryption (a.k.a. Encryption-at-rest) Auditing (MongoDB Enterprise or Percona Server for MongoDB only) MongoDB is no exception. [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/12/mongodb-security-vs-five-bad-guys/"><![CDATA[<img width="200" height="143" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-300x215.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-1024x734.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-367x263.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-58990" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg" alt="MongoDB Security" width="200" height="143" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-200x143.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-300x215.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-1024x734.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2-367x263.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Security-2.jpg 1600w" sizes="(max-width: 200px) 100vw, 200px" />Most any commercially mature DBMS provides the following five ways to secure the data you keep inside it:</p>
<ul>
<li>Authentication of user connections (== Identity)</li>
<li>Authorization (== DB command permissions) (a.k.a. Role-based access control)</li>
<li>Network Encryption (a.k.a. Transport encryption)</li>
<li>Storage Encryption (a.k.a. Encryption-at-rest)</li>
<li>Auditing (MongoDB Enterprise or <a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Percona Server for MongoDB</a> only)</li>
</ul>
<p>MongoDB is no exception. All of these have been present for quite a while, although infamously the first versions set &#8220;&#8211;auth&#8221; off by default and this is still in effect. (See more in the &#8220;Auth &#8211; Still disabled by default&#8221; section later.)</p>
<p>This article is an overview of all, plus some important clarification to sort out Authentication and Authorization. Network and storage encryption and Auditing will be expanded on in other articles.</p>
<h2><span style="font-weight: 400;">MongoDB Security: The bad guy line-up</span></h2>
<p><span style="font-weight: 400;">So what exactly do the five security subsystems do? Where does the responsibility and purpose of one stop, and others, start?</span></p>
<p><span style="font-weight: 400;">I think the easiest way to explain it is to highlight the &#8216;bad guy&#8217; each one repels.</span></p>
<table style="border-collapse: collapse;">
<tbody>
<tr>
<td></td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top; font-weight: bold;">Bad guy</td>
</tr>
<tr>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top; width: 10em;">Authentication</td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">An unknown person, whom you didn&#8217;t realize had network access to the database server, who just &#8216;walks in&#8217; and looks at, copies, or damages the database data.</td>
</tr>
<tr>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Authorization<br />
a.k.a. Access control</td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">A user or application that reads or alters or deletes data other than what they were <i>supposed</i> to.<br />
This &#8216;bad guy&#8217; is usually a colleague who does it by accident so it&#8217;s mostly for <i>safety</i> rather than security, but it also prevents malicious cases too.</td>
</tr>
<tr>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Network Encryption</td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Someone who takes a copy of the data being transferred over a network link somewhere between server A and server B.</td>
</tr>
<tr>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Storage Encryption</td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Someone who breaks into your datacenter and steals your server&#8217;s hard disk so they can read the data files on it.<br />
In practice, they would probably steal the file data over the network or get the disk in a second-hand hardware sale, but the concept is still someone who obtains a copy of the underlying database files.</td>
</tr>
<tr>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">Auditing</td>
<td style="padding: 4px; border: 1px solid #808080; vertical-align: top;">A privileged database user who knows how to cover up their tracks after altering database data.<br />
An important caveat &#8211; all bets are off if a unix account with the privilege to overwrite the audit log is controlled by db-abusing adversary.</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;"><br />
Authentication and authorization must be activated in unison, and auditing requires authentication as a prerequisite, but otherwise, they can be used independently of each other and there is little if any entanglement of code between one the subsystems above and another.</span></p>
<p><span style="font-weight: 400;">Apart from the caveats of the paragraph above, if you believe that certain bad guys are not a problem for you then you don&#8217;t have to use the relevant section.</span></p>
<h3><span style="font-weight: 400;">Which ones should I use, must I use?</span></h3>
<p><span style="font-weight: 400;">Excluding those who are building a public sandpit or honey-trap, no-one can say the &#8220;Authentication&#8221; bad guy or the &#8220;Authorization&#8221; bad guy would be an acceptable visitor to their database. So you should at least be using Authorization and Authentication.</span></p>
<p><span style="font-weight: 400;">Network encryption is almost a no-brainer for me too, but I assume insecure networks as a matter of course. If your MongoDB cluster and all its clients are, for example, inside a virtual private network that you believe has no firewall holes and no privilege escalation risk from other apps then no, you don&#8217;t need network encryption.</span></p>
<p><span style="font-weight: 400;">But if you are a network security expert who is good enough to guarantee your VPN is risk-free, I assume you&#8217;re also skilled at TLS/SSL certificate generation. In this case, you will find that setting up TLS/SSL in MongoDB is pretty easy, so why not do it too?</span></p>
<p><span style="font-weight: 400;">Storage encryption (a.k.a. encryption-at-rest) and Auditing are only high value when certain other risks are eliminated (e.g. unix root access can&#8217;t be gained by an attacker on the servers running the live mongod nodes). Storage encryption has a slight performance cost. If Auditing is used suitably, there is not much performance cost, but beware that performance will quickly choke if audit filters are made too broad.</span></p>
<p><span style="font-weight: 400;">Having said that, it is worth repeating that these last two are high value to some users. If you know you are one of those, please refer to our later articles on them.</span></p>
<h2><span style="font-weight: 400;">Where in the config?</span></h2>
<p><span style="font-weight: 400;">Although it&#8217;s all security, it isn&#8217;t configured all in the same place.</span></p>
<table style="border-collapse: collapse; boder: 2px solid grey;">
<tbody>
<tr>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top; width: 10em;">Authentication</td>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;"><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#security-options" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><b><i>security</i></b></a><i>.authorization</i>, (and/or <i>&#8230;keyfile</i> or<i> &#8230;clusterAuthMode</i> which imply/force it too).<br />
<a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#security-sasl-options" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><i>security.sasl</i></a> and <a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#security-ldap-options" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><i>security.ldap</i></a> are sub-sections for those optional authentication methods.</td>
</tr>
<tr>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;">Authorization</td>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;"><i>(Enabled simultaneously with &#8220;Authentication&#8221; above.)<br />
</i>Users and roles are not in config files &#8211; they are stored within the db itself. Typically in the <i>admin</i> db&#8217;s <i>system.users</i> and <i>system.roles </i>collections).</td>
</tr>
<tr>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;">Network encryption</td>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;"><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#net-ssl-options" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><b><i>net</i></b><i>.ssl</i></a> N.b. not inside the <i>security.*</i> section of the config file.</td>
</tr>
<tr>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;">Storage encryption</td>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;"><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#security.enableEncryption" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><b><i>security</i></b><i>.enableEncryption</i></a> is disk encryption</td>
</tr>
<tr>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;">Auditing</td>
<td style="padding: 4px 2px 4px 8px; border: 1px solid grey; vertical-align: top;"><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#auditlog-options" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><b><i>auditLog</i></b></a> section of the config file, especially the <a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#auditLog.filter" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><i>auditLog.filter</i></a> option.</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400;">The above config pointers are just the root positions; in total there are dozens of different settings underneath them.</span></p>
<h2><span style="font-weight: 400;">Authentication and Authorization</span></h2>
<p><em><strong>Question: &#8220;These Authxxxx and Authyyyy words … the same thing right?&#8221;</strong></em></p>
<p><em><strong>The Answer: 1) No, 2) Yes. 3) Yes, 4) No.</strong></em></p>
<ol>
<li><b> No</b><span style="font-weight: 400;">: Authentication and authorization are </span><i><span style="font-weight: 400;">not</span></i><span style="font-weight: 400;"> the same things because they are two parts of the software that do different things</span></li>
</ol>
<p><span style="font-weight: 400;">Authentication  == User Identity, by means of credential checking.</span></p>
<p><span style="font-weight: 400;">Authorization == Assigning and enforcing DB object and DB command permissions.</span></p>
<ol start="2">
<li><b> Yes</b><span style="font-weight: 400;">: Authentication and authorization are kind of a single unit because enabling Authentication automatically enables Authorization too.</span></li>
</ol>
<p><span style="font-weight: 400;">I assume it was made like this because this matches user expectations from other older databases, and besides, why authenticate if you don&#8217;t want to stop unknown users for accessing or changing data? Authorization is enabled in unison with authentication, so connections from unknown users will have no privilege to do anything with database data.</span></p>
<p><span style="font-weight: 400;">Authorization requires the user name (verified by Authentication) to know which privileges apply to a connection&#8217;s requests. So it can&#8217;t be enabled independently to the other either.</span></p>
<ol start="3">
<li><b> Yes</b><span style="font-weight: 400;">: Authentication and authorization </span><i><span style="font-weight: 400;">are</span></i><span style="font-weight: 400;"> sort of the same thing in unfortunate, legacy naming of configuration options</span></li>
</ol>
<p><span style="font-weight: 400;">The commandline argument for enabling authentication (which forces authorization to be on too) is simply &#8220;&#8211;auth&#8221;. Even worse, the configuration file option name for the same thing authentication is <code>security.authorization</code> rather than <code>security.authentication</code>. When you use it, though, the first thing that is being enabled is Authentication, and Authorization is only enabled as an after-effect.</span></p>
<ol start="4">
<li><b> No</b><span style="font-weight: 400;">: There is one exception to the &#8216;Authentication and authorization on together&#8217; rule: during initial setup Authentication is disabled for </span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-users/#localhost-exception" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">localhost connections</span></a><span style="font-weight: 400;">. This is brief though &#8211; you get one opportunity to create the first user, then the exception privilege is dropped.</span></li>
</ol>
<p><span style="font-weight: 400;">Another exception is when 3.4+ replica set or cluster uses </span><a target="_blank" href="https://docs.mongodb.com/manual/reference/configuration-options/index.html#security.transitionToAuth" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">security.transitionToAuth</span></a><span style="font-weight: 400;">, but its point is obvious and I won&#8217;t expand on it here.</span></p>
<h3><span style="font-weight: 400;">Auth &#8211; Still disabled by default</span></h3>
<p><span style="font-weight: 400;">MongoDB&#8217;s first versions set &#8220;&#8211;auth&#8221; off by default. This has been widely regarded as a bad move.</span></p>
<p><span style="font-weight: 400;">You might think that by now (mid-2019, v4.0) it would be on by default &#8211; but </span><b>you&#8217;d be wrong</b><span style="font-weight: 400;">. Blank configuration still equates to authorization being off, albeit with startup warnings (</span><a target="_blank" href="https://jira.mongodb.org/browse/SERVER-22708" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">on</span></a><span style="font-weight: 400;"> and </span><a target="_blank" href="https://jira.mongodb.org/browse/SERVER-23938" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">off</span></a><span style="font-weight: 400;"> again in v3.2, </span><a target="_blank" href="https://jira.mongodb.org/browse/SERVER-23838" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">on</span></a><span style="font-weight: 400;"> again v3.4) and various exposure reductions such as </span><a target="_blank" href="https://jira.mongodb.org/browse/SERVER-28229" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">localhost becoming the only default-bound network device</span></a><span style="font-weight: 400;"> in v3.6.</span></p>
<p><span style="font-weight: 400;">Feeling nervous about your MongoDB instances now? If the mongod config files do not have </span><i><span style="font-weight: 400;">security.authorization</span></i><span style="font-weight: 400;"> set to &#8220;enabled&#8221;, nor include </span><i><span style="font-weight: 400;">security.keyfile</span></i><span style="font-weight: 400;"> or a </span><i><span style="font-weight: 400;">security.clusterAuthMode</span></i><span style="font-weight: 400;"> settings which force it on, then you are not using authentication. You can try this quick mongo shell one-liner (with no user credential arguments set) to double-check if you have authentication and authorization enabled or not. </span></p>
<p style="padding-left: 40px;"><span style="font-weight: 400;"><code>mongo --host &lt;target_host&gt;:&lt;port&gt; --quiet --eval 'db.adminCommand({listDatabases: 1})'</code></span></p>
<p><span style="font-weight: 400;">The response you </span><i><span style="font-weight: 400;">want to see</span></i><span style="font-weight: 400;"> is an &#8220;unauthorized&#8221; error. If you get a list of the database names on the other hand, sorry, you have a naked MongoDB deployment.</span></p>
<p><span style="font-weight: 400;">Note: Stick with the &#8220;listDatabases&#8221; example above for simplicity. There are some commands such as </span><i><span style="font-weight: 400;">ismaster</span></i><span style="font-weight: 400;"> that don&#8217;t require authorization at any time. If you use those, you aren&#8217;t proving or disproving anything about auth mode.</span></p>
<h3><span style="font-weight: 400;">External Authentication</span></h3>
<p><span style="font-weight: 400;">As most people intuitively expect, this is about allowing users to be authenticated in an external service. As one exception it can&#8217;t be used for the internal mongodb __system user, but using it for any real human user or client application service account is perfectly suitable. In concrete terms, the external auth service will be a Kerberos KDC, or an ActiveDirectory or OpenLDAP server. </span></p>
<p><span style="font-weight: 400;">Using external authentication doesn&#8217;t prevent you from having ordinary MongoDB user accounts at the same time. A common situation is that one DBA user for setup and maintenance reasons was created in the normal way (i.e. with </span><i><span style="font-weight: 400;">db.createUser(&#8230;) </span></i><span style="font-weight: 400;">in the &#8220;admin&#8221; db) but otherwise every other account is managed centrally in Kerberos or LDAP.</span></p>
<h3><span style="font-weight: 400;">Internal Authentication</span></h3>
<p><span style="font-weight: 400;">Confusingly MongoDB &#8220;internal authentication&#8221; doesn&#8217;t mean the opposite of the external authentication discussed just above.</span></p>
<p><span style="font-weight: 400;">It would have been better named &#8216;peer mongodb node authentication as the __system user&#8217;. A mongod node running with authentication enabled won&#8217;t trust that any TCP peer is another mongod or mongos node just because it talks like one. Rather it requires that the peer authenticates by proof of a shared secret.</span></p>
<h4><span style="font-weight: 400;">Keyfile Internal Authentication (Default)</span></h4>
<p><span style="font-weight: 400;">In the basic case, the shared secret is the </span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-internal-authentication/#keyfiles"><span style="font-weight: 400;">keyfile</span></a><span style="font-weight: 400;"> saved in an identical file distributed to each mongod and mongos node in the cluster. &#8220;Key&#8221; suggests an asymmetric encryption key but in reality, it is just a password even if you generated it from /dev/random, etc. per the documentation&#8217;s advice.</span></p>
<p><span style="font-weight: 400;">Once the password is used successfully, a mongod node will permit commands coming from the authenticated peer to run as the &#8220;__system&#8221; superuser.</span></p>
<p><span style="font-weight: 400;"><strong>Unfunny fact</strong>: if someone has a copy of the keyfile they can simply strip control and non-printing chars from the key file to make the password string that will let them connect as the &#8220;__system&#8221; user.</span></p>
<p style="padding-left: 40px;"><code>mongo --authenticationDatabase local -u __system -p "$(tr -d '\011-\015\040' &lt; /path/to/keyfile)"</code></p>
<p><span style="font-weight: 400;">Don&#8217;t panic if you try this right now as the mongod (or root) user on one of your MongoDB servers and it succeeds. These unix users already have the permissions to disable security and restart nodes anyway. It is not an extra vulnerability if they can do it. There won&#8217;t be accidental read-privilege leaking either &#8211; mongod will abort on startup if the keyfile is in anything other than 400 (or 600) file permissions mode.</span></p>
<p><span style="font-weight: 400;">It is, however, a security failure if users who aren&#8217;t DBAs (or the server admins with root) are able to read a copy of the keyfile. This can happen by accidentally saving the keyfile in your world-readable source control, or putting them in deployment &#8216;recipes&#8217;. An intermediate-risk increase is when the keyfile is distributed with mongos nodes owned and run as one of the application team&#8217;s unix users instead of &#8220;mongod&#8221; or other DBA team-owned unix user.</span></p>
<h4><strong>X.509 Internal Authentication</strong></h4>
<p><span style="font-weight: 400;">The x.509 authentication mechanism does actually use asymmetric public/private keys, unlike the &#8220;security.keyfile&#8221; above. It must be used in conjunction with TLS/SSL.</span></p>
<p><span style="font-weight: 400;">It can be used for client connections as well as internal authentication. Information regarding x.509 authentication is spread over two places in the documentation as a result.</span></p>
<p><span style="font-weight: 400;">The benefit of x.509 is that, compared to the really-just-a-big-password &#8216;keyfile&#8217; above, it is less likely that one of the keys deployed with mongod and mongos nodes can be abused by an attacker who gets a copy of it. It depends on how strictly the x.509 certificates are set up, however. To be practical if you do not have a dedicated security team that understands x.509 concepts and best practices, and takes on the administrative responsibility for it, you won&#8217;t be getting the best points of x.509. These better practices include tightening down which hosts it will work on and being able to revoke and rollover certificates.</span></p>
<h3><strong>Following up</strong></h3>
<p><span style="font-weight: 400;">That&#8217;s the end of &#8216;five bad guys&#8217; overview for MongoDB security, with some clarification about Authentication and Authorization thrown in for good measure.</span></p>
<p><span style="font-weight: 400;">To give them the space they deserve, the following two subsystems will be covered in later articles:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Network encryption</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Storage encryption (a.k.a. disk encryption or encryption-at-rest)</span></li>
</ul>
<p><span style="font-weight: 400;">For Auditing please see this earlier Percona blog <a target="_blank" href="https://www.percona.com/blog/2017/03/03/mongodb-audit-log-why-and-how/">MongoDB Audit Log.</a></span></p>
<h3><span style="font-weight: 400;">Quick documentation links</span></h3>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Authentication (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/authentication/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB docs</span></a><span style="font-weight: 400;">)</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">External authentication (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/authentication-mechanisms-enterprise/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB Enterprise docs</span></a><span style="font-weight: 400;">) (</span><a target="_blank" href="https://www.percona.com/doc/percona-server-for-mongodb/ext-auth.html"><span style="font-weight: 400;">Percona Server for MongoDB</span></a><span style="font-weight: 400;">)</span></li>
</ul>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Authorization a.k.a Role-based Access Control (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/authorization/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB docs</span></a><span style="font-weight: 400;">)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Network encryption a.k.a. TLS/SSL Transport Encryption (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-transport-encryption/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB docs</span></a><span style="font-weight: 400;">)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Storage encryption a.k.a. Encryption-at-rest (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/security-encryption-at-rest/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB docs</span></a><span style="font-weight: 400;">) (</span><a target="_blank" href="https://www.percona.com/doc/percona-server-for-mongodb/4.0/data_at_rest_encryption.html"><span style="font-weight: 400;">Percona Server for MongoDB docs</span></a><span style="font-weight: 400;">)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Auditing (</span><a target="_blank" href="https://docs.mongodb.com/manual/core/auditing/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">MongoDB Enterprise docs</span></a><span style="font-weight: 400;">) (</span><a target="_blank" href="https://www.percona.com/doc/percona-server-for-mongodb/auditing.html"><span style="font-weight: 400;">Percona Server for MongoDB docs</span></a><span style="font-weight: 400;">)
<p><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Learn more about Percona Server for MongoDB</a></span></li>
</ul>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/12/mongodb-security-vs-five-bad-guys/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/12/mongodb-security-vs-five-bad-guys/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>John Lionis</name>
					</author>
		<title type="html"><![CDATA[Docker Security Considerations Part I]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/11/docker-security-considerations-part-i/" />
		<id>https://www.percona.com/blog/?p=58745</id>
		<updated>2019-07-11T18:15:56Z</updated>
		<published>2019-07-11T14:52:48Z</published>
		<category scheme="https://www.percona.com/blog" term="Docker" /><category scheme="https://www.percona.com/blog" term="Security" /><category scheme="https://www.percona.com/blog" term="security" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Docker security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />Docker Security Considerations – PART I Why Docker Security Matters It is a fact that Docker has found widespread use during the past years, mostly because it is very easy to use as well as fast and easy to deploy when compared with a full-blown virtual machine. More and more servers are being operated as [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/11/docker-security-considerations-part-i/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Docker security" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><em><b><img class="alignright wp-image-58917 size-thumbnail" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg" alt="Docker security" width="200" height="133" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Docker-Security.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />Docker Security Considerations – PART I</b></em></p>
<h3><b>Why Docker Security Matters</b></h3>
<p>It is a fact that Docker has found widespread use during the past years, mostly because it is very easy to use as well as fast and easy to deploy when compared with a full-blown virtual machine. More and more servers are being operated as Docker hosts on which micro-services run in containers. From a security point of view, two aspects of it arise in the context of this blog post and the inherent limitations it has as a means of communication.</p>
<p>First, the answer to the already quite talked-through question, “Is it secure?” Second, the less analyzed aspect of incident analysis, and the changes introduced with respect to known methods and evidence. In order to answer the first question, some key facts about Docker need to be noted. The second aspect, being less examined in general, will be a future post in this mini-series about Docker security.</p>
<p>The basic technology that enables containers in Linux is kernel name-spaces. These are what allow the kernel to offer private resources to a process, while hiding the resources of other processes, all without actually creating full guest OSes or kernels per process. One very important fact is that containers run without cgroup limitations by default, but CPU and memory usage can be easily constrained with the application of a few flags. Docker presents an option called cgroup-parent for this purpose, and you can check the official docker <a target="_blank" href="https://docs.docker.com/engine/reference/commandline/dockerd/#miscellaneous-options">documentation on cgroup-parent</a>. Under no circumstances should containers (or any process) be allowed to run on a production server without such cgroup limitations. In addition, predefined resource consumption for containers could help in limiting the costs for cloud infrastructure as it better defines them, leading to more targeted purchases.</p>
<p>Docker uses a default profile to create a system call whitelist. In this profile, “dangerous” syscalls are missing. If you have a solid understanding of exactly the system calls needed by your containerized process, you can customize this profile or write your own to lock out every syscall not needed by your container. Also, note that custom profiles can be applied on a container-by-container basis. In addition, Linux capabilities can be a very good way with regards to considerations on Docker security. Quoting from the man-pages:</p>
<blockquote><p>For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero). Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process&#8217;s credentials (usually: effective UID, effective GID, and supplementary group list).</p>
<p>Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.  Capabilities are a per-thread attribute.</p></blockquote>
<p>In the example below, we can see the basic syntax for manipulating and auditing capabilities. Perhaps the most secure way to run Docker is using the &#8211;user flag (no capabilities), but this is not always possible since there is no way to add capabilities to a nonzero UID. In cases where this is needed, running as root with &#8211;cap-drop ALL and &#8211;cap-add to add back the minimum set of capabilities is the most secure practical configuration. Note that since a Docker container is fundamentally just a process, it’s simple to enforce these permissions at runtime using the native Linux functionality designed to manage capabilities per process. No ‘under-the-hood’ configuration inside the container is required, but it would be to manipulate the capabilities of groups of processes running in a VM.</p>
<p>A closer look at these should highlight the need to manage them and will give us a better understanding of what&#8217;s going on. The default list of capabilities available to privileged processes in a docker container (mostly quoting<a target="_blank" href="https://www.redhat.com/en/blog/secure-your-containers-one-weird-trick" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"> https://www.redhat.com/en/blog/secure-your-containers-one-weird-trick</a>) is presented below. A complete list of capabilities can be found at the following link <a target="_blank" href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">http://man7.org/linux/man-pages/man7/capabilities.7.html</a>. At this point we should note that in Dockerfile v3 and above, capabilities can be added and images built; e.g.</p><pre class="crayon-plain-tag">cap_drop:

- ALL

cap_add:

- chown</pre><p>The default list of capabilities available in a Docker container:</p>
<p><b>chown</b>:</p>
<p>This is described as the ability to make arbitrary changes to file UIDs and GIDs. Unless a shell is running inside a container and packages installed into it, this should be dropped. If chown is needed, allow it, do the work, and then drop it again.</p>
<p><b>dac_override:</b></p>
<p>(Discretionary access control)_override.</p>
<p>Man pages state that this cap allows root to bypass file r, w &amp; x permissions. It is as odd as it sounds, that any app will need this. As noted by RedHat sec experts “nothing should need this. If your container needs this, it probably doing something horrible.”</p>
<p><b>fowner</b>:</p>
<p>fowner transfers the ability to bypass permission checks on operations that normally require the file system UID to match the UID of the file. Quite similar to dac_overide. Could be needed for docker build but it should be dropped when the container is in production.</p>
<p><b>fsetid</b>:</p>
<p>Quoting the man page &#8220;don&#8217;t clear set-user-ID and set-group-ID mode bits when a file is modified; set the set-group-ID bit for a file whose GID does not match the file system or any of the supplementary GIDs of the calling process.&#8221;</p>
<p>So, if you are not running an installation, you probably do not need this capability.</p>
<p><b>kill</b></p>
<p>This capability basically means that a root-owned process can send kill signals to non-root processes. If your container is running all processes as root or the root processes never kills processes running as non-root, you do not need this capability. If you are running systemd as PID 1 inside of a container, and you want to stop a container running with a different UID, you <i>might</i> need this capability.</p>
<p><b>setgid</b></p>
<p>In short, a process with this capability can change its GID to any other GID. Basically allows full group access to all files on the system. If your container processes do not change UIDs/GIDs, they do not need this capability.</p>
<p><b>setuid</b></p>
<p>A process with this capability can change its UID to any other UID. Basically, it allows full access to all files on the system. If your container processes do not change UIDs/GIDs always running as the same UID, preferably non-root, they do not need this capability. Applications that need setuid usually start as root in order to bind to ports below 1024 and then change their UIDS and drop capabilities. Apache binding to port 80 requires net_bind_service, usually starting as root. It then needs setuid/setgid to switch to the apache user and drop capabilities. Most containers can safely drop setuid/setgid capability.</p>
<p><b>setpcap</b></p>
<p>From the man page description: &#8220;Add any capability from the calling thread&#8217;s bounding set to its inheritable set; drop capabilities from the bounding set (via prctl(2) PR_CAPBSET_DROP); make changes to the securebits flags.&#8221;</p>
<p>A process with this capability can change its current capability set within its bounding set. Meaning a process could drop capabilities or add capabilities if it did not currently have them, but is limited by the bounding set capabilities.</p>
<p><b>net_bind_service</b></p>
<p>If you have this capability, you can bind to privileged ports (&lt; 1024).</p>
<p>When binding to a port below 1024 is needed, this capability will do the job. For services that listen to a port above 1024, this capability should be dropped. The risk of this capability is a rogue process interpreting a service like sshd, and collecting user passwords. Running a container in a different network namespace reduces the risk of this capability. It would be difficult for the container process to get to the public network interface</p>
<p><b>net_raw</b></p>
<p>This access allows a process to spy on packets on its network. Most container processes would not need this access so it probably should be dropped. Note this would only affect the containers that share the same network that your container process is running on, usually preventing access to the <i>real</i> network. RAW sockets also give an attacker the ability to inject almost anything onto the network.</p>
<p><b>sys_chroot</b></p>
<p>This capability allows the use of chroot(). In other words, it allows your processes to chroot into a different rootfs. chroot is probably not used within your container, so it should be dropped.</p>
<p><b>mknod</b></p>
<p>If you have this capability, you can create special files using mknod.</p>
<p>This allows your processes to create device nodes. Containers are usually provided all of the device nodes they need in /dev, so this could and should be dropped by default. Almost no containers ever do this, and even fewer containers <i>should</i> do this.</p>
<p><b>audit_write</b></p>
<p>With this capability, you can write a message to the kernel auditing log. Few processes attempt to write to the audit log (login programs, su, sudo) and processes inside of the container are probably not trusted. The audit subsystem is not currently namespace-aware, so this should be dropped by default.</p>
<p><b>setfcap</b></p>
<p>Finally, the setfcap capability allows you to set file capabilities on a file system. Might be needed for doing installs during builds, but in production, it should probably be dropped.</p>
<p>The following exercise, the output of which is displayed below, should highlight the way of manipulating capabilities along with some of the points made above.</p>
<p>First, we run a container that changes ownership of the root fs:</p><pre class="crayon-plain-tag">Status: Image is up to date for centos:latest
john@seceng:~/Documents/docker-sec$
john@seceng:~/Documents/docker-sec$ docker container run --rm centos:latest chown nobody /
john@seceng:~/Documents/docker-sec$</pre><p>We can see that the root user is able to execute this without a problem.</p>
<p>Next, we drop all capabilities from the user in the container, and when we try running the above command again, we can see it fails, as the user no longer has the necessary chown capability (or any other root capabilities):</p><pre class="crayon-plain-tag">john@seceng:~/Documents/docker-sec$ docker container run --rm --cap-drop all centos:latest chown nobody /
chown: changing ownership of '/': Operation not permitted
john@seceng:~/Documents/docker-sec$</pre><p>Now we add the necessary capability back in:</p><pre class="crayon-plain-tag">john@seceng:~/Documents/docker-sec$ docker container run --rm --cap-drop all --cap-add chown centos:latest chown nobody /
john@seceng:~/Documents/docker-sec$</pre><p>Dropping all capabilities and adding back only the required ones makes for the most secure containers. An attacker who breaches this container won’t have any root capabilities except chown.</p>
<p>Now, if we try adding chown to a non-root user, the operation fails since capabilities manipulation won’t grant additional capabilities under any circumstances to a process run by a non-root user, as noted on the following screenshot.</p><pre class="crayon-plain-tag">john@seceng:~/Documents/docker-sec$ docker container run --rm --user 1000:1000 --cap-add chown centos:latest chown nobody /
chown: changing ownership of '/': Operation not permitted
john@seceng:~/Documents/docker-sec$</pre><p>Another Linux feature that can aid the process of securing our containers is Secure computing mode (seccomp). Seccomp is a Linux kernel feature. You can use it to restrict the actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. You can use this feature to restrict your application’s access.</p>
<p>This feature is available only if Docker has been built with seccomp and the kernel is configured with CONFIG_SECCOMP enabled. More info about the seccomp profiles for Docker can be found <a target="_blank" href="https://docs.docker.com/engine/security/seccomp/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">here</a>. To check if your kernel supports seccomp:</p><pre class="crayon-plain-tag">john@seceng:~/Documents/docker-sec$ grep CONFIG_SECCOMP= /boot/config-$(uname -r)
CONFIG_SECCOMP=y
john@seceng:~/Documents/docker-sec$</pre><p>While capabilities and seccomp allow restricting access to root capabilities and system calls, SELinux goes a step further and helps protect the file system of the host from malicious code. Default SELinux labels restrict access to directories and those labels can be changed by the :z and :Z flags when mounting directories to relabel those directories for sharing or private use by containers, respectively.</p>
<p>Additionally, AppArmor (Application Armor) is a Linux security module that protects an operating system and its applications from security threats. To use it, a system administrator associates an AppArmor security profile with each program. Docker expects to find an AppArmor policy loaded and enforced. Docker automatically generates and loads a default profile for containers named docker-default. On Docker versions 1.13.0 and later, the Docker binary generates this profile in tmpfs and then loads it into the kernel. On Docker versions earlier than 1.13.0, this profile is generated in /etc/apparmor.d/docker instead. It should be noted, though, that this profile is used on containers, <i>not</i> on the Docker daemon. More on AppArmor and Docker can be found at <a target="_blank" href="https://docs.docker.com/engine/security/apparmor/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">docker&#8217;s official documentation.</a></p>
<p>Last in this list of facts, but certainly not in general, is the default software-defined network firewalling imposed by Docker, and the iptables rules automatically managed by Docker. The Docker Networking Model was designed around software-defined networks exactly so that Docker could easily manage iptables firewall rules on create and destroy operations of Docker networks and containers. Rather than writing complicated iptables rules explicitly, effective firewalling can be imposed on a containerized data center simply by designing and declaring a sensible software-defined network topology.</p>
<p>Basically, but for sure not limited to, Docker Security considerations rise because the Docker daemon runs as root and is being used widely in production systems. Having said that and taking into account the previously mentioned facts, one can easily understand that configuration, bugs, vulnerabilities or other security issues are of high risk and impact factor. With this in mind, the following example, which highlights the non-secure default configuration of Docker, is probably the best “teaser” for the next part of this post, in which we will cover some recent vulnerabilities and their mitigation.</p><pre class="crayon-plain-tag">john@seceng:~/Documents/docker-sec$ id

uid=1000(john) gid=1000(john) groups=1000(john),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),108(netdev),998(docker)
john@seceng:~/Documents/docker-sec$ whoami

john
john@seceng:~/Documents/docker-sec$ docker run -v /home/${user}:/h_docs ubuntu bash -c "cp /bin/bash /h_docs/rootshell &amp;&amp; chmod 4777 /h_docs/rootshell;" &amp;&amp; ~/rootshell -p

rootshell-4.4# id
uid=1000(john) gid=1000(john) euid=0(root) groups=1000(john),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),108(netdev),998(docker)
rootshell-4.4# whoami

root
rootshell-4.4# uname -a

Linux seceng 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64 GNU/Linux
rootshell-4.4# exit

exit
john@seceng:~/Documents/docker-sec$</pre><p>&nbsp;</p>
<p style="text-align: center;">From user to root, in one line. It’s there by default.</p>
<p style="text-align: right;">John Lionis, Security Engineer | Percona LLC</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/11/docker-security-considerations-part-i/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/11/docker-security-considerations-part-i/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Borys Belinsky</name>
						<uri>http://www.percona.com</uri>
						</author>
		<title type="html"><![CDATA[Percona XtraBackup 2.4.15 Is Now Available]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/10/percona-xtrabackup-2-4-15-is-now-available/" />
		<id>https://www.percona.com/blog/?p=58749</id>
		<updated>2019-07-10T16:05:37Z</updated>
		<published>2019-07-10T16:05:37Z</published>
		<category scheme="https://www.percona.com/blog" term="Events and Announcements" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona Software" /><category scheme="https://www.percona.com/blog" term="Percona XtraBackup" /><category scheme="https://www.percona.com/blog" term="artful" /><category scheme="https://www.percona.com/blog" term="backup" /><category scheme="https://www.percona.com/blog" term="Recovery" /><category scheme="https://www.percona.com/blog" term="xbcrypt" />		<summary type="html"><![CDATA[<img width="180" height="150" src="https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-180x150.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona XtraBackup 8.0" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-180x150.jpg 180w, https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-300x250.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK.jpg 500w" sizes="(max-width: 180px) 100vw, 180px" />Percona is glad to announce the release of Percona XtraBackup 2.4.15 on July 10, 2019. You can download it from our download site and apt and yum repositories. Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/10/percona-xtrabackup-2-4-15-is-now-available/"><![CDATA[<img width="180" height="150" src="https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-180x150.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona XtraBackup 8.0" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-180x150.jpg 180w, https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK-300x250.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2017/11/Percona_XtraBackupLogoVert_CMYK.jpg 500w" sizes="(max-width: 180px) 100vw, 180px" /><p><em><a target="_blank" href="https://www.percona.com/blog/wp-content/uploads/2016/07/Percona_XtraBackupLogoVert_CMYK-1.jpg"><img class="alignright wp-image-36920 size-full" src="https://www.percona.com/blog/wp-content/uploads/2016/07/Percona_XtraBackupLogoVert_CMYK-1-e1496163100924.jpg" alt="Percona XtraBackup" width="240" height="200" /></a></em>Percona is glad to announce the release of <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-xtrabackup">Percona XtraBackup 2.4.15</a> on July 10, 2019. You can download it from our <a target="_blank" href="https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/">download site</a> and <a target="_blank" href="https://www.percona.com/doc/percona-xtrabackup/2.4/installation/apt_repo.html">apt</a> and <a target="_blank" href="https://www.percona.com/doc/percona-xtrabackup/2.4/installation/yum_repo.html">yum</a> repositories.</p>
<p>Percona XtraBackup enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Offered free as an open source solution, it drives down backup costs while providing unique features for MySQL backups.</p>
<p>All Percona software is open-source and free.</p>
<h4>Bugs Fixed</h4>
<ul>
<li>When the <em>encrypted tablespaces</em> feature was enabled, encrypted and compressed<br />
tables were not usable on the joiner node (Percona XtraDB Cluster) via SST<br />
(State Snapshot Transfer) with the <code>xtrabackup-v2</code> method. Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1867" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1867</a>.</li>
<li><code>xbcloud</code> did not update date related fields of the HTTP<br />
header when retrying a request. Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1874" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1874</a>.</li>
<li><code>xbcloud</code> did not retry to send the request after receiving the HTTP 408<br />
error (request timeout). Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1875" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1875</a>.</li>
<li>If the user tried to merge an already prepared incremental backup, a<br />
misleading error was produced without informing that incremental backups may<br />
not be used twice. Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1862" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1862</a>.</li>
<li><code>xbcloud</code> could crash with the <em>Swift</em> storage when project options were<br />
not included. Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1844" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1844</a>.</li>
<li><code>xtrabackup</code> did not accept decimal fractions as values of the<br />
<code>innodb_max_dirty_pages_pct</code> option. Bug fixed <a target="_blank" href="https://jira.percona.com/browse/PXB-1807" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1807</a>.</li>
</ul>
<p><strong>Other bugs fixed</strong>:  <a target="_blank" href="https://jira.percona.com/browse/PXB-1850" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1850</a>, <a target="_blank" href="https://jira.percona.com/browse/PXB-1879" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1879</a>, <a target="_blank" href="https://jira.percona.com/browse/PXB-1887" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1887</a>, <a target="_blank" href="https://jira.percona.com/browse/PXB-1888" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1888</a>, <a target="_blank" href="https://jira.percona.com/browse/PXB-1890" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PXB-1890</a>.</p>
<p>Release notes with all the improvements for Percona XtraBackup 2.4.15 are available in our <a target="_blank" href="https://www.percona.com/doc/percona-xtrabackup/2.4/release-notes/2.4/2.4.15.html">online documentation</a>. Please report any bugs to the <a target="_blank" href="https://jira.percona.com/projects/PXB" target="_blank" rel="&quot;&quot;&quot;nofollow noopener noreferrer" noopener noreferrer">issue tracker</a>.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/10/percona-xtrabackup-2-4-15-is-now-available/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/10/percona-xtrabackup-2-4-15-is-now-available/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Jobin Augustine</name>
					</author>
		<title type="html"><![CDATA[PostgreSQL WAL Retention and Clean Up: pg_archivecleanup]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/10/wal-retention-and-clean-up-pg_archivecleanup/" />
		<id>https://www.percona.com/blog/?p=58578</id>
		<updated>2019-07-10T14:54:27Z</updated>
		<published>2019-07-10T14:44:08Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="93" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x93.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x93.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-300x139.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-1024x474.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-367x170.jpeg 367w" sizes="(max-width: 200px) 100vw, 200px" />WAL retention is a very important topic for PostgreSQL database management. But very often we come across DBAs getting into surprise situations such as: 1. Several TBs of WALs piled up in archive destination 2. WALs filling up pg_wal/pg_xlog directory due to failing archive 3. Necessary WALs are no longer preserved External backup projects for [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/10/wal-retention-and-clean-up-pg_archivecleanup/"><![CDATA[<img width="200" height="93" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x93.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x93.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-300x139.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-1024x474.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-367x170.jpeg 367w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-58888" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x127.png" alt="PostgreSQL WAL Retention pg_archivecleanup" width="200" height="127" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup-200x127.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PostgreSQL-WAL-Retention-pg_archivecleanup.png 300w" sizes="(max-width: 200px) 100vw, 200px" />WAL retention is a very important topic for PostgreSQL database management. But very often we come across DBAs getting into surprise situations such as:</p>
<p><strong>1.</strong> Several TBs of WALs piled up in archive destination</p>
<p><strong>2.</strong> WALs filling up pg_wal/pg_xlog directory due to failing archive</p>
<p><strong>3.</strong> Necessary WALs are no longer preserved</p>
<p>External backup projects for PostgreSQL are good in addressing retention policies. But there is a simple program named <strong><a target="_blank" href="https://www.postgresql.org/docs/current/pgarchivecleanup.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">pg_archivecleanup</a></strong> which comes along with PostgreSQL binaries which might be useful in both maintaining the WAL retention as well as handling an emergency situation. pg_archivecleanup is probably the least talked about utility among standard PostgreSQL binaries. It is extremely simple and useful, and it can work in 2 scenarios:</p>
<ul>
<li>Can be used as a standalone program to clean up old WAL files from any file system location.</li>
<li>It can also be used in Standby side for cleaning up WAL files which are no longer required for Standby.</li>
</ul>
<p>This program can operate in dryrun mode (-n option) or delete mode (-d option).</p>
<h3>Dryrun (-n)</h3>
<p>This feature is useful to list all WALs older than a specific WAL. In the following demonstration, I am listing all WALs which are still in the default wal location (pg_wal) inside my data directory and older than a specific WAL:</p><pre class="crayon-plain-tag">$ ./pg_archivecleanup -n ~/bigsql/data/pg11/pg_wal 00000001000000000000001E
/home/jobin/bigsql/data/pg11/pg_wal/00000001000000000000001B
/home/jobin/bigsql/data/pg11/pg_wal/000000010000000000000017
...</pre><p></p>
<h3>Delete Mode (-d)</h3>
<p>In this mode, pg_archivecleanup does the cleanup by removing all the candidate WALs:</p><pre class="crayon-plain-tag">$ pg_archivecleanup -d /home/postgres/archive 00000002000000000000006B
pg_archivecleanup: keeping WAL file "/home/postgres/archive/00000002000000000000006B" and later
pg_archivecleanup: removing file "/home/postgres/archive/000000020000000000000069"
pg_archivecleanup: removing file "/home/postgres/archive/00000002000000000000006A"
...</pre><p></p>
<h2>Understanding WAL retention criteria</h2>
<p>In order to do a clean up of WALs, we need to keep in mind the WAL retention requirements and criteria, as losing essential WALs can be disastrous.</p>
<p><strong>Criteria 1:</strong> All WALs from the time of the oldest backup, which still falls in the backup retention policy and Point-in-time requirement, need to be preserved.</p>
<p>For example, a user may ask for restore like: A table X was accidentally deleted as part of last change rolled out on Sunday night 12. Restore the database to another location and recover the table as of latest time possible. In such a case, the DBA needs to get an old backup and associated WAL files.</p>
<p><strong>Criteria 2:</strong>  All WALs from the start position of a backup until the end of the backup is required to consistently restore any backup. Many times, backup tools make a copy of WAL and include it as part of Backup. Without it, the backups will be invalid. If the backup tool/script is not taking care of WALs, the responsibility falls on us as a DBA.</p>
<p><strong>Criteria 3:</strong> All WALs from the restart point of Standby is required by the Standby database. A restart point is a point from where standby can restart the recovery operation. This is similar to Checkpoint on the Primary side.</p>
<p><strong>Criteria 4: </strong>All the WALs from the last checkpoint is required by Primary for any crash recovery.</p>
<p>pg_archivecleanup doesn’t have all the intelligence built-in, but it can be utilized inside custom scripts as an intelligent way to achieve the goals of WAL retention. Now think about how to implement all these criteria for WAL retention in archive location.</p>
<p>Criteria No.1 is simple to implement. Any file older than the backup retention period is a candidate for deletion.</p>
<p>Criteria No. 2 will be a little more complex. But luckily PostgreSQL helps us by generating a backup history file in the archive location with a <pre class="crayon-plain-tag">.backup</pre>  extension. For example:</p>
<p><pre class="crayon-plain-tag">0000000200000000000000C9.000002D0.backup</pre></p>
<p>Any WALs older than this is not required for backup. This backup history file can be directly specified pg_archivelogcleanup as follows:</p><pre class="crayon-plain-tag">$ pg_archivecleanup -d /home/postgres/archive 000000020000000000000069.000003E0.backup
pg_archivecleanup: keeping WAL file "/home/postgres/archive/000000020000000000000069" and later
pg_archivecleanup: removing file "/home/postgres/archive/00000002000000000000005D"
pg_archivecleanup: removing file "/home/postgres/archive/00000002000000000000005E"

...</pre><p>Criteria No. 3 needs to be taken care of by Standby because only Standby knows what the current restart point is. Again, a built-in feature in PostgreSQL helps us for calling a cleanup shell script. A shell script can be specified for parameter archive_cleanup_command in recovery.conf. This feature can be used for calling pg_archivecleanup like:</p><pre class="crayon-plain-tag">archive_cleanup_command = 'pg_archivecleanup archivelocation %r'</pre><p>However, things will get complicated if there is more than one standby server referring to the same archive location. One standby might be lagging behind and wants a WAL which is already cleaned by another standby. In such cases, we may have to use custom scripts/program which embeds pg_archivecleanup.</p>
<p>Criteria No. 4 requires information from the control file about the WAL file location of the checkpoint.</p><pre class="crayon-plain-tag">$ pg_controldata -D /var/lib/pgsql/11/data/
pg_control version number: 1100
Catalog version number: 201809051
Database system identifier: 6676014631308187830
Database cluster state: in production
pg_control last modified: Friday 05 July 2019 12:59:32 PM UTC
Latest checkpoint location: 3/430324A0
Latest checkpoint's REDO location: 3/37752C70
Latest checkpoint's REDO WAL file: 000000020000000300000037
Latest checkpoint's TimeLineID: 2
Latest checkpoint's PrevTimeLineID: 2</pre><p>All these different criteria create a set of requirements which can be addressed by custom scripts or are already addressed by different backup solutions for PostgreSQL. If we want to implement WAL retention policies with its own logic, we may end up executing pg_archivecleanup from multiple wrapper scripts. We may also think about adding more capabilities to pg_archivecleanup for addressing custom requirements.</p>
<p>**pg_archivecleanup can also take .partial file which gets generated as part of standby promotion as a reference WAL.</p>
<h3>pg_archivecleanup code walkthrough for the novice user</h3>
<p>Luckily this is a very small, standalone, single file, (no need of postgresql server), easy to read source code with less than 400 lines. It is designed for user customization, including extensive comments. The <pre class="crayon-plain-tag">pg_archivecleanup.c</pre>  file can be located in the <pre class="crayon-plain-tag">/src/bin/pg_archivecleanup</pre>  directory. Since it is a standalone program, users with minimal C knowledge can easily customize the program. So this writeup is about the walkthrough of the code and its normal functionality.</p>
<p>A couple of important variables which this program uses are:</p>
<p><em>Important functions</em></p>
<p><strong>Initialize(void):</strong> As the name indicates, this function will be called only once as part of the initialization of the program. It checks whether the directory location provided in the command line (value of <pre class="crayon-plain-tag">archiveLocation</pre> ) is a directory. If not, it will error out.</p>
<p><strong>TrimExtension(char *filename, char *extension):</strong> This function will be called for every file in the directory for the removal of extension. The filename will be trimmed for removing the extension.</p>
<p><strong>CleanupPriorWALFiles(void):</strong> This function opens the directory and iterates over all the files in that directory. <pre class="crayon-plain-tag">exclusiveCleanupFileName</pre> is used as a reference for comparison. Any filename smaller than this file will be considered for deletion.</p>
<p>Now the question remains is how to get the <pre class="crayon-plain-tag">exclusiveCleanupFileName</pre>, which is the reference for comparison.</p>
<p><strong>SetWALFileNameForCleanup(void):</strong> Exactly addresses the above requirement; setting the <pre class="crayon-plain-tag">exclusiveCleanupFileName</pre>. The user might be giving a WAL segment file without extensions &#8211; sometimes .partial WAL segment file, sometimes .backup file. In this function, the timeline number, logical wal/log number, and wal segment number are extracted from the file name specified and reconstitutes to form a valid WAL segment file using XLogFileNameById. m</p>
<p>Overall flow is : Initialize, SetWALFileNameForCleanup, and then CleanupPriorWALFiles.</p>
<p>Enjoy the freedom offered by PostgreSQL to modify and customize the program as a user. Happy Coding!</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/10/wal-retention-and-clean-up-pg_archivecleanup/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/10/wal-retention-and-clean-up-pg_archivecleanup/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Emily Ikuta</name>
					</author>
		<title type="html"><![CDATA[Upcoming Webinar 7/10: Learn how to run MongoDB Inside of a Containerized Environment]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/08/upcoming-webinar-7-10-learn-how-to-run-mongodb-inside-of-a-containerized-environment/" />
		<id>https://www.percona.com/blog/?p=58318</id>
		<updated>2019-07-19T20:08:43Z</updated>
		<published>2019-07-08T14:00:53Z</published>
		<category scheme="https://www.percona.com/blog" term="MongoDB" /><category scheme="https://www.percona.com/blog" term="Technical Webinars" />		<summary type="html"><![CDATA[<img width="200" height="100" src="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Inside of a Containerized Environment" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-367x184.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment.jpg 1500w" sizes="(max-width: 200px) 100vw, 200px" />Please join Percona Consultant Doug Duncan as he presents his talk “Building Kubernetes Operator for Percona Server for MongoDB” on Wednesday, July 10th, 2019 at 10:00 AM PDT (UTC-7). Register Now Doug will discuss the basic knowledge needed to understand the complications of running MongoDB inside of a containerized environment and then to go over [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/08/upcoming-webinar-7-10-learn-how-to-run-mongodb-inside-of-a-containerized-environment/"><![CDATA[<img width="200" height="100" src="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB Inside of a Containerized Environment" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-367x184.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment.jpg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-58709" src="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg" alt="MongoDB Inside of a Containerized Environment" width="200" height="100" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment-367x184.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/MongoDB-Inside-of-a-Containerized-Environment.jpg 1500w" sizes="(max-width: 200px) 100vw, 200px" />Please join Percona Consultant Doug Duncan as he presents his talk “<a target="_blank" href="https://www.percona.com/resources/webinars/building-kubernetes-operator-percona-server-mongodb">Building Kubernetes Operator for Percona Server for MongoDB</a>” on Wednesday, July 10th, 2019 at 10:00 AM PDT (UTC-7).</p>
<p style="text-align: center;"><a target="_blank" class="btn btn-primary btn-lg" href="https://www.percona.com/resources/webinars/building-kubernetes-operator-percona-server-mongodb" rel="noopener">Register Now</a></p>
<p>Doug will discuss the basic knowledge needed to understand the complications of running MongoDB inside of a containerized environment and then to go over the specifics of how Percona solved these challenges in the <a target="_blank" href="https://www.percona.com/doc/kubernetes-operator-for-psmongodb/kubernetes.html">PSMDB Operator</a>. It also will provide an overview of PSMDB Operator features, and a sneak peek at future plans.</p>
<div class="action-buttons"><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Learn more about Percona Server for MongoDB</a></div>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/08/upcoming-webinar-7-10-learn-how-to-run-mongodb-inside-of-a-containerized-environment/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/08/upcoming-webinar-7-10-learn-how-to-run-mongodb-inside-of-a-containerized-environment/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Carlos Tutte</name>
					</author>
		<title type="html"><![CDATA[Fixing MySQL 1045 Error: Access Denied]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/05/fixing-a-mysql-1045-error/" />
		<id>https://www.percona.com/blog/?p=58058</id>
		<updated>2019-07-19T20:09:41Z</updated>
		<published>2019-07-05T17:49:23Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="129" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL 1045 error Access Denied" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-300x194.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-1024x661.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-367x237.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />During our work in support, we see this again and again: &#8220;I try to connect to MySQL and am getting a 1045 error&#8221;, and most times it comes accompanied with &#8220;&#8230;but I am sure my user and password are OK&#8221;.  So we decided it was worth showing other reasons this error may occur. MySQL 1045 [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/05/fixing-a-mysql-1045-error/"><![CDATA[<img width="200" height="129" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MySQL 1045 error Access Denied" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-300x194.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-1024x661.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-367x237.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-weight: 400;"><img class="alignright wp-image-58617 size-thumbnail" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg" alt="MySQL 1045 error Access Denied" width="200" height="129" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-200x129.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-300x194.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-1024x661.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error-367x237.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MySQL-1045-Error.jpeg 1600w" sizes="(max-width: 200px) 100vw, 200px" />During our work in support, we see this again and again: &#8220;I try to connect to MySQL and am getting a 1045 error&#8221;, and most times it comes accompanied with &#8220;&#8230;but I am sure my user and password are OK&#8221;.  So we decided it was worth showing other reasons this error may occur.</span></p>
<h2>MySQL 1045 error Access Denied triggers in the following cases:</h2>
<h4>1) Connecting to wrong host:</h4>
<p></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u root -psekret
mysql: [Warning] Using a password on the command line interface can be insecure. 
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)</pre><p>If not specifying the host to connect (with -h flag), MySQL client will try to connect to the localhost instance while you may be trying to connect to another host/port instance.</p>
<p><strong>Fix:</strong> Double check if you are trying to connect to localhost, or be sure to specify host and port if it&#8217;s not localhost:</p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u root -psekret -h &lt;IP&gt; -P 3306</pre><p>&nbsp;</p>
<h4>2) User does not exist:</h4>
<p></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u nonexistant -psekret -h localhost
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'nonexistant'@'localhost' (using password: YES)</pre><p><span style="font-weight: 400;"><strong>Fix:</strong> Double check if the user exists:</span></p><pre class="crayon-plain-tag">mysql&gt; SELECT User FROM mysql.user WHERE User='nonexistant';
Empty set (0.00 sec)</pre><p>If the user does not exist, create a new user:</p><pre class="crayon-plain-tag">mysql&gt; CREATE USER 'nonexistant'@'localhost' IDENTIFIED BY 'sekret';
Query OK, 0 rows affected (0.00 sec)</pre><p>&nbsp;</p>
<h4>3) User exists but client host does not have permission to connect:</h4>
<p></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u nonexistant -psekret
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'nonexistant'@'localhost' (using password: YES)</pre><p><strong>Fix:</strong> You can check to see which host user/host MySQL allows connections with the following query:</p><pre class="crayon-plain-tag">mysql&gt; SELECT Host, User FROM mysql.user WHERE User='nonexistant';
+-------------+-------------+
| Host        | User        |
+-------------+-------------+
| 192.168.0.1 | nonexistant |
+-------------+-------------+
1 row in set (0.00 sec)</pre><p>If you need to check from which IP the client is connecting, you can use the following Linux commands for server IP:</p><pre class="crayon-plain-tag">[engineer@percona]# ip address | grep inet | grep -v inet6
    inet 127.0.0.1/8 scope host lo
    inet 192.168.0.20/24 brd 192.168.0.255 scope global dynamic wlp58s0</pre><p>or for public IP:</p><pre class="crayon-plain-tag">[engineer@percona]# dig +short myip.opendns.com @resolver1.opendns.com
177.128.214.181</pre><p>You can then create a user with correct Host (client IP), or with &#8216;%&#8217; (wildcard) to match any possible IP:</p><pre class="crayon-plain-tag">mysql&gt; CREATE USER 'nonexistant'@'%' IDENTIFIED BY 'sekret';
Query OK, 0 rows affected (0.00 sec)</pre><p>&nbsp;</p>
<h4>4) Password is wrong, or the user forgot his password:</h4>
<p></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u nonexistant -pforgotten
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'nonexistant'@'localhost' (using password: YES)</pre><p><strong>Fix:</strong> Check and/or reset password:</p>
<p>You cannot read user passwords in plain text from MySQL as the password hash is used for authentication, but you can compare hash strings with &#8220;PASSWORD&#8221; function:</p><pre class="crayon-plain-tag">mysql&gt; SELECT Host, User, authentication_string, PASSWORD('forgotten') FROM mysql.user WHERE User='nonexistant';  
+-------------+-------------+-------------------------------------------+-------------------------------------------+
| Host        | User        | authentication_string                     | PASSWORD('forgotten')                     |
+-------------+-------------+-------------------------------------------+-------------------------------------------+
| 192.168.0.1 | nonexistant | *AF9E01EA8519CE58E3739F4034EFD3D6B4CA6324 | *70F9DD10B4688C7F12E8ED6C26C6ABBD9D9C7A41 |
| %           | nonexistant | *AF9E01EA8519CE58E3739F4034EFD3D6B4CA6324 | *70F9DD10B4688C7F12E8ED6C26C6ABBD9D9C7A41 |
+-------------+-------------+-------------------------------------------+-------------------------------------------+
2 rows in set, 1 warning (0.00 sec)</pre><p>We can see that PASSWORD(&#8216;forgotten&#8217;) hash does not match the authentication_string column, which means password string=&#8217;forgotten&#8217; is not the correct password to log in. Also, in case the user has multiple hosts (with different password), he may be trying to connect using the password for the wrong host.</p>
<p>In case you need to override the password you can execute the following query:</p><pre class="crayon-plain-tag">mysql&gt; set password for 'nonexistant'@'%' = 'hello$!world';
Empty set (0.00 sec)</pre><p>&nbsp;</p>
<h4>5) Special characters in the password being converted by Bash:</h4>
<p></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u nonexistant -phello$!world 
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'nonexistant'@'localhost' (using password: YES)</pre><p><span style="font-weight: 400;"><strong>Fix:</strong> Prevent bash from interpreting special characters by wrapping password in single quotes:</span></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u nonexistant -p'hello$!world'
mysql: [Warning] Using a password on the command line interface can be insecure
...
mysql&gt;</pre><p>&nbsp;</p>
<h4>6) SSL is required but the client is not using it:</h4>
<p></p><pre class="crayon-plain-tag">mysql&gt; create user 'ssluser'@'%' identified by 'sekret';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; alter user 'ssluser'@'%' require ssl;
Query OK, 0 rows affected (0.00 sec)
...
[engineer@percona]# mysql -u ssluser -psekret
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'ssluser'@'localhost' (using password: YES)</pre><p><span style="font-weight: 400;"><strong>Fix:</strong> Adding &#8211;ssl-mode flag (<a target="_blank" href="https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-11.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">&#8211;ssl flag is deprecated</a> but can be used too)</span></p><pre class="crayon-plain-tag">[engineer@percona]# mysql -u ssluser -psekret --ssl-mode=REQUIRED
...
mysql&gt;</pre><p>You can read more in-depth on how to configure SSL in MySQL in the blog post about &#8220;<a target="_blank" href="https://www.percona.com/blog/2013/06/22/setting-up-mysql-ssl-and-secure-connections/">Setting up MySQL SSL and Secure Connections</a>&#8221; and &#8220;<a target="_blank" href="https://www.percona.com/blog/2017/06/27/ssl-connections-in-mysql-5-7/">SSL in 5.6 and 5.7</a>&#8220;.</p>
<h4>7) PAM backend not working:</h4>
<p></p><pre class="crayon-plain-tag">mysql&gt; CREATE USER 'ap_user'@'%' IDENTIFIED WITH auth_pam;
Query OK, 0 rows affected (0.00 sec)
...
[engineer@percona]# mysql -u ap_user -pap_user_pass
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'ap_user'@'localhost' (using password: YES)</pre><p><span style="font-weight: 400;"><strong>Fix:</strong> Double check user/password is correct for the user to authenticate with the PAM currently being used. </span></p>
<p>In my example, I am using Linux shadow files for authentication. In order to check if the user exists:</p><pre class="crayon-plain-tag">[engineer@percona]# cat /etc/passwd | grep ap_user
ap_user:x:1000:1000::/home/ap_user:/bin/bash</pre><p>To reset password:</p><pre class="crayon-plain-tag">[engineer@percona]# sudo passwd ap_user
Changing password for user ap_user.
New password:</pre><p>Finally, if you are genuinely locked out and need to circumvent the authentication mechanisms in order to regain access to the database, here are a few simple steps to do so:</p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">Stop the instance</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Edit my.cnf and add skip-grant-tables under [mysqld] (this will allow access to MySQL without prompting for a password). On MySQL 8.0, skip-networking is automatically enabled (only allows access to MySQL from localhost), but for previous MySQL versions it&#8217;s suggested to also add &#8211;skip-networking under [mysqld]</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Start the instance</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Access with root user (mysql -uroot -hlocalhost); </span></li>
<li style="font-weight: 400;">
<div>Issue the necessary GRANT/CREATE USER/SET PASSWORD to correct the issue (likely setting a known root password will be the right thing: SET PASSWORD FOR ‘root’@’localhost’ = ‘S0vrySekr3t’). Using grant-skip-tables won&#8217;t read grants into memory and GRANT/CREATE/SET PASSWORD statements won&#8217;t work straight away. First, you need to execute &#8220;FLUSH PRIVILEGES;&#8221; before executing any GRANT/CREATE/SET PASSWORD statement, or you can modify mysql.users table with a query which modifies the password for User and Host like &#8220;UPDATE mysql.user SET authentication_string=<wbr />PASSWORD(&#8216;newpwd&#8217;) WHERE User=&#8217;root&#8217; and Host=&#8217;localhost&#8217;;&#8221;</div>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Stop the instance</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Edit my.cnf and remove skip-grant-tables and skip-networking</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Start MySQL again</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">You should be able to login with root from the localhost and do any other necessary corrective operations with root user.
<p></span></li>
</ol>
<p><a target="_blank" href="https://www.percona.com/software/mysql-database/percona-server">Learn more about Percona Server for MySQL</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/05/fixing-a-mysql-1045-error/#comments" thr:count="10"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/05/fixing-a-mysql-1045-error/feed/atom/" thr:count="10"/>
		<thr:total>10</thr:total>
			</entry>
		<entry>
		<author>
			<name>Adamo Tonete</name>
					</author>
		<title type="html"><![CDATA[Hiding Fields in MongoDB: Views + Custom Roles]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/05/hiding-fields-in-mongodb-views-custom-roles/" />
		<id>https://www.percona.com/blog/?p=57611</id>
		<updated>2019-07-19T20:10:29Z</updated>
		<published>2019-07-05T15:11:02Z</published>
		<category scheme="https://www.percona.com/blog" term="MongoDB" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="hiding fields in MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />A time ago we wrote about how personalized roles may help you to give specific permissions when it is needed. This time we want to discuss how a custom role, combined with a MongoDB View, can hide sensitive information from the client. Hiding Fields in MongoDB Suppose you have a collection that needs to be [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/05/hiding-fields-in-mongodb-views-custom-roles/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="hiding fields in MongoDB" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-thumbnail wp-image-58605" src="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg" alt="hiding fields in MongoDB" width="200" height="133" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/hiding-fields-in-MongoDB.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />A time ago we wrote about how <a target="_blank" href="https://www.percona.com/blog/2017/05/17/mongodb-authentication-and-roles-creating-your-first-personalized-role/">personalized roles may help you to give specific permissions</a> when it is needed. This time we want to discuss how a custom role, combined with a MongoDB View, can hide sensitive information from the client.</p>
<h2>Hiding Fields in MongoDB</h2>
<p>Suppose you have a collection that needs to be shared with a different team, but this team should not be able to see some fields &#8211; in our case, to make it easy: the salary field.</p>
<p>Views in MongoDB can hide sensitive information and change the data visualization as needed &#8211; It was discussed <a target="_blank" href="https://www.percona.com/blog/2017/01/13/mongodb-3-4-views/">here</a>. For this example, we will use the collection employee with some data, with a user that has permission. Let&#8217;s insert some objects in the percona database</p><pre class="crayon-plain-tag">use percona

db.employees.insert({ "_id" : ObjectId("5ce5e609444cde8078f337f2"), "name" : "Adamo Tonete", 
    "salary" : { "year" : 1, "bonus" : 1 } })
db.employees.insert({ "_id" : ObjectId("5ce5e616444cde8078f337f3"), "name" : "Vinicius Grippa", 
    "salary" : { "year" : 1, "bonus" : 1 } })
db.employees.insert({ "_id" : ObjectId("5ce5e627444cde8078f337f4"), "name" : "Marcos Albe", 
    "salary" : { "year" : 1, "bonus" : 1 } })
db.employees.insert({ "_id" : ObjectId("5ce5e63f444cde8078f337f5"), "name" : "Vinodh Krishnaswamy", 
    "salary" : { "year" : 1, "bonus" : 1 } })
db.employees.insert({ "_id" : ObjectId("5ce5e655444cde8078f337f6"), "name" : "Aayushi Mangal", 
    "salary" : { "year" : 1, "bonus" : 1 } })</pre><p>Then let&#8217;s create a view for this collection:</p><pre class="crayon-plain-tag">db.createView('employees_name', 'employees',
   [{ $project: { _id: 1, name : 1 } } ]
)</pre><p>If we type <code>show dbs</code>; we will be able to see both collections, so, a read-only user still able to read the employees collection.</p>
<p>In order to secure the employees&#8217; collection, we are creating a custom role that one has permission to see the employees_names collection and nothing else. In that way the fields salary will never exist to the user:</p><pre class="crayon-plain-tag">use admin
db.createRole(
   {
     role: "view_views",
     privileges: [
       { resource: { db: "percona", collection: "system.views" }, actions: [ "find" ] },
       { resource: { db: "percona", collection: "employees_name" }, actions: [ "find","collStats"]}
     ],
     roles: [
       { role: "read", db: "admin" }
     ]
   }
)</pre><p>Then we will create a user that only has permission to read data from the view (belongs to the role &#8220;view_views&#8221;);</p><pre class="crayon-plain-tag">db.createUser({user : 'intern', pwd : '123', roles : ["view_views"]})</pre><p>Now the user can only see the collection <code>employees_name</code> in the percona database and nothing else.</p>
<p>Running the query as the user intern:</p><pre class="crayon-plain-tag">&gt; show dbs
admin    0.000GB
percona  0.000GB
&gt; use percona
switched to db percona
&gt; db.employees_name.find()
{ "_id" : ObjectId("5ce5e609444cde8078f337f2"), "name" : "Adamo Tonete" }
{ "_id" : ObjectId("5ce5e616444cde8078f337f3"), "name" : "Vinicius Grippa" }
{ "_id" : ObjectId("5ce5e627444cde8078f337f4"), "name" : "Marcos Albe" }
{ "_id" : ObjectId("5ce5e63f444cde8078f337f5"), "name" : "Vinodh Krishnaswamy" }
{ "_id" : ObjectId("5ce5e655444cde8078f337f6"), "name" : "Aayushi Mangal" }</pre><p>There are several ways to do that. For instance, if you were using an application it would do the same thing, but the purpose of this blog is to demonstrate how a combination of two technologies can help in hiding fields in MongoDB</p>
<p>I hope you liked the blog, feel free to reach out me on @<a target="_blank" href="http://twitter.com/adamotonete" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">AdamoTonete</a> or @<a target="_blank" href="http://twitter.com/percona" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">percona</a> for questions.</p>
<p><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Learn more about Percona Server for MongoDB</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/05/hiding-fields-in-mongodb-views-custom-roles/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/05/hiding-fields-in-mongodb-views-custom-roles/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Akira Kurogane</name>
					</author>
		<title type="html"><![CDATA[MongoDB Disaster, Snapshot Restore and Point-in-time Replay]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/05/mongodb-disaster-snapshot-restore-and-point-in-time-replay/" />
		<id>https://www.percona.com/blog/?p=58392</id>
		<updated>2019-07-19T20:11:05Z</updated>
		<published>2019-07-05T15:02:35Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB update schema disaster" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-1024x681.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-367x244.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />Mistakes can happen. If only we could go back in time to the very second before that mistake was made. Act 1: The Disaster Plain text version for those who cannot run the asciicast above: [crayon-5d433ac37c295141930148/] Act 2: Time travel with a Snapshot restore + Oplog replay Plain text version for those who cannot run [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/05/mongodb-disaster-snapshot-restore-and-point-in-time-replay/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="MongoDB update schema disaster" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-1024x681.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster-367x244.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/MongoDB-Disaster.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-weight: 400;">Mistakes can happen. If only we could go back in time to the very second before that mistake was made.</span></p>
<h2>Act 1: The Disaster</h2>
<p><script src="https://asciinema.org/a/254684.js" id="asciicast-254684" async speed="1.5"></script></p>
<p>Plain text version for those who cannot run the asciicast above:</p><pre class="crayon-plain-tag">akira@perc01:/data$ #OK, let's get this party started!
akira@perc01:/data$ # The frontend has been shut down for 20 mins so they can
akira@perc01:/data$ # update that part, and I can update the schema in he 
akira@perc01:/data$ # backend simultaneously.
akira@perc01:/data$ #Easy-peasy ...
akira@perc01:/data$ date
Tue Jul  2 13:34:09 JST 2019
akira@perc01:/data$ #Just set my auth details.(NO PEEKING!)
akira@perc01:/data$ conn_args="--host localhost:27017 --username akira --password secret --authenticationDatabase admin"
akira@perc01:/data$ mongo ${conn_args} --quiet
testrs:PRIMARY&gt; use payments
switched to db payments
testrs:PRIMARY&gt; show collections
TheImportantCollection
testrs:PRIMARY&gt; //Ah, there it is. Time to work!
testrs:PRIMARY&gt; db.TheImportantCollection.count()
174662
testrs:PRIMARY&gt; db.TheImportantCollection.findOne()
{
    "_id" : 0,
    "customer" : {
        "fn" : "Smith",
        "gn" : "Ken",
        "city" : "Georgevill",
        "street1" : "1 Wishful St.",
        "postcode" : "45031"
    },
    "order_ids" : [ ]
}
testrs:PRIMARY&gt; //Ah, there it is. The "customer" object that has the 
testrs:PRIMARY&gt; //address fields in it. We're going to move those out.
testrs:PRIMARY&gt; //Copy the whole collection, adding the new "addresses" array
testrs:PRIMARY&gt; var counter = 0;
testrs:PRIMARY&gt; db.TheImportantCollection.find().forEach(function(d) {
...   d["adresses"] = [ ];
...   db.TheImportantCollectionV2.insert(d);
...   counter += 1;
...   if (counter % 25000 == 0) { print(counter + " updates done"); }
... });
25000 updates done
50000 updates done
75000 updates done
100000 updates done
125000 updates done
150000 updates done
testrs:PRIMARY&gt; //Cool. Let's look at the temp table
testrs:PRIMARY&gt; db.TheImportantCollectionV2.findOne()
{
    "_id" : 0,
    "customer" : {
        "fn" : "Smith",
        "gn" : "Ken",
        "city" : "Georgevill",
        "street1" : "1 Wishful St.",
        "postcode" : "45031"
    },
    "order_ids" : [ ],
    "adresses" : [ ]
}
testrs:PRIMARY&gt; //?AH!!
testrs:PRIMARY&gt; //typo. I misspelled "addresses".
testrs:PRIMARY&gt; //I'll just drop this and go again
testrs:PRIMARY&gt; db.TheImportantCollectionV2.remove({})
WriteResult({ "nRemoved" : 174662 })
testrs:PRIMARY&gt; //ooops. Why did I bother deleting the docs?
testrs:PRIMARY&gt; //I need to *drop* the collection
testrs:PRIMARY&gt; db.TheImportantCollection.drop()
true
testrs:PRIMARY&gt; //!!!!
testrs:PRIMARY&gt; //Wait!
testrs:PRIMARY&gt; show collections
TheImportantCollectionV2
testrs:PRIMARY&gt; //...
testrs:PRIMARY&gt; //I've done a bad    thing ....
testrs:PRIMARY&gt; //Let me see
testrs:PRIMARY&gt; //in the oplog
testrs:PRIMARY&gt; use local
switched to db local
testrs:PRIMARY&gt; db.oplog.rs.findOne({"o.drop": "TheImportantCollection"})
{
    "ts" : Timestamp(1562042272, 1),
    "t" : NumberLong(6),
    "h" : NumberLong("6726633412398410781"),
    "v" : 2,
    "op" : "c",
    "ns" : "payments.$cmd",
    "ui" : UUID("abc9c1f9-71c0-45ea-aeba-ea239b975a95"),
    "wall" : ISODate("2019-07-02T04:37:52.171Z"),
    "o" : {
        "drop" : "TheImportantCollection"
    }
}
testrs:PRIMARY&gt; //AH. 1562042272, you are the worst unix epoch second of my
testrs:PRIMARY&gt; // life.
testrs:PRIMARY&gt;</pre><p></p>
<h2>Act 2: Time travel with a Snapshot restore + Oplog replay</h2>
<p><script src="https://asciinema.org/a/254687.js" id="asciicast-254687" async speed="1.5"></script></p>
<p>Plain text version for those who cannot run the asciicast above:</p><pre class="crayon-plain-tag">akira@perc01:/data$ #OK, OK, this is bad. I dropped TheImportantCollection
akira@perc01:/data$ #Breathe. Breathe Akira.
akira@perc01:/data$ #Right! Backups!
akira@perc01:/data$ #I have backups!
akira@perc01:/data$ ls /backups/
20190624_2300  20190626_2300  20190628_2300
20190625_2300  20190627_2300  20190629_2300
akira@perc01:/data$ #OK, I have one from 23:00 JST ... which is a while ago.
akira@perc01:/data$ #I can use the latest backup, then roll forward from
akira@perc01:/data$ # there using this neat thing you can do with
akira@perc01:/data$ #  mongorestore (the standard mongo utils command)
akira@perc01:/data$ #You can replay a dumped oplog bson file 
akira@perc01:/data$ # on a primary like it was receiving as a secondary
akira@perc01:/data$ #Just as a secondary can catch up from a primary so
akira@perc01:/data$ # far the oplog window of time goes, a primary can
akira@perc01:/data$ # be given an oplog history to replay, using this 'trick'
akira@perc01:/data$ #(Not really a trick, but let's call it that)
akira@perc01:/data$ 
akira@perc01:/data$ #
akira@perc01:/data$ #But, before doing ANYTHING with the backups,
akira@perc01:/data$ # get a full dump of the oplog of the *live* replicaset
akira@perc01:/data$ # first
akira@perc01:/data$ conn_args="--host localhost:27017 --username akira --password secret --authenticationDatabase admin"
akira@perc01:/data$ mongodump ${conn_args} -d local -c oplog.rs --out /data/oplog_dump_full
2019-07-02T13:50:02.713+0900	writing local.oplog.rs to 
2019-07-02T13:50:03.635+0900	done dumping local.oplog.rs (825815 documents)
akira@perc01:/data$ #Oh wait.
akira@perc01:/data$ #We *do* need a trick
akira@perc01:/data$ #v3.6 and v4.0 added some system collections that cause
akira@perc01:/data$ # mongorestore to fail, no matter what we do.
akira@perc01:/data$ # This is just a 3.6 and 4.0 issue hopefully, but 4.2's 
akira@perc01:/data$ #  behaviour is not known at this date.
akira@perc01:/data$ #I'll do the dump again, removing these two collections
akira@perc01:/data$ mongodump ${conn_args} -d local -c oplog.rs \
&gt; --query '{"ns": {"$nin": ["config.system.sessions", "config.cache.collections"]}}' --out /data/oplog_dump_full
2019-07-02T13:52:08.841+0900	writing local.oplog.rs to 
2019-07-02T13:52:10.010+0900	done dumping local.oplog.rs (825781 documents)
akira@perc01:/data$ #So that was Trick #1. Removing those 2 specific 
akira@perc01:/data$ # config.* collections.
akira@perc01:/data$ #Now for #Trick 2
akira@perc01:/data$ #mongodump puts the dumped oplog.rs.bson file in subdirectory "local" like that is a whole DB to restore. But you don't do a restore of local like any other DB, it doesn't work like that.
akira@perc01:/data$ #So we MUST get  rid of subdirectory structure and just keep the single *.bson file
akira@perc01:/data$ ls -lR /data/oplog_dump_full/
/data/oplog_dump_full/:
total 146032
drwxr-xr-x 2 akira akira        57 Jul  2 13:50 local
-rw-r--r-- 1 akira akira 149534510 Jul  2 10:26 oplog.rs.bson

/data/oplog_dump_full/local:
total 233008
-rw-r--r-- 1 akira akira 238596091 Jul  2 13:52 oplog.rs.bson
-rw-r--r-- 1 akira akira       120 Jul  2 13:52 oplog.rs.metadata.json
akira@perc01:/data$ mv /data/oplog_dump_full/local/oplog.rs.bson /data/oplog_dump_full/
akira@perc01:/data$ rm -rf /data/oplog_dump_full/local
akira@perc01:/data$ ls -lR /data/oplog_dump_full/
/data/oplog_dump_full/:
total 233004
-rw-r--r-- 1 akira akira 238596091 Jul  2 13:52 oplog.rs.bson
akira@perc01:/data$ #OK.
akira@perc01:/data$ #Now let's look at this oplog. Does it go back as far as
akira@perc01:/data$ # the latest backup snapshot or more?
akira@perc01:/data$ ls /backups/ | tail -n 1
20190629_2300
akira@perc01:/data$ #By the way that is my JST timezone, not UTC
akira@perc01:/data$ #let's see ... check the bson file's first timestamp
akira@perc01:/data$ bsondump /data/oplog_dump_full/oplog.rs.bson 2&gt;/dev/null | head -n 1
{"ts":{"$timestamp":{"t":1561727517,"i":1}},"h":{"$numberLong":"212971303912007811"},"v":2,"op":"n","ns":"","wall":{"$date":"2019-06-28T13:11:57.633Z"},"o":{"msg":"initiating set"}}
akira@perc01:/data$ #I see the epoch timestamp there: 1561727517
akira@perc01:/data$ date -d @1561727517
Fri Jun 28 22:11:57 JST 2019
akira@perc01:/data$ #Ah, good, that's before 20190629_2300
akira@perc01:/data$ #We can do a oplog replay
akira@perc01:/data$ #Just for sanity's sake let's look for that "drop"
akira@perc01:/data$ #  command that is the disaster we want to avoid replaying
akira@perc01:/data$ bsondump /data/oplog_dump_full/oplog.rs.bson 2&gt;/dev/null | grep drop | grep '\bTheImportantCollection\b' | tail -n 1
{"ts":{"$timestamp":{"t":1562042272,"i":1}},"t":{"$numberLong":"6"},"h":{"$numberLong":"6726633412398410781"},"v":2,"op":"c","ns":"payments.$cmd","ui":{"$binary":"q8nB+XHARequuuojm5dalQ==","$type":"04"},"wall":{"$date":"2019-07-02T04:37:52.171Z"},"o":{"drop":"TheImportantCollection"}}
akira@perc01:/data$ #Let's see it was 1562042272, the worst epoch second of my
akira@perc01:/data$ # my life. Let's not go there again!
akira@perc01:/data$ #Time to shut the live replicaset down, restore a snapshot
akira@perc01:/data$ # backup from 20190629_2300
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
18119 mongod -f /data/n1/mongod.conf
18195 mongod -f /data/n2/mongod.conf
18225 mongod -f /data/n3/mongod.conf
akira@perc01:/data$ kill 18119 18195 18225
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
18119 mongod -f /data/n1/mongod.conf
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
18119 mongod -f /data/n1/mongod.conf
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
18119 mongod -f /data/n1/mongod.conf
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
akira@perc01:/data$ #OK, shutdown
akira@perc01:/data$ /data/dba_scripts/our_restore_script.sh 
usage: /data/dba_scripts/our_restore_script.sh XXXXXX
Choose one of these subdirectory names from /backups/:
  20190624_2300
  20190625_2300
  20190626_2300
  20190627_2300
  20190628_2300
  20190629_2300
akira@perc01:/data$ /data/dba_scripts/our_restore_script.sh 20190629_2300
Stopping mongod nodes
Restoring backup 20190629_2300 to one node dbpath
Restarting
about to fork child process, waiting until server is ready for connections.
forked process: 21776
child process started successfully, parent exiting
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
21776 mongod -f /data/n1/mongod.conf
akira@perc01:/data$ #I'll start the secondaries too
akira@perc01:/data$ rm -rf /data/n2/data/*
akira@perc01:/data$ mongod -f /data/n2/mongod.conf 
about to fork child process, waiting until server is ready for connections.
forked process: 21859
child process started successfully, parent exiting
akira@perc01:/data$ rm -rf /data/n3/data/*
akira@perc01:/data$ mongod -f /data/n3/mongod.conf 
about to fork child process, waiting until server is ready for connections.
forked process: 21896
child process started successfully, parent exiting
akira@perc01:/data$ ps -C mongod -o pid,args
  PID COMMAND
21776 mongod -f /data/n1/mongod.conf
21859 mongod -f /data/n2/mongod.conf
21896 mongod -f /data/n3/mongod.conf
akira@perc01:/data$ #I'm going to check my important collection is there again
akira@perc01:/data$ mongo ${conn_args} 
MongoDB shell version v4.0.10
connecting to: mongodb://localhost:27017/?authSource=admin&amp;gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("e5aa9b27-f26b-4c73-bdc1-bdaf494cf7ab") }
MongoDB server version: 4.0.10
testrs:PRIMARY&gt; use payments
switched to db payments
testrs:PRIMARY&gt; show collections
TheImportantCollection
testrs:PRIMARY&gt; //YES
testrs:PRIMARY&gt; db.TheImportantCollection.count()
174662
testrs:PRIMARY&gt; db.TheImportantCollection.findOne()
{
	"_id" : 0,
	"customer" : {
		"fn" : "Smith",
		"gn" : "Ken",
		"city" : "Georgevill",
		"street1" : "1 Wishful St.",
		"postcode" : "45031"
	},
	"order_ids" : [ ]
}
testrs:PRIMARY&gt; //Yes yes yes ... I live
testrs:PRIMARY&gt; 
bye
akira@perc01:/data$ #So the data is back ... but only some time way in the
akira@perc01:/data$ # past. I want to replay up until ...
akira@perc01:/data$ bad_drop_epoch_sec=1562042272
akira@perc01:/data$ #Trick 3: mongorestore always expects a directory name
akira@perc01:/data$ #We don't need any directories, but it's just hard-coded
akira@perc01:/data$ # to expect one. So let's make one. Can be anywhere
akira@perc01:/data$ # Just not a subdirectory under the oplog dump location please, that will confuse it maybe
akira@perc01:/data$ mkdir /tmp/fake_empty_dir
mkdir: cannot create directory ‘/tmp/fake_empty_dir’: File exists
akira@perc01:/data$ #Ah, I got it already.
akira@perc01:/data$ ls /tmp/fake_empty_dir
akira@perc01:/data$ mongorestore ${conn_args} \
&gt;   --oplogReplay \
&gt;    --oplogFile /data/oplog_dump_full/oplog.rs.bson \
&gt;   --oplogLimit ${bad_drop_epoch_sec}:0  \
&gt;   --stopOnError /tmp/fake_empty_dir
2019-07-02T14:04:35.742+0900	preparing collections to restore from
2019-07-02T14:04:35.742+0900	replaying oplog
2019-07-02T14:04:38.715+0900	oplog  5.47MB
2019-07-02T14:04:41.715+0900	oplog  11.0MB
2019-07-02T14:04:44.715+0900	oplog  16.6MB
2019-07-02T14:04:47.715+0900	oplog  22.2MB
2019-07-02T14:04:50.715+0900	oplog  27.6MB
2019-07-02T14:04:53.715+0900	oplog  32.8MB
2019-07-02T14:04:56.715+0900	oplog  37.9MB
2019-07-02T14:04:59.715+0900	oplog  43.0MB
2019-07-02T14:05:02.715+0900	oplog  48.3MB
2019-07-02T14:05:05.715+0900	oplog  53.9MB
2019-07-02T14:05:08.715+0900	oplog  59.5MB
2019-07-02T14:05:11.715+0900	oplog  65.1MB
2019-07-02T14:05:14.715+0900	oplog  70.2MB
2019-07-02T14:05:17.715+0900	oplog  75.0MB
2019-07-02T14:05:20.715+0900	oplog  79.6MB
2019-07-02T14:05:23.715+0900	oplog  84.1MB
2019-07-02T14:05:26.715+0900	oplog  88.5MB
2019-07-02T14:05:29.715+0900	oplog  93.0MB
2019-07-02T14:05:32.715+0900	oplog  97.6MB
2019-07-02T14:05:35.715+0900	oplog  101MB
2019-07-02T14:05:38.715+0900	oplog  104MB
2019-07-02T14:05:41.715+0900	oplog  107MB
2019-07-02T14:05:44.715+0900	oplog  110MB
2019-07-02T14:05:47.715+0900	oplog  113MB
2019-07-02T14:05:50.715+0900	oplog  115MB
2019-07-02T14:05:53.715+0900	oplog  118MB
2019-07-02T14:05:56.715+0900	oplog  123MB
2019-07-02T14:05:59.715+0900	oplog  128MB
2019-07-02T14:06:02.715+0900	oplog  133MB
2019-07-02T14:06:05.715+0900	oplog  138MB
2019-07-02T14:06:08.715+0900	oplog  142MB
2019-07-02T14:06:11.715+0900	oplog  146MB
2019-07-02T14:06:14.715+0900	oplog  151MB
2019-07-02T14:06:17.715+0900	oplog  156MB
2019-07-02T14:06:20.715+0900	oplog  161MB
2019-07-02T14:06:23.715+0900	oplog  166MB
2019-07-02T14:06:26.715+0900	oplog  171MB
2019-07-02T14:06:29.715+0900	oplog  176MB
2019-07-02T14:06:32.715+0900	oplog  181MB
2019-07-02T14:06:35.715+0900	oplog  186MB
2019-07-02T14:06:38.715+0900	oplog  192MB
2019-07-02T14:06:41.715+0900	oplog  197MB
2019-07-02T14:06:44.715+0900	oplog  201MB
2019-07-02T14:06:47.715+0900	oplog  204MB
2019-07-02T14:06:50.715+0900	oplog  206MB
2019-07-02T14:06:53.715+0900	oplog  209MB
2019-07-02T14:06:56.715+0900	oplog  211MB
2019-07-02T14:06:59.715+0900	oplog  213MB
2019-07-02T14:07:02.715+0900	oplog  216MB
2019-07-02T14:07:05.715+0900	oplog  218MB
2019-07-02T14:07:08.715+0900	oplog  220MB
2019-07-02T14:07:11.715+0900	oplog  223MB
2019-07-02T14:07:14.715+0900	oplog  225MB
2019-07-02T14:07:17.715+0900	oplog  227MB
2019-07-02T14:07:17.753+0900	oplog  227MB
2019-07-02T14:07:17.753+0900	done
akira@perc01:/data$ #Yay! I hope! Let's check
akira@perc01:/data$ mongo ${conn_args} 
MongoDB shell version v4.0.10
connecting to: mongodb://localhost:27017/?authSource=admin&amp;gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("302f2c26-7416-4e18-bd02-1bd67626d062") }
MongoDB server version: 4.0.10
testrs:PRIMARY&gt; use payments
switched to db payments
testrs:PRIMARY&gt; show collections
TheImportantCollection
TheImportantCollectionV2
testrs:PRIMARY&gt; //Yes! both there!
testrs:PRIMARY&gt; db.TheImportantCollection.count()
174662
testrs:PRIMARY&gt; //plus the 'V2' table I was working on when I made my 
testrs:PRIMARY&gt; // 'fat thumb' mistake
testrs:PRIMARY&gt; //There we go, a point-in-time restore from a snapshot
testrs:PRIMARY&gt; // backup + a mongorestore --oplogReplay --oplogFile
testrs:PRIMARY&gt; // operation.
testrs:PRIMARY&gt; //Hold on for one last trick (which I didn't have to use today)
testrs:PRIMARY&gt; // Trick #4: ultimate permissions are sometimes needed.
testrs:PRIMARY&gt; // The config.system.sessions and config.transactions(?) 
testrs:PRIMARY&gt; //  system collections are currently unreplayable (3.6, 4.0,
testrs:PRIMARY&gt; //  4.2. TBD).
testrs:PRIMARY&gt; // They are not the only system collections that you can stuck on, because systems collections are mostly not covered by the "backup" and "restore" built-in roles.
testrs:PRIMARY&gt; // E.g. if you are replaying updates to the admin.system.users
testrs:PRIMARY&gt; //  collection that will fail.
testrs:PRIMARY&gt; // But you can allow if you make a *custom* role that grants
testrs:PRIMARY&gt; //  "anyAction" on "anyResource" (see the docs), and grant that
testrs:PRIMARY&gt; // to your backup and restore user, that will make it possible for those to succeed too.
testrs:PRIMARY&gt; //good night
testrs:PRIMARY&gt;</pre><p>&nbsp;</p>
<h3>The &#8216;TLDR&#8217;</h3>
<p>The oplog of the damaged replicaset is your valuable, idempotent history <em>if</em> you have a backup from a recent enough time to apply it on.</p>
<ul>
<li>Identify your disaster operation&#8217;s timestamp value in the oplog.</li>
<li><em>Before</em> shutting the damaged replicaset down: <code>mongodump connection-args --db local --collection oplog.rs</code>
<ul>
<li>(Necessary workaround #1) use a <code>--query '{"ns": {"$nin": ["config.system.sessions", "config.transactions", "config.transaction_coordinators"]}}'</code> argument to avoid transaction-related system collections from v3.6 and v4.0 (and maybe 4.2+ too) that can&#8217;t be restored.</li>
</ul>
</li>
<li>(Necessary workaround #2) Get rid of the subdirectory structure mongodump makes and keep just the oplog.rs.bson file.</li>
<li>(Necessary workaround #3) Make a fake, empty directory somewhere else too, to trick mongorestore later.</li>
<li>Use <code>bsondump oplog.rs.bson | head -n 1</code> to check that this oplog starts before the time of your last backup</li>
<li>Shut the damaged DB down.</li>
<li>Restore to the latest backup before the disaster.</li>
<li>(Possibly-required workaround #4) If the oplog updates other system collections, create a user-defined role that grants anyAction on anyResource and grants it to your user as well. (See special section on system collections below.)</li>
<li>Replay up to but not including the disaster second: mongorestore <em>connection-args </em>&#8211;oplog<strong>Replay</strong> &#8211;oplog<strong>File</strong> oplog.rs.bson &#8211;oplog<strong>Limit</strong> <em>disaster_epoch_sec</em>:0 /tmp/fake_empty_directory</li>
</ul>
<p>See the &#8216;Act 2&#8217; video for the details.</p>
<h2>So how did that work?</h2>
<p>If you&#8217;re having the kind of disaster presented in this article I assume you are already familiar with the <a target="_blank" href="https://docs.mongodb.com/manual/reference/program/mongodump/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">mongodump</a> and <a target="_blank" href="https://docs.mongodb.com/manual/reference/program/mongorestore/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">mongorestore</a> tools and MongoDB Oplog <a target="_blank" href="https://en.wikipedia.org/wiki/Idempotence#Computer_science_meaning" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">idempotency</a>. Taking that for granted let&#8217;s go to the next level of detail.</p>
<h3>The <code>applyOps</code> command &#8211; Kinda secret; Actually public</h3>
<p>In theory you could iterate oplog documents and write an application that runs an insert command for an &#8220;i&#8221; op, an update for the &#8220;u&#8221; ops, various different commands for the &#8220;c&#8221; op, etc, but the simpler way is to submit them as they are (well almost exactly as they are) using the <a target="_blank" href="https://docs.mongodb.com/manual/reference/command/applyOps/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">applyOps</a> command, and this is what the mongorestore tool <a target="_blank" href="https://github.com/mongodb/mongo-tools/blob/c9ccb31c7c0b5f805d3f2f39c4ce4f78a4c37999/mongorestore/oplog.go#L194-L207" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">does</a>.</p>
<p>The permission to run applyOps is granted to the <a target="_blank" href="https://docs.mongodb.com/manual/reference/built-in-roles/index.html#restore" target="_blank" rel="&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">&#8220;restore&#8221;</a> role for all non-system collections, and there is no &#8216;reject if a primary&#8217; rule. So you can make a primary apply oplog docs like a secondary does.</p>
<p>N.b. for some system collections, the &#8220;restore&#8221; role is not enough. See the bottom section for more details.</p>
<p>It might seem a bit strange users can have this privilege but without it, there would be no convenient way for dump-restore tools to guarantee consistency. The &#8220;consistency&#8221; here means all that the restored data will be exactly as it was at <em>some</em> point in time &#8211; the end of the dump &#8211; and not contain earlier versions of documents from some midpoint time during the dumping process.</p>
<p>Achieving that data consistency is why the <code>--oplog</code> option for mongodump was created, and why mongorestore has the matching <code>--oplogReplay</code> option. (Those two options should be on by default i.m.o. but they are not). The short oplog span made during a normal dump will be at  &lt;dump_directory&gt;/oplog.rs.bson, but the <code>--oplogFile</code> argument lets you choose any arbitrary path.</p>
<h2><code>--oplogLimit</code></h2>
<p>We could have limited the oplog docs during mongodump to only include those before the disaster time with &#8211;query parameter such as the following:</p>
<p><code>mongodump ... --query '{"ts": {"$lt": new Timestamp(1560915610, 0)}}' ...</code></p>
<p>But <code>--oplogLimit</code> makes it easier. You can dump everything, but then use <code>--oplogLimit &lt;epoch_sec_value&gt;[:&lt;counter&gt;]</code> when you run mongorestore with the &#8211;oplogReplay argument.</p>
<p><span style="font-weight: 400;">If you&#8217;re getting confused about whether it&#8217;s UTC or your server timezone &#8211; it&#8217;s UTC. All timestamps inside MongoDB are UTC if they represent &#8216;wall clock&#8217; times, and for &#8216;logical clocks&#8217; timezone is a non-applicable concept.</span></p>
<h2>When the oplog includes system collection updates</h2>
<p>In the <a target="_blank" href="https://docs.mongodb.com/v4.0/reference/built-in-roles/" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">built-in roles</a> documentation, inserted after the usual and mostly fair warnings on why you should not grant users the most powerful internal role, comes this extra note that tells you what you actually need to do to allow oplog-replay updates on all system collections too:</p>
<blockquote><p>If you need access to all actions on all resources, for example to run applyOps commands &#8230; create a user-defined role that grants anyAction on anyResource and ensure that only the users who need access to these operations have this access.</p></blockquote>
<p>Translation: if your oplog replay fails because it hit a system collection update the &#8220;restore&#8221; role doesn&#8217;t cover, upgrade your user to be able to run with all the privileges that a secondary runs oplog replication with.</p><pre class="crayon-plain-tag">use admin
db.createRole({
  "role": "CustomAllPowersRole", 
  "privileges": [ 
    { "resource": { "anyResource": true }, "actions": [ "anyAction" ] }, 
  ],
  "roles": [ ] });
db.grantRolesToUser("&lt;bk_and_restore_username&gt;", [ "CustomAllPowersRole" ])

//For afterwards:
//use admin
//db.revokeRolesFromUser("&lt;bk_and_restore_username&gt;", [ "CustomAllPowersRole" ])
//db.dropRole("CustomAllPowersRole")</pre><p>Alternatively, to granting the role shown above, you could restart the mongod with security disabled; in this mode, all operations work without access control restrictions.</p>
<p>It&#8217;s not quite as simple as that though because transaction stuff is currently (v3.6, v4.0) throwing a spanner in the works. So I&#8217;ve found explicitly excluding <em>config.system.sessions</em> and <em>config.transactions</em> during mongodump is the best way to avoid those updates. They are logically unnecessary in a restore because the sessions/transactions finished when the replica set was completely shut down.</p>
<p><a target="_blank" href="https://www.percona.com/software/mongo-database/percona-server-for-mongodb">Learn more about Percona Server for MongoDB</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/05/mongodb-disaster-snapshot-restore-and-point-in-time-replay/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/05/mongodb-disaster-snapshot-restore-and-point-in-time-replay/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>David Quilty</name>
					</author>
		<title type="html"><![CDATA[Blog Poll: What Keeps You Up At Night?]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/03/blog-poll-what-keeps-you-up-at-night/" />
		<id>https://www.percona.com/blog/?p=58435</id>
		<updated>2019-07-03T17:17:16Z</updated>
		<published>2019-07-03T16:56:00Z</published>
		<category scheme="https://www.percona.com/blog" term="Database Poll" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="database poll" />		<summary type="html"><![CDATA[<img width="200" height="100" src="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-200x100.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Blog Poll: What Keeps You Up At Night" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-367x184.jpg 367w" sizes="(max-width: 200px) 100vw, 200px" />Last year, we asked you a few questions in a blog poll and we received a great amount of feedback. This year, we wanted to follow up on those same survey questions to see what may have changed over the last 12 months. So with that in mind, we&#8217;re hoping you can take a minute [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/03/blog-poll-what-keeps-you-up-at-night/"><![CDATA[<img width="200" height="100" src="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-200x100.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Blog Poll: What Keeps You Up At Night" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-367x184.jpg 367w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright wp-image-58438 size-medium" src="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-300x150.jpg" alt="Blog Poll: What Keeps You Up At Night" width="300" height="150" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-300x150.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-200x100.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-1024x512.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/What-Keeps-You-Up-At-Night-367x184.jpg 367w" sizes="(max-width: 300px) 100vw, 300px" />Last year, we asked you a few questions in a blog poll and we received a great amount of feedback. This year, we wanted to follow up on those same survey questions to see what may have changed over the last 12 months. So with that in mind, we&#8217;re hoping you can take a minute or so to answer the first survey question in this series: <strong>What Keeps You Up At Night?</strong> Is it fixing emergencies? Bad queries? Cost concerns? Inquiring minds want to know!</p>
Note: There is a poll embedded within this post, please visit the site to participate in this post's poll.
<p>This poll question will be up for one month and will be maintained over in the sidebar should you wish to come back at a later date and take part. We look forward to seeing your responses!</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/03/blog-poll-what-keeps-you-up-at-night/#comments" thr:count="2"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/03/blog-poll-what-keeps-you-up-at-night/feed/atom/" thr:count="2"/>
		<thr:total>2</thr:total>
			</entry>
		<entry>
		<author>
			<name>Przemysław Malkowski</name>
					</author>
		<title type="html"><![CDATA[Percona Server for MySQL Highlights &#8211; binlog_space_limit]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/03/percona-server-for-mysql-highlights-binlog_space_limit/" />
		<id>https://www.percona.com/blog/?p=57460</id>
		<updated>2019-07-03T14:19:00Z</updated>
		<published>2019-07-03T14:19:00Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona Server for MySQL" /><category scheme="https://www.percona.com/blog" term="Replication" /><category scheme="https://www.percona.com/blog" term="Binary Log" /><category scheme="https://www.percona.com/blog" term="Disk space" />		<summary type="html"><![CDATA[<img width="200" height="105" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-200x105.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Server for MySQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" />I think it is often confusing to compare upstream MySQL and Percona Server for MySQL, and some helpful information can be found in the introductory notes. But what does that really mean for an ordinary DBA, especially if none of the known big extra features are important in a particular use case? In this article, [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/03/percona-server-for-mysql-highlights-binlog_space_limit/"><![CDATA[<img width="200" height="105" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-200x105.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Server for MySQL" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL.jpg 1200w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright wp-image-58421 size-medium" src="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-300x157.jpg" alt="Percona Server for MySQL" width="300" height="157" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/Percona-Server-for-MySQL.jpg 1200w" sizes="(max-width: 300px) 100vw, 300px" />I think it is often confusing to compare upstream MySQL and Percona Server for MySQL, and some helpful information can be found in the <a target="_blank" href="https://www.percona.com/software/mysql-database/percona-server">introductory notes</a>. But what does that really mean for an ordinary DBA, especially if none of the known big extra features are important in a particular use case?</p>
<p>In this article, I would like to start a series of short blog posts highlighting small, often less known, but potentially useful features, available in Percona Server for MySQL. Let&#8217;s start with a relatively new feature.</p>
<h3>Limit the disk space used by binary logs problem</h3>
<p>Historically, dealing with binary logs and the disk space used by them was quite challenging. The only thing that let you control this is the <a target="_blank" class="external-link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_expire_logs_days" rel="nofollow"><code class="literal">expire_logs_days</code></a> variable, which can be useless in some scenarios (binlogs growing faster than can be purged) and does not solve all problems. Therefore, in order to avoid disk full and server crashes, it is often needed to create smart scripts that monitor the situation and call <code class="literal">PURGE BINARY LOGS TO</code> when necessary.</p>
<p>This isn&#8217;t very convenient and so a feature request appeared in September 2012 &#8211; <a target="_blank" href="https://bugs.mysql.com/bug.php?id=66733">https://bugs.mysql.com/bug.php?id=66733</a> to use the already existing logic for relay logs and apply it for binary logs as well. It has not been addressed in upstream yet.</p>
<p>We attempted to deal with it in Percona Server by introducing <a target="_blank" href="https://www.percona.com/doc/percona-server/5.5/flexibility/max_binlog_files.html#max_binlog_files"><code class="literal">max_binlog_files</code></a> variable (<em>Percona Server</em> 5.5.27, October 2012). This was a good step forward, but still not as good as expected. Even though limiting the number of binary log files gives much better control in terms of disk space used by them, it has one serious downside. It does not take into account situations when there may be binary logs lot smaller or a lot bigger then the <code>max_binlog_size</code> specifies.</p>
<p>Most common situations like that may be that the server rotates to a new binary log faster due to FLUSH LOGS issued, or due to restarts. A couple of such FLUSH commands in a row may completely ruin the ability to point in time recovery by losing a large part of the history of the binary log. An opposite case is when a binary log gets bigger than maximum size due to large transactions (which are never going to be split).</p>
<p>Recently, a new variable in MySQL 8.0 has replaced the <a target="_blank" class="external-link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_expire_logs_days" rel="nofollow"><code class="literal">expire_logs_days</code></a>&#8211; the <a target="_blank" class="link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_binlog_expire_logs_seconds"><code class="literal">binlog_expire_logs_seconds</code></a>and also a period of 30 days became the new default (previously it was unlimited).  It is certainly better to have more precise control on binary logs history then &#8220;days&#8221;, for example, we can set the rotation period to 2 days 6 hours and 30 minutes or whatever best matches the backup schedule. Again, a good time limit control on binary logs may not be sufficient in all cases and won&#8217;t help much when it is hard to predict how much data will be written or changed over time. Only space-based limits can actually save us from hitting the disk space problem.</p>
<h3>Expected solution is there</h3>
<p>Finally, something better has arrived, the <strong><a target="_blank" href="https://www.percona.com/doc/percona-server/5.7/flexibility/max_binlog_files.html#binlog_space_limit"><code class="literal">binlog_space_limit</code></a> </strong>variable, introduced in Percona Server 5.7.23 on September 2018 (ported to 8.0 as well). The idea is very simple: it does the same thing as <a target="_blank" class="link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-slave.html#sysvar_relay_log_space_limit"><code class="literal">relay_log_space_limit</code></a> does for the relay logs. Finally, both sides of binary logging have comparable functionality when it comes to controlling the space on the disk.</p>
<h3>How to use it</h3>
<p>When the <code>binlog_space_limit</code> variable is set to 0 (default), the feature is turned off and no limit is set apart from the one imposed by <a target="_blank" class="external-link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_expire_logs_days" rel="nofollow"><code class="literal">expire_logs_days</code></a>. I think it is a good idea is to dedicate a separate disk partition for the binary logs (or binary and redo logs), big enough to keep as many of them as needed for a good backup and replication strategy, and set the variable to around 95% of this partition size. Remember, that even though <a target="_blank" class="external-link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_max_binlog_size" rel="nofollow"><code>max_binlog_size</code></a>defines the maximum size of a single binary log, this is not a hard limit. This is because a single transaction is never split between multiple binlog files and has to be written to the same binary log as a whole.</p>
<p>Let&#8217;s say the <code>max_binlog_size</code> is set to 1GB (default and maximum), and the <code>binlog_space_limit</code> is set to 10GB, but it happens that a really big transaction changes 3GB of data. What will happen with the <code>binlog_space_limit</code> set is that in order to store that new 3GB in the binary log (when the total size of logs is going to hit the limit), it will remove as many old binary logs as needed first, in this case, up to three oldest ones. This way there is no risk in hitting disk full situation even if the free disk space is smaller then the new binary log addition would need. But still, I would recommend reserving at least little more than 1GB of free disk space for such partition, just in case.</p>
<p>Example settings for a 16GB disk partition, dedicated for binary logs only, may look like this:</p>
<div class="code panel pdl conf-macro output-block" data-hasbody="true" data-macro-name="code">
<div class="codeContent panelContent pdl">
<div>
<div id="highlighter_967192" class="syntaxhighlighter sh-confluence nogutter java">
<table border="0" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td class="code">
<div class="container" title="Hint: double-click to select code">
<div class="line number1 index0 alt2"><code class="java plain">[mysqld]</code></div>
<div class="line number2 index1 alt1"><code class="java plain">max_binlog_size = 512M</code></div>
<div class="line number3 index2 alt2"><code class="java plain">binlog_space_limit = 15G</code></div>
</div>
</td>
</tr>
</tbody>
</table>
<p>One thing that could be still improved about this functionality is to make this variable dynamic. But to stay consistent with the one for relay logs, it would be best if the upstream makes this one dynamic first 🙂</p>
<h3>Best binlog rotation strategy?</h3>
</div>
<p>There may be still cases where I think you would rather prefer to stay with a time-based limit. Let&#8217;s imagine a situation where the full backups are made every week, and the <code>binlog_space_limit</code> is set to hold an average total size of two weeks of writes, so all should be safe. However, some heavy maintenance tasks alter a large portion of the data, creating more binary logs in one day then usually it takes two weeks. This may also break the point in time recovery capability. So if possible, maybe better to give an ample supply for the disk partition and specify 8 days rotation via <a target="_blank" class="external-link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_expire_logs_days" rel="nofollow"><code class="literal">expire_logs_days</code></a> or <a target="_blank" class="link" href="https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_binlog_expire_logs_seconds"><code class="literal">binlog_expire_logs_seconds</code></a>instead.</p>
<div id="highlighter_967192" class="syntaxhighlighter sh-confluence nogutter java">
<h3>Do you like it?</h3>
<p>I am very interested to find out if this feature was already noticed by the community and how useful you think it is. Your comments are more than welcome!</p>
</div>
</div>
</div>
</div>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/03/percona-server-for-mysql-highlights-binlog_space_limit/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/03/percona-server-for-mysql-highlights-binlog_space_limit/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>David Quilty</name>
					</author>
		<title type="html"><![CDATA[Percona Live 2019&#8217;s Top 10 Highest-Rated Talks]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/02/percona-live-2019-highest-rated-talks/" />
		<id>https://www.percona.com/blog/?p=58319</id>
		<updated>2019-07-02T15:17:51Z</updated>
		<published>2019-07-02T15:17:51Z</published>
		<category scheme="https://www.percona.com/blog" term="Percona Live" /><category scheme="https://www.percona.com/blog" term="percona live" />		<summary type="html"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Live 2019 Highest Rated Talks" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-300x169.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-367x206.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked.jpg 712w" sizes="(max-width: 200px) 100vw, 200px" />The votes have been tallied, and we&#8217;re happy to fill you in on the Top 10 highest-rated talks at the Percona Live Open Source Database Conference 2019! A big thanks to everyone who voted, and if you didn&#8217;t attend any of these at the conference, now&#8217;s your chance to see what you missed. Percona Live [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/02/percona-live-2019-highest-rated-talks/"><![CDATA[<img width="200" height="112" src="https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-200x112.jpg" class="webfeedsFeaturedVisual wp-post-image" alt="Percona Live 2019 Highest Rated Talks" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-200x112.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-300x169.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked-367x206.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/PL-Video-Post-Highest-Ranked.jpg 712w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-size: 17px;">The votes have been tallied, and we&#8217;re happy to fill you in on the <strong>Top 10 highest-rated talks</strong> at the Percona Live Open Source Database Conference 2019! A big thanks to everyone who voted, and if you didn&#8217;t attend any of these at the conference, now&#8217;s your chance to see what you missed. </span>Percona Live conferences provide the open source database community with an opportunity to discover and discuss the latest open source trends, technologies, and innovations. The conference includes the best and brightest innovators and influencers in the open source database industry.</p>
<h3>10. MongoDB Data Security &#8211; Custom Rules and Views &#8211; Adamo Tonete (Percona)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/YqJ12QkRbCg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<p>In this presentation, Adamo Tonete discusses how to create custom rules when the default rules are not enough for the application. Have you ever needed to give a more permissive rule to a user just because this user wanted to run a specific command? Also, he discusses how to use view for hiding fields from users when we don&#8217;t want them to read all of the collection.</p>
<p>&nbsp;</p>
<h3>9. Scaling MySQL at Venmo &#8211; Dong Wang (PayPal), Heidi Wang (PayPal), Van Pham (Venmo)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/7lQ-iBU7auQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<p>In this presentation, speakers discuss the MySQL architecture and application ecosystem in Venmo, scalability challenges of MySQL for Venmo applications for Super Bowl peak traffic, short term scalability improvements for peak traffic, including horizontal and vertical scalability approaches, and case studies of MySQL performance tuning.</p>
<p>&nbsp;</p>
<h3>8. Vitess: Running Sharded MySQL on Kubernetes &#8211; Sugu Sougoumarane &amp; Dan Kozlowski (PlanetScale)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/v7oxiVmGXp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></p>
<p>Sugu Sougoumarane &amp; Dan Kozlowski (PlanetScale) discuss how to shard MySQL on Kubernetes in Vitess. Vitess has continued to evolve into a massively scalable sharded solution for the cloud. It&#8217;s is now used for storing core business data for companies like Slack, Square, JD.com, and many others. This session will cover the high-level features of Vitess with a focus on what makes it cloud-native.</p>
<p>&nbsp;</p>
<h3>7. MySQL Technology Evolutions at Facebook &#8211; Yoshinori Matsunobu (Facebook)</h3>
<p><img class="alignnone size-full wp-image-58403" src="https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post.jpg" alt="" width="560" height="315" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post.jpg 560w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-200x113.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-300x169.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-367x206.jpg 367w" sizes="(max-width: 560px) 100vw, 560px" /></p>
<p>In this talk, Yoshinori Matsunobu (Facebook) introduces how they evolved MySQL for their workloads and modern hardware, including enhanced replication, space-efficient storage engine, and multi-tenancy.</p>
<p>&nbsp;</p>
<h3>6. The Highs and Lows of Running a Distributed Database on Kubernetes &#8211; Alex Robinson (Cockroach Labs)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/LhrN_8EizmE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<p>Alex Robinson talks about his experiences trying to reliably run CockroachDB, the open source distributed SQL database, on Kubernetes, optimize its performance, and help others do the same in their heterogeneous environments. Learn about what kinds of stateful applications can most easily be run in containers, which Kubernetes features and usage patterns are most helpful for running them, along with some pitfalls encountered along the way.</p>
<p>&nbsp;</p>
<h3>5. Databases at Scale, at Square &#8211; Emily Slocombe (Square)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/972Hn16QRT4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></p>
<p>Emily Slocombe (Square) discusses databases at scale. At Square, they operate thousands of database instances to power a financial network, from payments to payroll. In a word: money. &#8220;Mission-critical&#8221; isn&#8217;t critical enough. Come learn how they operate MySQL and Redis with billions of dollars at stake. They&#8217;ll look at everything: configuration, management, monitoring, tooling, security, high-availability, replication and more.</p>
<p>&nbsp;</p>
<h3>4. Managing MySQL at Scale in Facebook &#8211; Junyi Lu (Facebook)</h3>
<p><img class="alignnone size-full wp-image-58403" src="https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post.jpg" alt="" width="560" height="315" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post.jpg 560w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-200x113.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-300x169.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/My-Post-367x206.jpg 367w" sizes="(max-width: 560px) 100vw, 560px" /></p>
<p>Junyi Lu (Facebook) covers a quick intro to shard/replicaset/instance/host and the scale of FB, the lifecycle of an instance, the tools used for moving instance between states: MPS Copy, rebuild_db etc., shard movements(OLM), and test infrastructure: Shadow and Merlindb.</p>
<p>&nbsp;</p>
<h3>3. MySQL Group Replication: The Magic Explained v2 &#8211; Frédéric Descamps (Oracle)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/DkKfuAP764k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></p>
<p>Frédéric Descamps (Oracle) explains how MySQL Group Replication replication works. This is a theoretical talk in which he explains in detail what replication is and how it works, what certification is and how it&#8217;s done. Also, he talks about XCOM and GCS.</p>
<p>&nbsp;</p>
<h3>2. Part 2: MySQL InnoDB Cluster in a Nutshell: The Saga Continues with 8.0, the full guide &#8211; Kenny Grypp &amp; Frédéric Descamps (Oracle)</h3>
<p><a target="_blank" href="https://www.percona.com/resources/technical-presentations/part-2-mysql-innodb-cluster-nutshell-saga-continues-80-full-guide"><img class="alignleft wp-image-58324 size-full" src="https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-InnoDB-Cluster-in-a-Nutshell.png" alt="MySQL InnoDB Cluster in a Nutshell" width="560" height="308" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-InnoDB-Cluster-in-a-Nutshell.png 560w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-InnoDB-Cluster-in-a-Nutshell-200x110.png 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-InnoDB-Cluster-in-a-Nutshell-300x165.png 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/MySQL-InnoDB-Cluster-in-a-Nutshell-367x202.png 367w" sizes="(max-width: 560px) 100vw, 560px" /></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>During this tutorial, attendees got their hands on virtual machines and migrated standard Master-Slave architectures to the new MySQL InnoDB Cluster (native Group Replication) with minimal downtime. After explaining what Group Replication is and how it works (the magic behind it), Kenny and Frédéric experiment with multiple use cases to understand MySQL Group Replication. (Click image for slides)</p>
<p>&nbsp;</p>
<h3>1. MySQL Shell: The Best DBA tool? How to Use the MySQL Shell as a Framework for DBAs &#8211; Frédéric Descamps (Oracle)</h3>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/KBwTfn69feI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="allowfullscreen"></iframe></p>
<p>In this talk, Frédéric shows us how to first configure the Shell for a nice, good looking experience with MySQL, and show some of the basic commands and objects. After this brief overview,  he shows how it&#8217;s possible to extend the Shell, and how he hacked it to create an Innotop clone inside the MySQL Shell.</p>
<p><span style="font-size: 17px;">Thanks to all sponsors, presenters, and attendees for a successful Percona Live Open Source Database Conference 2019!  <a target="_blank" href="https://www.percona.com/live/amsterdam19" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Percona Live Europe 2019</a> is coming up next, taking</span> place in Amsterdam, Netherlands from September 30 to October 2, 2019.  The <a target="_blank" href="https://www.percona.com/cfp" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">Call for Papers</a> is now open, and we invite you to submit your speaking proposal for breakouts, tutorials, or lightning talks at any level of expertise, from beginner to expert.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/02/percona-live-2019-highest-rated-talks/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/02/percona-live-2019-highest-rated-talks/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Mike Benshoof</name>
					</author>
		<title type="html"><![CDATA[ALTERS, Foreign Keys, and Metadata Locks, oh my!]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/02/alters-foreign-keys-and-metadata-locks-oh-my/" />
		<id>https://www.percona.com/blog/?p=57411</id>
		<updated>2019-07-02T13:23:36Z</updated>
		<published>2019-07-02T13:23:36Z</published>
		<category scheme="https://www.percona.com/blog" term="Percona Server for MySQL" /><category scheme="https://www.percona.com/blog" term="Performance Schema" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="ALTERS, Foreign Keys, and Metadata Locks" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />I’m sure you’ve seen it &#8211; you kick off an ALTER and you get the dreaded “waiting on metadata lock”.  In many cases, this is expected if you are actively working on the table. However, I recently had a case with a client where the table being altered was rarely updated and very small (&#60;100 [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/02/alters-foreign-keys-and-metadata-locks-oh-my/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="ALTERS, Foreign Keys, and Metadata Locks" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-weight: 400;"><img class="alignright size-medium wp-image-58384" src="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-300x200.jpeg" alt="ALTERS, Foreign Keys, and Metadata Locks" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-1024x683.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/ALTERS-Foreign-Keys-and-Metadata-Locks.jpeg 1500w" sizes="(max-width: 300px) 100vw, 300px" />I’m sure you’ve seen it &#8211; you kick off an ALTER and you get the dreaded “waiting on metadata lock”.  In many cases, this is expected if you are actively working on the table. However, I recently had a case with a client where the table being altered was rarely updated and very small (&lt;100 rows).  The ALTER just sat for hours during a load test (more on this shortly) and never completed until the load test was stopped. Upon stopping the load test, the ALTER completed in less than a second as expected.  So what was going on here?</span></p>
<h3>Check Foreign Keys</h3>
<p><span style="font-weight: 400;">My first instinct, whenever there is odd locking, is to check foreign keys.  Naturally, this table had some FKs referencing a much busier table. However, this behavior still seemed rather strange.  When running an ALTER against a table, there is a request for a SHARED_UPGRADEABLE metadata lock against the child table. There is also a SHARED_READ_ONLY metadata lock against the parent and this is where things can get messy.</span></p>
<p><span style="font-weight: 400;">Let’s take a look at how MDLs are acquired per the documentation (</span><a target="_blank" href="https://dev.mysql.com/doc/refman/en/metadata-locking.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">https://dev.mysql.com/doc/refman/en/metadata-locking.html</span></a><span style="font-weight: 400;">:</span></p>
<p><i><span style="font-weight: 400;">If there are multiple waiters for a given lock, the </span></i><b><i>highest-priority lock request is satisfied first</i></b><i><span style="font-weight: 400;">, with an exception related to the max_write_lock_count system variable. </span></i><b><i>Write lock requests have higher priority than read lock requests.</i></b></p>
<p><span style="font-weight: 400;">It is important to note that the lock order is serialized:</span></p>
<p><i><span style="font-weight: 400;">Statements acquire metadata locks one by one, not simultaneously, and perform deadlock detection in the process.</span></i></p>
<p><span style="font-weight: 400;">Normally, when thinking of a queue, we think of a FIFO process.  If I issue the following three statements (in this order), they would complete in this order:</span></p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;">INSERT INTO parent…</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">ALTER TABLE child…</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">INSERT INTO parent…</span></li>
</ol>
<p><span style="font-weight: 400;">However, as the child ALTER statement requests a </span><b>read</b><span style="font-weight: 400;"> lock against parent, the two inserts will complete PRIOR to the ALTER despite the ordering.  Here is a sample scenario in which this can be demonstrated:</span></p>
<p><b>Table setup and initial population:</b></p><pre class="crayon-plain-tag">CREATE TABLE `parent` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `val` varchar(10) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;

CREATE TABLE `child` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `parent_id` int(11) DEFAULT NULL,
  `val` varchar(10) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_parent` (`parent_id`),
  CONSTRAINT `fk_parent` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON DELETE CASCADE ON UPDATE NO ACTION
) ENGINE=InnoDB;

INSERT INTO `parent` VALUES (1, "one"), (2, "two"), (3, "three"), (4, "four");</pre><p><strong>Session 1:</strong></p><pre class="crayon-plain-tag">start transaction;
update parent set val = "four-new" where id = 4;</pre><p><strong>Session 2:</strong></p><pre class="crayon-plain-tag">alter table child add index `idx_new` (val);</pre><p><strong>Session 3:</strong></p><pre class="crayon-plain-tag">start transaction;
update parent set val = "three-new" where id = 3;</pre><p><span style="font-weight: 400;">At this point, session 1 has an open transaction and is in sleep with a write metadata lock granted on parent.  Session 2 has an upgradeable (write) lock granted on child and is waiting on a read lock on the parent. Finally, session 3 has a granted write lock against parent:</span></p><pre class="crayon-plain-tag">mysql&gt; select * from performance_schema.metadata_locks;
+-------------+-------------+-------------------+---------------+-------------+
| OBJECT_TYPE | OBJECT_NAME | LOCK_TYPE         | LOCK_DURATION | LOCK_STATUS |
+-------------+-------------+-------------------+---------------+-------------+
| TABLE       | child       | SHARED_UPGRADABLE | TRANSACTION   | GRANTED     | &lt;- ALTER (S2)
| TABLE       | parent      | SHARED_WRITE      | TRANSACTION   | GRANTED     | &lt;- UPDATE (S1)
| TABLE       | parent      | SHARED_WRITE      | TRANSACTION   | GRANTED     | &lt;- UPDATE (S3)
| TABLE       | parent      | SHARED_READ_ONLY  | STATEMENT     | PENDING     | &lt;- ALTER (S2)
+-------------+-------------+-------------------+---------------+-------------+</pre><p><span style="font-weight: 400;">Notice, the only session with a pending lock status is session 2 (the ALTER).  Session 1 and session 3 (issued before and after the ALTER respectively) both have been granted the write locks.  Where the ordering breaks down is when the commit happens on session 1. When thinking about an ordered queue, one would expect session 2 to acquire the lock and things would just move on.  However, due to the priority nature of the metadata lock system, session 2 still waits and now session 3 has the lock. </span></p>
<p><span style="font-weight: 400;">If another write session comes in and starts a new transaction and acquires a write lock against the parent table, then even when session 3 completes, the ALTER will still be blocked.  You can see where this is going… </span></p>
<p><span style="font-weight: 400;">As long as I keep an active transaction that has an MDL against the parent table open, the ALTER on the child table will never complete.  Making this worse, since the write lock on the child table was successful (but the full statement is waiting on acquiring the parent read lock), all incoming read requests against the child table will be blocked!!</span></p>
<p><span style="font-weight: 400;">Also, think about how you normally try to troubleshoot a statement that won’t complete.  You look at transactions (both in the processlist and InnoDB status) that have been open for a longer time.  But since the blocking thread is now </span><b>younger</b><span style="font-weight: 400;"> than the ALTER thread, the oldest transaction/thread you will see is the ALTER.  Queue hair pulling!!</span></p>
<p><span style="font-weight: 400;">This is exactly what was happening in this scenario.  In preparation for a release, our client was running their ALTER statements in conjunction with a load test (a very good practice!) to ensure a smooth release.  The problem was that the load test kept an active write transaction open against the parent table. That isn’t to say it just kept writing, but rather there were multiple threads and one was </span><b>ALWAYS</b><span style="font-weight: 400;"> active.  This prevented the ALTER from ever completing and blocked ensuing read requests to the relatively static child table.</span></p>
<p><span style="font-weight: 400;">Fortunately, there is a solution to this problem (other than banishing FKs from the schema).  The variable </span><b>max_write_lock_count</b><span style="font-weight: 400;"> (</span><a target="_blank" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_write_lock_count" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_write_lock_count</span></a><span style="font-weight: 400;">) can be used to allow read locks to be granted ahead of write locks after a threshold of consecutive write locks.  By default, this variable is set to 18,446,744,073,709,551,615. So you just have to wait for </span><b><i>18 quintillion </i></b><span style="font-weight: 400;">write requests to complete before the read would be granted.  To put that in perspective, if you were issuing 10,000 writes/sec against that table, your read would be blocked for </span><b>58 million years</b><span style="font-weight: 400;">.  </span></p>
<p><span style="font-weight: 400;">To prevent this from happening, you can simply reduce max_write_lock_count to a small number (say 10?) and after every 10 write locks get acquired, the MDL subsystem will look for pending read locks, grant one, then move back to writes.  Problem solved!</span></p>
<p><span style="font-weight: 400;">Being a dynamic variable, this can be adjusted at runtime to allow the waiting ALTER to complete.  In general, this is more of an edge case as there normally is some time in between writes to a table for read locks to get acquired.  </span><span style="font-weight: 400;">However, if your use case keeps concurrent sessions running that ALWAYS have a transaction against a table that is referenced as an FK, you could see this situation crop up.  Fortunately, the fix is straightforward and can be done on the fly!</span></p>
<p><span style="font-weight: 400;">NOTE:  This troubleshooting was made possible through the performance schema and enabling the metadata_locks table as described here: </span><a target="_blank" href="https://dev.mysql.com/doc/refman/5.7/en/metadata-locks-table.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">https://dev.mysql.com/doc/refman/5.7/en/metadata-locks-table.html</span></a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/02/alters-foreign-keys-and-metadata-locks-oh-my/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/02/alters-foreign-keys-and-metadata-locks-oh-my/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Mike Benshoof</name>
					</author>
		<title type="html"><![CDATA[MySQL to the Cloud!  Thoughts on Migrating Away from On-Premise]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/01/mysql-migrating-from-on-premise-to-the-cloud/" />
		<id>https://www.percona.com/blog/?p=58303</id>
		<updated>2019-07-19T20:11:47Z</updated>
		<published>2019-07-01T17:07:04Z</published>
		<category scheme="https://www.percona.com/blog" term="MySQL" />		<summary type="html"><![CDATA[<img width="200" height="113" src="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-200x113.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="migrating from on-premise to the cloud" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-200x113.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-300x169.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-1024x576.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-367x206.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" />The CTO has issued the decree: “We are moving to the cloud!”  Great, so now what do we do? When it comes to migrating from on-premise to the cloud, there are many factors to consider and decisions that need to be made.  First (and probably most important) on that list: managed DBaaS or setup and [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/01/mysql-migrating-from-on-premise-to-the-cloud/"><![CDATA[<img width="200" height="113" src="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-200x113.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="migrating from on-premise to the cloud" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-200x113.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-300x169.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-1024x576.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-367x206.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud.jpeg 1500w" sizes="(max-width: 200px) 100vw, 200px" /><p><span style="font-weight: 400;"><img class="alignright size-medium wp-image-58376" src="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-300x169.jpeg" alt="migrating from on-premise to the cloud" width="300" height="169" srcset="https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-300x169.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-200x113.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-1024x576.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud-367x206.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/07/migrating-from-on-premise-to-the-cloud.jpeg 1500w" sizes="(max-width: 300px) 100vw, 300px" />The CTO has issued the decree: “We are moving to the cloud!”  Great, so now what do we do? When it comes to migrating from on-premise to the cloud, there are many factors to consider and decisions that need to be made.  First (and probably most important) on that list: managed DBaaS or setup and manage our own infrastructure?</span></p>
<h2>Type of Cloud</h2>
<p><span style="font-weight: 400;">Managed DBaaS options are great but come with some limitations.  The main two questions that should be considered here are around staff/experience and current architecture/database design.  In cases where there is a very limited database (or operations) team, a DBaaS is a great choice. Much of the operational infrastructure is already in place with general best practices in place.  However, a big tradeoff comes around flexibility. In cases where the current infrastructure is complex (for reasons right or wrong), a DBaaS is definitely the wrong choice as you will be extremely limited in what you can do. </span></p>
<p><span style="font-weight: 400;">Here are some high-level considerations for determining if DBaaS is a good fit:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Simple schema</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Standard existing architecture (master/slave, VIP failover)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Standard CRUD workload (no elaborate procedures or business logic in the database)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Fairly easy to shard/partition data (vertical and/or horizontal)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Management considerations (DBaaS is “managed”, but still requires manual work that is different from on-premise installations)</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Minor upgrades/maintenance windows</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Query analysis</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Some standard operations (such as point-in-time recovery) are different in DBaaS</span></li>
</ul>
</li>
</ul>
<p><span style="font-weight: 400;">Naturally, this list isn’t all inclusive and the best fit will vary based on your specific use case, but it can serve as a good starting point.  Also, the complexity of managing a production-grade infrastructure in the cloud manually isn&#8217;t trivial and many current on-premise techniques will need to be modified.</span></p>
<p><span style="font-weight: 400;">Once you have landed on a destination, the most important phase begins.  Testing! I can’t emphasize enough how important testing is for migration into the cloud.  But what should be tested and how? Here are the main things that need to be tested thoroughly before the migration:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Schema</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Workload</span></li>
</ul>
<h2>Schema Testing</h2>
<p><span style="font-weight: 400;">This aspect of the testing should be fairly straightforward. Regardless of the specifics of your migration (DBaaS or self-managed), you’ll want to take a current snapshot of your database.  Ideally, this would be done using Percona’s Xtrabackup tool (procedure </span><a target="_blank" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;">).  However, mysqldump (or mydumper/myloader) is also a viable option if a binary snapshot isn’t supported by your target DBaaS.  The biggest downside to a logical dump is the time it takes to restore, but it does allow you to make tweaks to the schema prior to the reload.</span></p>
<p><span style="font-weight: 400;">One of the biggest issues when doing a restore into a DBaaS is having triggers or procedures defined by the root user.  In many cases (such as Amazon RDS), the root user isn’t available and this will cause the import to fail. As a best practice, you shouldn’t be creating procedures or triggers with the root user, but unfortunately, it isn’t uncommon.  Once you have verified that the snapshot is successfully restored in your target cloud environment, you can move onto the workload and performance testing.</span></p>
<h2>Workload Testing</h2>
<p><span style="font-weight: 400;">In many cases, these can be tested in parallel and both are equally important.  In preparation to test query compatibility and performance, you will want to capture live query traffic </span><b><i>immediately following</i></b><span style="font-weight: 400;"> the snapshot used to test schema compatibility.  It is important to make sure the capture is done right after a snapshot to ensure an accurate representation of the schema in time that matches the traffic.  Testing concurrent write performance on a traffic set that will never lock or modify any rows won’t be realistic when compared with traffic that is actually modifying production data.</span></p>
<p><span style="font-weight: 400;">The best way to capture traffic is by using the slow query log (as it is most compatible with other tools and shares the most information).  Ideally, you can set the </span><b>long_query_time = 0</b><span style="font-weight: 400;"> to capture all query traffic, but your production traffic may make that challenging and adjustments may need to be made.</span></p>
<p><span style="font-weight: 400;">From Percona Toolkit, pt-upgrade is the primary tool to test that there are no differences in the workload.  The tool checks to make sure that the following are the same on both the current server and the new server:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">Row Count</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Row Data</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Query Time (difference should be within an order a magnitude)</span></li>
<li style="font-weight: 400;"><span style="font-weight: 400;">Warnings/errors</span></li>
</ul>
<p><span style="font-weight: 400;">Assuming this tool shows no errors, you’ll want to ensure similar performance.  If your organization has a way to generate a realistic load (note that being realistic is important here), an extra step would be to compare performance metrics between the current environment and the target environment.  The easiest way to compare is to use a visualization tool such as <a target="_blank" href="https://www.percona.com/software/database-tools/percona-monitoring-and-management">Percona Monitoring and Management</a> to ensure no detrimental metrics (lower throughput, major differences in IO/CPU, etc) are displayed.</span></p>
<h2>Pre-Migration</h2>
<p><span style="font-weight: 400;">Now that all the testing has happened, results have been signed off on, and the stakeholders are happy, it is time to actually move your data to the cloud.  The first step is to restore a snapshot of the current production. Again, this can be done using a logical snapshot (mysqldump) or a binary snapshot (xtrabackup if possible).  Just like setting up any other slave server, capturing the binlog coordinates is also needed.  </span></p>
<p><span style="font-weight: 400;">With the snapshot loaded in the target cloud server, it is time to set up replication from the current production environment to the new cloud server.  This is one very important step in the migration process &#8211; there needs to be a secure connection set up between the cloud server and production. This can be achieved in a few ways:</span></p>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">VPN tunnel to the cloud provider</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">This is the easiest method, assuming you have a VPN device available onsite</span></li>
</ul>
</li>
<li style="font-weight: 400;"><span style="font-weight: 400;">SSH tunnel</span>
<ul>
<li style="font-weight: 400;"><span style="font-weight: 400;">In this method, you create an SSH tunnel locally on the source server and connect to that port from the cloud server</span></li>
</ul>
</li>
</ul>
<p><span style="font-weight: 400;">In either scenario, you will use the replication coordinates from your snapshot and start replication from that point.  Again, various cloud deployments may be slightly different in terms of how this is set up (RDS, for example, uses the </span><b><i>CALL</i></b><a target="_blank" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/mysql_rds_set_external_master.html" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer"><b><i> mysql.rds_set_external_master</i></b></a><span style="font-weight: 400;"> procedure), but the concept is generally the same.</span></p>
<p><span style="font-weight: 400;">It is very important that this initial replication link between the cloud server and the current production database is encrypted.  Even with “non-critical” data, it is a best practice to ensure that your database traffic is encrypted if it needs to be sent over the WAN!</span></p>
<h2>Cutover Time</h2>
<p><span style="font-weight: 400;">Finally, a replica is in the cloud, is in sync, and you are ready to pull the trigger.  Through all of the testing, this should hopefully be a “trivial non-event”. In the case where the entire environment is moving to the cloud (application servers included), the new instances should be already be configured to use the new cloud server for a data source.  For the migration to go live, traffic simply needs to hit the new application servers in the cloud.</span></p>
<p><span style="font-weight: 400;">In the other scenario of keeping the application servers local while just moving the database tier to the cloud, you would simply need to update your application servers to point to the new database server.  Like the replication piece, if the application servers will live outside of the cloud, </span><b>SSL connectivity is still a best practice</b><span style="font-weight: 400;">.    </span></p>
<h2>Summary</h2>
<p><span style="font-weight: 400;">Thankfully, this process isn’t too different from a standard migration to a new server.  The biggest differences could potentially be found when migrating from on-premise to a DBaaS solution, but even then the process for executing the migration is similar.  Also, planning for cloud component failure (which again is a standard best practice, but equally important in the cloud) is something that should be considered in the plan.</span></p>
<p><a target="_blank" href="https://www.percona.com/software/mysql-database/percona-server">Learn more about Percona Server for MySQL</a></p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/01/mysql-migrating-from-on-premise-to-the-cloud/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/01/mysql-migrating-from-on-premise-to-the-cloud/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
		<entry>
		<author>
			<name>Jaime Sicam</name>
					</author>
		<title type="html"><![CDATA[Setting World-Writable File Permissions Prior to Preparing the Backup Can Break It]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/01/setting-world-writable-file-permissions-prior-to-preparing-the-backup-can-break-it/" />
		<id>https://www.percona.com/blog/?p=57909</id>
		<updated>2019-07-01T14:51:00Z</updated>
		<published>2019-07-01T14:50:03Z</published>
		<category scheme="https://www.percona.com/blog" term="Backups" /><category scheme="https://www.percona.com/blog" term="MySQL" /><category scheme="https://www.percona.com/blog" term="Percona XtraBackup" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Setting World-Writable File Permissions" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-1024x682.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions.jpeg 1400w" sizes="(max-width: 200px) 100vw, 200px" />It&#8217;s bad practice to provide world-writable access to critical files in Linux, though we&#8217;ve seen time and time again that this is done to conveniently share files with other users, applications, or services. But with Xtrabackup, preparing backups could go wrong if the backup configuration has world-writable file permissions. Say you performed a backup on [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/01/setting-world-writable-file-permissions-prior-to-preparing-the-backup-can-break-it/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="Setting World-Writable File Permissions" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-1024x682.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions.jpeg 1400w" sizes="(max-width: 200px) 100vw, 200px" /><p><img class="alignright size-medium wp-image-58294" src="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-300x200.jpeg" alt="Setting World-Writable File Permissions" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-1024x682.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/Setting-World-Writable-File-Permissions.jpeg 1400w" sizes="(max-width: 300px) 100vw, 300px" />It&#8217;s bad practice to provide world-writable access to critical files in Linux, though we&#8217;ve seen time and time again that this is done to conveniently share files with other users, applications, or services. But with Xtrabackup, preparing backups could go wrong if the backup configuration has world-writable file permissions.</p>
<p>Say you performed a backup on a MySQL instance configured with data-at-rest encryption using the keyring plugin. On the backup directory, the generated backup-my.cnf contains these instructions to load this plugin that will be used by Xtrabackup while preparing the backup:</p>
<p><em>backup-my.cnf</em></p><pre class="crayon-plain-tag">[mysqld]
innodb_checksum_algorithm=crc32
innodb_log_checksum_algorithm=strict_crc32
innodb_data_file_path=ibdata1:12M:autoextend
innodb_log_files_in_group=2
innodb_log_file_size=1073741824
innodb_fast_checksum=false
innodb_page_size=16384
innodb_log_block_size=512
innodb_undo_directory=./
innodb_undo_tablespaces=0
server_id=0
redo_log_version=1
plugin_load=keyring_file.so
server_uuid=00005726-0000-0000-0000-000000005726
master_key_id=1</pre><p>Perhaps you wanted to share the backup with another user, but made a mistake of making the directory and its contents world-writable: <code>chmod -R 777 /backup/mysql</code></p>
<p>When that user prepares the backup, the corresponding output will show that <span style="font-size: 16px;">Xtrabackup</span> ignored reading <em>backup-my.cnf</em> and so it doesn&#8217;t know that it has to load the keyring plugin to decrypt the .ibd files:</p><pre class="crayon-plain-tag">~$ xtrabackup --prepare --keyring-file-data=/backup/mysql/keyring --target-dir=/backup/mysql 
xtrabackup: [Warning] World-writable config file '/backup/mysql/backup-my.cnf' is ignored.
xtrabackup: recognized server arguments: 
xtrabackup: [Warning] World-writable config file '/backup/mysql/backup-my.cnf' is ignored.
xtrabackup: recognized client arguments: --prepare=1 --target-dir=/backup/mysql 
xtrabackup version 2.4.14 based on MySQL server 5.7.19 Linux (x86_64) (revision id: ef675d4)
xtrabackup: cd to /backup/mysql/
xtrabackup: This target seems to be not prepared yet.
InnoDB: Number of pools: 1
xtrabackup: xtrabackup_logfile detected: size=215089152, start_lsn=(3094928949)
xtrabackup: using the following InnoDB configuration for recovery:
xtrabackup:   innodb_data_home_dir = .
xtrabackup:   innodb_data_file_path = ibdata1:10M:autoextend
xtrabackup:   innodb_log_group_home_dir = .
xtrabackup:   innodb_log_files_in_group = 1
xtrabackup:   innodb_log_file_size = 215089152
xtrabackup: [Warning] World-writable config file './backup-my.cnf' is ignored.
xtrabackup: using the following InnoDB configuration for recovery:
xtrabackup:   innodb_data_home_dir = .
xtrabackup:   innodb_data_file_path = ibdata1:10M:autoextend
xtrabackup:   innodb_log_group_home_dir = .
xtrabackup:   innodb_log_files_in_group = 1
xtrabackup:   innodb_log_file_size = 215089152
xtrabackup: Starting InnoDB instance for recovery.
xtrabackup: Using 104857600 bytes for buffer pool (set by --use-memory parameter)
InnoDB: PUNCH HOLE support available
InnoDB: Mutexes and rw_locks use GCC atomic builtins
InnoDB: Uses event mutexes
InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
InnoDB: Compressed tables use zlib 1.2.8
InnoDB: Number of pools: 1
InnoDB: Using CPU crc32 instructions
InnoDB: Initializing buffer pool, total size = 100M, instances = 1, chunk size = 100M
InnoDB: Completed initialization of buffer pool
InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
InnoDB: Highest supported file format is Barracuda.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest2.ibd can't be decrypted.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest1.ibd can't be decrypted.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest4.ibd can't be decrypted.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest3.ibd can't be decrypted.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest5.ibd can't be decrypted.
InnoDB: Log scan progressed past the checkpoint lsn 3094928949
** redacted **
InnoDB: Doing recovery: scanned up to log sequence number 3097681408 (1%)
InnoDB: Doing recovery: scanned up to log sequence number 3102924288 (4%)
InnoDB: Doing recovery: scanned up to log sequence number 3108167168 (6%)
InnoDB: Doing recovery: scanned up to log sequence number 3113410048 (9%)
InnoDB: Doing recovery: scanned up to log sequence number 3118652928 (12%)
InnoDB: Doing recovery: scanned up to log sequence number 3123895808 (15%)
InnoDB: Doing recovery: scanned up to log sequence number 3129138688 (17%)
InnoDB: Doing recovery: scanned up to log sequence number 3134381568 (20%)
InnoDB: Starting an apply batch of log records to the database...
InnoDB: Progress in percent: 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 
** redacted **
InnoDB: Doing recovery: scanned up to log sequence number 3265453568 (89%)
InnoDB: Doing recovery: scanned up to log sequence number 3270696448 (91%)
InnoDB: Doing recovery: scanned up to log sequence number 3275939328 (94%)
InnoDB: Doing recovery: scanned up to log sequence number 3281182208 (97%)
InnoDB: Doing recovery: scanned up to log sequence number 3286158358 (100%)
InnoDB: Starting an apply batch of log records to the database...
InnoDB: Progress in percent: 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 
InnoDB: Apply batch completed
InnoDB: xtrabackup: Last MySQL binlog file position 568369058, file name mysql-bin.000004
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest1.ibd can't be decrypted.
InnoDB: Removing missing table `sbtest/sbtest1` from InnoDB data dictionary.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest2.ibd can't be decrypted.
InnoDB: Removing missing table `sbtest/sbtest2` from InnoDB data dictionary.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest3.ibd can't be decrypted.
InnoDB: Removing missing table `sbtest/sbtest3` from InnoDB data dictionary.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest4.ibd can't be decrypted.
InnoDB: Removing missing table `sbtest/sbtest4` from InnoDB data dictionary.
InnoDB: Encryption can't find master key, please check the keyring plugin is loaded.
InnoDB: Encryption information in datafile: ./sbtest/sbtest5.ibd can't be decrypted.
InnoDB: Removing missing table `sbtest/sbtest5` from InnoDB data dictionary.
InnoDB: Creating shared tablespace for temporary tables
InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
InnoDB: File './ibtmp1' size is now 12 MB.
InnoDB: 96 redo rollback segment(s) found. 1 redo rollback segment(s) are active.
InnoDB: 32 non-redo rollback segment(s) are active.
InnoDB: page_cleaner: 1000ms intended loop took 6627ms. The settings might not be optimal. (flushed=0 and evicted=0, during the time.)
InnoDB: 5.7.19 started; log sequence number 3286158358
InnoDB: xtrabackup: Last MySQL binlog file position 568369058, file name mysql-bin.000004</pre><p>Even if you fix the permissions on backup-my.cnf, if you try to prepare the same backup again, Xtrabackup will warn you that it has already prepared the backup.</p><pre class="crayon-plain-tag">~$ xtrabackup --prepare --keyring-file-data=/backup/mysql/keyring --target-dir=/backup/mysql 
xtrabackup: recognized server arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksum_algorithm=strict_crc32 --innodb_data_file_path=ibdata1:12M:autoextend --innodb_log_files_in_group=2 --innodb_log_file_size=1073741824 --innodb_fast_checksum=0 --innodb_page_size=16384 --innodb_log_block_size=512 --innodb_undo_directory=./ --innodb_undo_tablespaces=0 --server-id=0 --redo-log-version=1 
xtrabackup: recognized client arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksum_algorithm=strict_crc32 --innodb_data_file_path=ibdata1:12M:autoextend --innodb_log_files_in_group=2 --innodb_log_file_size=1073741824 --innodb_fast_checksum=0 --innodb_page_size=16384 --innodb_log_block_size=512 --innodb_undo_directory=./ --innodb_undo_tablespaces=0 --server-id=0 --redo-log-version=1 --prepare=1 --target-dir=/backup/mysql 
xtrabackup version 2.4.14 based on MySQL server 5.7.19 Linux (x86_64) (revision id: ef675d4)
xtrabackup: cd to /backup/mysql/
xtrabackup: This target seems to be already prepared.
InnoDB: Number of pools: 1</pre><p>This means that changes made while the backup was taking place will not be applied and what you have restored is an inconsistent, potentially corrupt backup. You need to perform a full backup again and make sure that you do not place world/other writable permissions on the backup this around so that you will not face the same issue.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/01/setting-world-writable-file-permissions-prior-to-preparing-the-backup-can-break-it/#comments" thr:count="1"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/01/setting-world-writable-file-permissions-prior-to-preparing-the-backup-can-break-it/feed/atom/" thr:count="1"/>
		<thr:total>1</thr:total>
			</entry>
		<entry>
		<author>
			<name>Ibrar Ahmed</name>
					</author>
		<title type="html"><![CDATA[Deep Dive Into PostgreSQL Indexes Webinar: Q &#038; A]]></title>
		<link rel="alternate" type="text/html" href="https://www.percona.com/blog/2019/07/01/deep-dive-into-postgresql-indexes-webinar-q-a/" />
		<id>https://www.percona.com/blog/?p=58322</id>
		<updated>2019-07-01T13:41:39Z</updated>
		<published>2019-07-01T13:39:13Z</published>
		<category scheme="https://www.percona.com/blog" term="PostgreSQL" />		<summary type="html"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="PostgreSQL Indexes" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-1024x682.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes.jpeg 1400w" sizes="(max-width: 200px) 100vw, 200px" />I want to thank everybody who attended my session “Deep Dive in PostgreSQL Indexes” in Austin. It was quite a wonderful experience! To cover a bigger audience, I also did a webinar on the same topic. There were many questions in that webinar, but unfortunately, there was not enough time to cover each and every [&#8230;]]]></summary>
				<content type="html" xml:base="https://www.percona.com/blog/2019/07/01/deep-dive-into-postgresql-indexes-webinar-q-a/"><![CDATA[<img width="200" height="133" src="https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-200x133.jpeg" class="webfeedsFeaturedVisual wp-post-image" alt="PostgreSQL Indexes" style="display: block; margin-bottom: 5px; clear:both;max-width: 100%;" link_thumbnail="" srcset="https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-1024x682.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2019/06/PostgreSQL-Indexes.jpeg 1400w" sizes="(max-width: 200px) 100vw, 200px" /><p>I want to thank everybody who attended my session “<a target="_blank" href="https://www.percona.com/resources/videos/deep-dive-postgresql-indexing">Deep Dive in PostgreSQL Indexes</a>” in Austin. It was quite a wonderful experience! To cover a bigger audience, I also did a webinar on the same topic. There were many questions in that webinar, but unfortunately, there was not enough time to cover each and every question, so I decided to have a followup Q&amp;A session with a blog post on the topic.</p>
<p>The recording and slides are available <a target="_blank" href="https://www.percona.com/resources/webinars/deep-dive-postgresql-indexing">here</a>. Below is the list of your questions.</p>
<h4>Q1: Is there a concept of the functional index in PostgreSQL?</h4>
<p>The functional index is when an index is defined on the result of a function. This is present in PostgreSQL 7. In later releases of PostgreSQL, you can define the index on the result of an expression. The expression can be any PostgreSQL expression. The function is a subset of expression.</p>
<h4>Q2: Can I create an index using a table row data that is attached to the index? Does my query have to use index-only-scan?</h4>
<p>Yes, if all selected columns are in the index, then index-only-scans will be used.</p>
<h4>Q3: It&#8217;s a little bit hard to choose the right index type. Is there any offer mechanism in Postgres for index types?</h4>
<p>Yes, that’s true that is hard to choose, but it all depends on your data and queries types. I have mentioned recommendations in my slides. There is no mechanism for that in PostgreSQL.</p>
<h4>Q4: Is it possible to manipulate which index is used by a query if you have multiple indexes?</h4>
<p>No, it’s optimizer’s responsibility to choose between defined the indexes. Believe that the PostgreSQL optimizer will choose the best option for you.</p>
<h4>Q5: If there&#8217;s a need to index a date column, would it be better to use B-TREE or BRIN index?</h4>
<p>A date data type is mostly aligned with physical storage. If you are not making many updates and delete operations, then go for the BRIN. But if you have too many updates and deletes, then B-TREE is best for you.</p>
<h4>Q6: For time-based filters what kind of index is preferable?</h4>
<p>Time is also aligned with physical storage on disk, please see the answer for Q5 above.</p>
<h4>Q6: How does the optimizer decide which index to use when multiple indexes are available?</h4>
<p>Optimizer calculates the cost of each plan and chooses the best based on the cost.</p>
<h4>Q7: How to identify if the index needs to rebuild or not?</h4>
<p>You can check the size of an index. Index bloat detecting is a technique. You can check PostgreSQL wiki <a target="_blank" href="https://wiki.postgresql.org/wiki/Index_Maintenance" target="_blank" rel="&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">https://wiki.postgresql.org/wiki/Index_Maintenance</a></p>
<h4>Q8: Can we create a clustered index like Oracle in PostgreSQL? More like the physical order of data in the index?</h4>
<p>PostgreSQL offers CLUSTER INDEX using which we can store the data of a table in the order of an Index. However, upon updates/deletes, the data has to be re-clustered to put back in the order of the specified index. However, all the features offered by a clustered index in Oracle are not possible.</p>
<h4>Q9: How does GiST index compare to things like Elasticsearch?</h4>
<p>If you want to see the comparison of Elasticsearch and PostgreSQL full search text, both are based on inverted indices. I don’t think there should be any significant performance difference, but I don’t have the numbers.</p>
<h4>Q10: Is Bitmap index default in PostgreSQL?</h4>
<p>No, B-TREE index is default in PostgreSQL.</p>
<h4>Q11: If B tree and Hash index work for &#8220;=&#8221; operator then which one need to select Hash?</h4>
<p>According to the documentation, hash performs better for the equality operator. In my opinion, if the column size is bigger then hash perform better than B-TREE, but in the case of smaller size index column B-TREE works better.</p>
<h4>Q13: Is there a best practice when to use GIN index and when to use B-Tree with the expression for JSONB columns?</h4>
<p>It’s not a matter of choice, there are some cases where you can only use GIN, and B-TREE will not work. There is an example in the slides for the same.</p>
<p>Once again thank you, everybody, for attending the session. If you still have any questions, do attend my session in <a target="_blank" href="https://postgresconf.org/users/1983" target="_blank" rel="&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;noopener&quot; noopener noreferrer" noopener noreferrer">PostgreSQL Beijing</a>, or write the question in the comment section.</p>
]]></content>
						<link rel="replies" type="text/html" href="https://www.percona.com/blog/2019/07/01/deep-dive-into-postgresql-indexes-webinar-q-a/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="https://www.percona.com/blog/2019/07/01/deep-dive-into-postgresql-indexes-webinar-q-a/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
			</entry>
	</feed>
