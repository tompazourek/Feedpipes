<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netflix TechBlog - Medium]]></title>
        <description><![CDATA[Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium]]></description>
        <link>https://medium.com/netflix-techblog?source=rss----2615bd06b42e---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netflix TechBlog - Medium</title>
            <link>https://medium.com/netflix-techblog?source=rss----2615bd06b42e---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 01 Aug 2019 21:16:42 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/netflix-techblog" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Evolution of Netflix Conductor:]]></title>
            <link>https://medium.com/netflix-techblog/evolution-of-netflix-conductor-16600be36bca?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/16600be36bca</guid>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[netflixoss]]></category>
            <category><![CDATA[workflow-automation]]></category>
            <category><![CDATA[workflow]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 31 Jul 2019 01:07:55 GMT</pubDate>
            <atom:updated>2019-07-31T01:45:58.064Z</atom:updated>
            <content:encoded><![CDATA[<h4>v2.0 and beyond</h4><p>By <a href="https://www.linkedin.com/in/anoop-panicker/">Anoop Panicker</a> and <a href="https://www.linkedin.com/in/kishore-banala/">Kishore Banala</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/591/1*2xkaGPihB1DcDgT4WECK3w.png" /></figure><p>Conductor is a workflow orchestration engine developed and open-sourced by Netflix. If you’re new to Conductor, this <a href="https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40">earlier blogpost</a> and the <a href="https://netflix.github.io/conductor/">documentation</a> should help you get started and acclimatized to Conductor.</p><p><a href="https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40">Netflix Conductor: A microservices orchestrator</a></p><p>In the last two years since inception, Conductor has seen wide adoption and is instrumental in running numerous core workflows at Netflix. Many of the Netflix Content and Studio Engineering services rely on Conductor for efficient processing of their business flows. The <a href="https://medium.com/netflix-techblog/implementing-the-netflix-media-database-53b5a840b42a">Netflix Media Database (NMDB)</a> is one such example.</p><p>In this blog, we would like to present the latest updates to Conductor, address some of the frequently asked questions and thank the community for their contributions.</p><h3>How we’re using Conductor at Netflix</h3><h4>Deployment</h4><p>Conductor is one of the most heavily used services within Content Engineering at Netflix. Of the multitude of modules that can be plugged into Conductor as shown in the image below, we use the Jersey server module, Cassandra for persisting execution data, <a href="https://github.com/Netflix/dynomite">Dynomite</a> for persisting metadata, <a href="https://github.com/Netflix/dyno-queues">DynoQueues</a> as the queuing recipe built on top of Dynomite, Elasticsearch as the secondary datastore and indexer, and Netflix <a href="http://netflix.github.io/spectator/en/latest/">Spectator</a> + <a href="https://github.com/Netflix/atlas">Atlas</a> for Metrics. Our cluster size ranges from 12–18 instances of AWS EC2 m4.4xlarge instances, typically running at ~30% capacity.</p><figure><img alt="Components of Netflix Conductor" src="https://cdn-images-1.medium.com/max/726/0*D6ZTp8Cerlke-rYq" /><figcaption>* — Cassandra persistence module is a partial implementation.</figcaption></figure><p>We do not maintain an internal fork of Conductor within Netflix. Instead, we use a wrapper that pulls in the latest version of Conductor and adds Netflix infrastructure components and libraries before deployment. This allows us to proactively push changes to the open source version while ensuring that the changes are fully functional and well-tested.</p><h4>Adoption</h4><p>As of writing this blog, Conductor orchestrates 600+ workflow definitions owned by 50+ teams across Netflix. While we’re not (yet) actively measuring the nth percentiles, our production workloads speak for Conductor’s performance. Below is a snapshot of our Kibana dashboard which shows the workflow execution metrics over a typical 7-day period.</p><figure><img alt="Dashboard with typical Conductor usage over 7 days" src="https://cdn-images-1.medium.com/max/1024/0*vJfIP9SvnsBqvaR5" /><figcaption>Typical Conductor usage at Netflix over a 7 day period.</figcaption></figure><h4>Use Cases</h4><p>Some of the use cases served by Conductor at Netflix can be categorized under:</p><ul><li>Content Ingest and Delivery</li><li>Content Quality Control</li><li>Content Localization</li><li>Encodes and Deployments</li><li>IMF Deliveries</li><li>Marketing Tech</li><li>Studio Engineering</li></ul><h3>What’s New</h3><h4>gRPC Framework</h4><p>One of the key features in v2.0 was the introduction of the gRPC framework as an alternative/auxiliary to REST. This was contributed by our counterparts at GitHub, thereby strengthening the value of community contributions to Conductor.</p><h4>Cassandra Persistence Layer</h4><p>To enable horizontal scaling of the datastore for large volume of concurrent workflow executions (millions of workflows/day), Cassandra was chosen to provide elastic scaling and meet throughput demands.</p><h4>External Payload Storage</h4><p>External payload storage was implemented to prevent the usage of Conductor as a data persistence system and to reduce the pressure on its backend datastore.</p><h4>Dynamic Workflow Executions</h4><p>For use cases where the need arises to execute a large/arbitrary number of varying workflow definitions or to run a one-time ad hoc workflow for testing or analytical purposes, registering definitions first with the metadata store in order to then execute them only once, adds a lot of additional overhead. The ability to dynamically create and execute workflows removes this friction. This was another great addition that stemmed from our collaboration with GitHub.</p><h4>Workflow Status Listener</h4><p>Conductor can be configured to publish notifications to external systems or queues upon completion/termination of workflows. The workflow status listener provides <a href="https://netflix.github.io/conductor/extend/#workflow-status-listener">hooks</a> to connect to any notification system of your choice. The community has contributed an <a href="https://github.com/Netflix/conductor/blob/master/contribs/src/main/java/com/netflix/conductor/contribs/listener/DynoQueueStatusPublisher.java">implementation</a> that publishes a message on a dyno queue based on the status of the workflow. An event handler can be configured on these queues to trigger workflows or tasks to perform specific actions upon the terminal state of the workflow.</p><h4>Bulk Workflow Management</h4><p>There has always been a need for bulk operations at the workflow level from an operability standpoint. When running at scale, it becomes essential to perform workflow level operations in bulk due to bad downstream dependencies in the worker processes causing task failures or bad task executions. Bulk APIs enable the operators to have macro-level control on the workflows executing within the system.</p><h4>Decoupling Elasticsearch from Persistence</h4><p>This inter-dependency was removed by moving the indexing layer into separate persistence modules, exposing a property (<em>workflow.elasticsearch.instanceType</em>) to choose the type of indexing engine. Further, the indexer and persistence layer have been decoupled by moving this orchestration from within the primary persistence layer to a service layer through the ExecutionDAOFacade.</p><h4>ES5/6 Support</h4><p>Support for Elasticsearch versions 5 and 6 have been added as part of the major version upgrade to v2.x. This addition also provides the option to use the Elasticsearch RestClient instead of the Transport Client which was enforced in the previous version. This opens the route to using a managed Elasticsearch cluster (a la AWS) as part of the Conductor deployment.</p><h4>Task Rate Limiting &amp; Concurrent Execution Limits</h4><p>Task rate limiting helps achieve bounded scheduling of tasks. The task definition parameter <em>rateLimitFrequencyInSeconds</em> sets the duration window, while <em>rateLimitPerFrequency</em> defines the number of tasks that can be scheduled in a duration window. On the other hand, <em>concurrentExecLimit</em> provides unbounded scheduling limits of tasks. I.e the total of current scheduled tasks at any given time will be under <em>concurrentExecLimit</em>. The above parameters can be used in tandem to achieve desired throttling and rate limiting.</p><h4>API Validations</h4><p>Validation was one of the core features missing in Conductor 1.x. To improve usability and operability, we added validations, which in practice has greatly helped find bugs during creation of workflow and task definitions. Validations enforce the user to create and register their task definitions before registering the workflow definitions using these tasks. It also ensures that the workflow definition is well-formed with correct wiring of inputs and outputs in the various tasks within the workflow. Any anomalies found are reported to the user with a detailed error message describing the reason for failure.</p><h4>Developer Labs, Logging and Metrics</h4><p>We have been continually improving logging and metrics, and revamped the documentation to reflect the latest state of Conductor. To provide a smooth on boarding experience, we have created developer labs, which guides the user through creating task and workflow definitions, managing a workflow lifecycle, configuring advanced workflows with eventing etc., and a brief introduction to Conductor API, UI and other modules.</p><h4>New Task Types</h4><p><a href="https://netflix.github.io/conductor/configuration/systask/">System tasks</a> have proven to be very valuable in defining the Workflow structure and control flow. As such, Conductor 2.x has seen several new additions to System tasks, mostly contributed by the community:</p><p><em>Lambda</em></p><p>Lambda Task executes ad-hoc logic at Workflow run-time, using the Nashorn Javascript evaluator engine. Instead of creating workers for simple evaluations, Lambda task enables the user to do this inline using simple Javascript expressions.</p><p><em>Terminate</em></p><p>Terminate task is useful when workflow logic should terminate with a given output. For example, if a decision task evaluates to false, and we do not want to execute remaining tasks in the workflow, instead of having a <em>DECISION</em> task with a list of tasks in one case and an empty list in the other, this can scope the decide and terminate workflow execution.</p><p><em>ExclusiveJoin</em></p><p>Exclusive Join task helps capture task output from a <em>DECISION</em> task’s flow. This is useful to wire task inputs from the outputs of one of the cases within a decision flow. This data will only be available during workflow execution time and the <em>ExclusiveJoin</em> task can be used to collect the output from one of the tasks in any of decision branches.</p><p>For in-depth implementation details of the new additions, please refer the <a href="https://netflix.github.io/conductor/technicaldetails/">documentation</a>.</p><h3>What’s next</h3><p>There are a lot of features and enhancements we would like to add to Conductor. The below wish list could be considered as a long-term road map. It is by no means exhaustive, and we are very much welcome to ideas and contributions from the community. Some of these listed in no particular order are:</p><h4><strong>Advanced Eventing with Event Aggregation and Distribution</strong></h4><p>At the moment, event generation and processing is a very simple implementation. An event task can create only one message, and a task can wait for only one event.</p><p>We envision an Event Aggregation and Distribution mechanism that would open up Conductor to a multitude of use-cases. A coarse idea is to allow a task to wait for multiple events, and to progress several tasks based on one event.</p><h4><strong>UI Improvements</strong></h4><p>While the current UI provides a neat way to visualize and track workflow executions, we would like to enhance this with features like:</p><ul><li>Creating metadata objects from UI</li><li>Support for starting workflows</li><li>Visualize execution metrics</li><li>Admin dashboard to show outliers</li></ul><h4><strong>New Task types like Goto, Loop etc.</strong></h4><p>Conductor has been using a Directed Acyclic Graph (DAG) structure to define a workflow. The Goto and Loop on tasks are valid use cases, which would deviate from the DAG structure. We would like to add support for these tasks without violating the existing workflow execution rules. This would help unlock several other use cases like streaming flow of data to tasks and others that require repeated execution of a set of tasks within a workflow.</p><h4><strong>Support for reusable commonly used tasks like Email, DatabaseQuery etc.</strong></h4><p>Similarly, we’ve seen the value of shared reusable tasks that does a specific thing. At Netflix internal deployment of Conductor, we’ve added tasks specific to services that users can leverage over recreating the tasks from scratch. For example, we provide a <em>TitusTask</em> which enables our users to launch a new <a href="https://netflix.github.io/titus/">Titus</a> container as part of their workflow execution.</p><p>We would like to extend this idea such that Conductor can offer a repository of commonly used tasks.</p><h4><strong>Push based task scheduling interface</strong></h4><p>Current Conductor architecture is based on polling from a worker to get tasks that it will execute. We need to enhance the grpc modules to leverage the bidirectional channel to push tasks to workers as and when they are scheduled, thus reducing network traffic, load on the server and redundant client calls.</p><h4><strong>Validating Task inputKeys and outputKeys</strong></h4><p>This is to provide type safety for tasks and define a parameterized interface for task definitions such that tasks are completely re-usable within Conductor once registered. This provides a contract allowing the user to browse through available task definitions to use as part of their workflow where the tasks could have been implemented by another team/user. This feature would also involve enhancing the UI to display this contract.</p><h4><strong>Implementing MetadataDAO in Cassandra</strong></h4><p>As mentioned <a href="https://github.com/Netflix/conductor/tree/master/cassandra-persistence#note">here</a>, Cassandra module provides a partial implementation for persisting only the workflow executions. Metadata persistence implementation is not available yet and is something we are looking to add soon.</p><h4><strong>Pluggable Notifications on Task completion</strong></h4><p>Similar to the <a href="https://netflix.github.io/conductor/configuration/workflowdef/#workflow-notifications">Workflow status listener</a>, we would like to provide extensible interfaces for notifications on task execution.</p><h4><strong>Python client in Pypi</strong></h4><p>We have seen wide adoption of Python client within the community. However, there is no official Python client in Pypi, and lacks some of the newer additions to the Java client. We would like to achieve feature parity and publish a client from Conductor Github repository, and automate the client release to Pypi.</p><h4><strong>Removing Elasticsearch from critical path</strong></h4><p>While Elasticsearch is greatly useful in Conductor, we would like to make this optional for users who do not have Elasticsearch set-up. This means removing Elasticsearch from the critical execution path of a workflow and using it as an opt-in layer.</p><h4><strong>Pluggable authentication and authorization</strong></h4><p>Conductor doesn’t support authentication and authorization for API or UI, and is something that we feel would add great value and is a frequent request in the community.</p><h4><strong>Validations and Testing</strong></h4><p><strong>Dry runs, </strong>i.e the ability to evaluate workflow definitions without actually running it through worker processes and all relevant set-up would make it much easier to test and debug execution paths.</p><p>If you would like to be a part of the Conductor community and contribute to one of the Wishlist items or something that you think would provide a great value add, please read through this <a href="https://github.com/Netflix/conductor/blob/master/CONTRIBUTING.md">guide</a> for instructions or feel free to start a conversation on our <a href="https://gitter.im/netflix-conductor/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge">Gitter</a> channel, which is Conductor’s user forum.</p><p>We also highly encourage to polish, genericize and share any customizations that you may have built on top of Conductor with the community.</p><p>We really appreciate and are extremely proud of the community involvement, who have made several important contributions to Conductor. We would like to take this further and make Conductor widely adopted with a strong community backing.</p><p>Netflix Conductor is maintained by the Media Workflow Infrastructure team. If you like the challenges of building distributed systems and are interested in building the Netflix Content and Studio ecosystem at scale, connect with <a href="https://www.linkedin.com/in/czhao/">Charles Zhao</a> to get the conversation started.</p><p><em>Thanks to Alexandra Pau, Charles Zhao, Falguni Jhaveri, Konstantinos Christidis and Senthil Sayeebaba.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=16600be36bca" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/evolution-of-netflix-conductor-16600be36bca">Evolution of Netflix Conductor:</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Re-Architecting the Video Gatekeeper]]></title>
            <link>https://medium.com/netflix-techblog/re-architecting-the-video-gatekeeper-f7b0ac2f6b00?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/f7b0ac2f6b00</guid>
            <category><![CDATA[caching]]></category>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[software-engineering]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 12 Jul 2019 13:01:01 GMT</pubDate>
            <atom:updated>2019-07-12T13:01:01.620Z</atom:updated>
            <content:encoded><![CDATA[<h4>By Drew Koszewnik</h4><p>This is the story about how the Content Setup Engineering team used Hollow, a Netflix OSS technology, to re-architect and simplify an essential component in our content pipeline — delivering a large amount of business value in the process.</p><h4>The Context</h4><p>Each movie and show on the Netflix service is carefully curated to ensure an optimal viewing experience. The team responsible for this curation is <em>Title Operations</em>. Title Operations will confirm, among other things:</p><ul><li>We are in compliance with the contracts — date ranges and places where we can show a video are set up correctly for each title</li><li>Video with captions, subtitles, and secondary audio “dub” assets are sourced, translated, and made available to the right populations around the world</li><li>Title name and synopsis are available and translated</li><li>The appropriate maturity ratings are available for each country</li></ul><p>When a title meets all of the minimum above requirements, then it is allowed to go live on the service. <em>Gatekeeper</em> is the system at Netflix responsible for evaluating the “liveness” of videos and assets on the site. A title doesn’t become visible to members until Gatekeeper approves it — and if it can’t validate the setup, then it will assist Title Operations by pointing out what’s missing from the baseline customer experience.</p><p>Gatekeeper accomplishes its prescribed task by aggregating data from multiple upstream systems, applying some business logic, then producing an output detailing the status of each video in each country.</p><h4>The Tech</h4><p><a href="http://hollow.how">Hollow</a>, an <a href="http://github.com/Netflix/hollow">OSS</a> technology we <a href="https://medium.com/netflix-techblog/netflixoss-announcing-hollow-5f710eefca4b">released</a> a few years ago, has been best described as a <em>total high-density near cache</em>:</p><ul><li><strong>Total</strong>: The entire dataset is cached on each node — there is no eviction policy, and there are no cache misses.</li><li><strong>High-Density</strong>: encoding, bit-packing, and deduplication techniques are employed to optimize the memory footprint of the dataset.</li><li><strong>Near</strong>: the cache exists in RAM on any instance which requires access to the dataset.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/478/0*KaxWTFTt3V41iXSF" /></figure><p>One exciting thing about the <em>total</em> nature of this technology — because we don’t have to worry about swapping records in-and-out of memory, we can make assumptions and do some precomputation of the in-memory representation of the dataset which would not otherwise be possible. The net result is, for many datasets, vastly more efficient use of RAM. Whereas with a traditional partial-cache solution you may wonder whether you can get away with caching only 5% of the dataset, or if you need to reserve enough space for 10% in order to get an acceptable hit/miss ratio — with the same amount of memory Hollow may be able to cache 100% of your dataset and achieve a 100% hit rate.</p><p>And obviously, if you get a 100% hit rate, you eliminate all I/O required to access your data — and can achieve orders of magnitude more efficient data access, which opens up many possibilities.</p><h4>The Status-Quo</h4><p>Until very recently, Gatekeeper was a completely event-driven system. When a change for a video occurred in any one of its upstream systems, that system would send an event to Gatekeeper. Gatekeeper would react to that event by reaching into each of its upstream services, gathering the necessary input data to evaluate the liveness of the video and its associated assets. It would then produce a single-record output detailing the status of that single video.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/689/0*tCC6kWANBMJMgFdk" /><figcaption>Old Gatekeeper Architecture</figcaption></figure><p>This model had several problems associated with it:</p><ul><li>This process was completely I/O bound and put a lot of load on upstream systems.</li><li>Consequently, these events would queue up throughout the day and cause processing delays, which meant that titles may not actually go live on time.</li><li>Worse, events would occasionally get missed, meaning titles wouldn’t go live <em>at all</em> until someone from Title Operations realized there was a problem.</li></ul><p>The mitigation for these issues was to “sweep” the catalog so Videos matching specific criteria (e.g., scheduled to launch next week) would get events automatically injected into the processing queue. Unfortunately, this mitigation added many more events into the queue, which exacerbated the problem.</p><p>Clearly, a change in direction was necessary.</p><h4>The Idea</h4><p>We decided to employ a <em>total high-density near cache</em> (i.e., Hollow) to eliminate our I/O bottlenecks. For each of our upstream systems, we would create a Hollow dataset which encompasses all of the data necessary for Gatekeeper to perform its evaluation. Each upstream system would now be responsible for keeping its cache updated.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/607/0*AND6Rc68Zp-k4bu6" /><figcaption>New Gatekeeper Architecture</figcaption></figure><p>With this model, liveness evaluation is conceptually separated from the data retrieval from upstream systems. Instead of reacting to events, Gatekeeper would <em>continuously</em> process liveness for all assets in all videos across all countries in a repeating cycle. The cycle iterates over every video available at Netflix, calculating liveness details for each of them. At the end of each cycle, it produces a complete output (also a Hollow dataset) representing the liveness status details of all videos in all countries.</p><p>We expected that this <em>continuous processing</em> model was possible because a complete removal of our I/O bottlenecks would mean that we should be able to operate orders of magnitude more efficiently. We also expected that by moving to this model, we would realize many positive effects for the business.</p><ul><li>A definitive solution for the excess load on upstream systems generated by Gatekeeper</li><li>A complete elimination of liveness processing delays and missed go-live dates.</li><li>A reduction in the time the Content Setup Engineering team spends on performance-related issues.</li><li>Improved debuggability and visibility into liveness processing.</li></ul><h4>The Problem</h4><p>Hollow can also be thought of like a time machine. As a dataset changes over time, it communicates those changes to consumers by breaking the timeline down into a series of discrete data <em>states</em>. Each data state represents a snapshot of the entire dataset at a specific moment in time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/554/0*ZkoW6RrX5f-GGlIf" /><figcaption>Hollow is like a time machine</figcaption></figure><p>Usually, consumers of a Hollow dataset are loading the latest data state and keeping their cache updated as new states are produced. However, they may instead point to a prior state — which will revert their view of the entire dataset to a point in the past.</p><p>The traditional method of producing data states is to maintain a single producer which runs a repeating <em>cycle</em>. During that <em>cycle</em>, the producer iterates over all records from the source of truth. As it iterates, it adds each record to the Hollow library. Hollow then calculates the differences between the data added during this cycle and the data added during the last cycle, then publishes the state to a location known to consumers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/617/0*Fq2ekb75p_GPv6Lb" /><figcaption>Traditional Hollow usage</figcaption></figure><p>The problem with this total-source-of-truth iteration model is that it can take a long time. In the case of some of our upstream systems, this could take hours. This data-propagation latency was unacceptable — we can’t wait hours for liveness processing if, for example, Title Operations adds a rating to a movie that needs to go live imminently.</p><h4>The Improvement</h4><p>What we needed was a faster time machine — one which could produce states with a more frequent cadence, so that changes could be more quickly realized by consumers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/555/0*lQ1R_pSHkk7l5JkK" /><figcaption>Incremental Hollow is like a faster time machine</figcaption></figure><p>To achieve this, we created an incremental Hollow infrastructure for Netflix, leveraging <a href="https://github.com/Netflix/hollow/pull/69">work</a> which had been done in the Hollow library earlier, and <a href="https://github.com/Netflix/hollow/pull/142">pioneered</a> in <a href="https://github.com/Netflix/hollow/pull/188">production</a> <a href="https://github.com/Netflix/hollow/pull/181">usage</a> by the Streaming Platform Team at Target (and is now a <a href="https://github.com/Netflix/hollow/pull/414">public non-beta API</a>).</p><p>With this infrastructure, each time a change is detected in a source application, the updated record is <a href="https://github.com/Netflix/hollow/pull/375">encoded</a> and emitted to a Kafka topic. A new component that is not part of the source application, the <em>Hollow Incremental Producer</em> service, performs a repeating cycle at a predefined cadence. During each cycle, it reads all messages which have been added to the topic since the last cycle and mutates the Hollow state engine to reflect the new state of the updated records.</p><p>If a message from the Kafka topic contains the exact same data as already reflected in the Hollow dataset, no action is taken.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/751/0*m2Dg_kSzZtp19aIN" /><figcaption>Hollow Incremental Producer Service</figcaption></figure><p>To mitigate issues arising from missed events, we implement a <em>sweep</em> mechanism that periodically iterates over an entire source dataset. As it iterates, it emits the content of each record to the Kafka topic. In this way, any updates which may have been missed will eventually be reflected in the Hollow dataset. Additionally, because this is not the primary mechanism by which updates are propagated to the Hollow dataset, this does not have to be run as quickly or frequently as a cycle must iterate the source in traditional Hollow usage.</p><p>The Hollow Incremental Producer is capable of reading a great many messages from the Kafka topic and mutating its Hollow state internally very quickly — so we can configure its cycle times to be very short (we are currently defaulting this to 30 seconds).</p><p>This is how we built a faster time machine. Now, if Title Operations adds a maturity rating to a movie, within 30 seconds, that data is available in the corresponding Hollow dataset.</p><h4>The Tangible Result</h4><p>With the data propagation latency issue solved, we were able to re-implement the Gatekeeper system to eliminate all I/O boundaries. With the prior implementation of Gatekeeper, re-evaluating all assets for all videos in all countries would have been unthinkable — it would tie up the entire content pipeline for more than a week (and we would then still be behind by a week since nothing else could be processed in the meantime). Now we re-evaluate everything in about 30 seconds — and we do that every minute.</p><p>There is no such thing as a missed or delayed liveness evaluation any longer, and the disablement of the prior Gatekeeper system reduced the load on our upstream systems — in some cases by up to 80%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HKPzBE_tMHd-gWEo" /><figcaption>Load reduction on one upstream system</figcaption></figure><p>In addition to these performance benefits, we also get a resiliency benefit. In the prior Gatekeeper system, if one of the upstream services went down, we were unable to evaluate liveness at all because we were unable to retrieve any data from that system. In the new implementation, if one of the upstream systems goes down then it does stop publishing — but we still gate <em>stale</em> data for its corresponding dataset while all others make progress. So for example, if the translated synopsis system goes down, we can still bring a movie on-site in a region if it was held back for, and then receives, the correct subtitles.</p><h4>The Intangible Result</h4><p>Perhaps even more beneficial than the performance gains has been the improvement in our development velocity in this system. We can now develop, validate, and release changes in minutes which might have before taken days or weeks — and we can do so with significantly increased release quality.</p><p>The time-machine aspect of Hollow means that every deterministic process which uses Hollow exclusively as input data is 100% reproducible. For Gatekeeper, this means that an exact replay of what happened at time X can be accomplished by reverting all of our input states to time X, then re-evaluating everything again.</p><p>We use this fact to iterate quickly on changes to the Gatekeeper business logic. We maintain a PREPROD Gatekeeper instance which “follows” our PROD Gatekeeper instance. PREPROD is also continuously evaluating liveness for the entire catalog, but publishing its output to a different Hollow dataset. At the beginning of each cycle, the PREPROD environment will gather the latest produced state from PROD, and set each of its input datasets to the exact same versions which were used to produce the PROD output.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/897/0*TyoJA01KCfat_6OY" /><figcaption>The PREPROD Gatekeeper instance “<em>follows”</em> the PROD instance</figcaption></figure><p>When we want to make a change to the Gatekeeper business logic, we do so and then publish it to our PREPROD cluster. The subsequent output state from PREPROD can be <a href="https://hollow.how/tooling/#diff-tool"><em>diffed</em></a> with its corresponding output state from PROD to view the precise effect that the logic change will cause. In this way, at a glance, we can validate that our changes have <em>precisely</em> the intended effect, and <em>zero</em> unintended consequences.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HR9Ea-8fKu05shFu" /><figcaption>A Hollow diff shows exactly what changes</figcaption></figure><p>This, coupled with some iteration on the deployment process, has resulted in the ability for our team to code, validate, and deploy impactful changes to Gatekeeper in literally minutes — at least an order of magnitude faster than in the prior system — and we can do so with a higher level of safety than was possible in the previous architecture.</p><h4>Conclusion</h4><p>This new implementation of the Gatekeeper system opens up opportunities to capture additional business value, which we plan to pursue over the coming quarters. Additionally, this is a pattern that can be replicated to other systems within the Content Engineering space and elsewhere at Netflix — already a couple of follow-up projects have been launched to formalize and capitalize on the benefits of this n-hollow-input, one-hollow-output architecture.</p><p>Content Setup Engineering is an exciting space right now, especially as we scale up our pipeline to produce more content with each passing quarter. We have many opportunities to solve real problems and provide massive value to the business — and to do so with a deep focus on computer science, using and often pioneering leading-edge technologies. If this kind of work sounds appealing to you, reach out to <a href="https://www.linkedin.com/in/ivanontiveros/">Ivan</a> to get the ball rolling.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f7b0ac2f6b00" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/re-architecting-the-video-gatekeeper-f7b0ac2f6b00">Re-Architecting the Video Gatekeeper</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bringing Rich Experiences to Memory-constrained TV Devices]]></title>
            <link>https://medium.com/netflix-techblog/bringing-rich-experiences-to-memory-constrained-tv-devices-6de771eabb16?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/6de771eabb16</guid>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[user-interface-design]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[graphics]]></category>
            <category><![CDATA[javascript-development]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 02 Jul 2019 00:55:11 GMT</pubDate>
            <atom:updated>2019-07-03T05:49:43.973Z</atom:updated>
            <content:encoded><![CDATA[<h3>Bringing Rich Experiences to Memory-Constrained TV Devices</h3><p>By Jason Munning, Archana Kumar, Kris Range</p><p>Netflix has over 148M paid members streaming on more than half a billion devices spanning over 1,900 different types. In the TV space alone, there are hundreds of device types that run the Netflix app. We need to support the same rich Netflix experience on not only high-end devices like the PS4 but also memory and processor-constrained consumer electronic devices that run a similar chipset as was used in an iPhone 3Gs.</p><p>In a <a href="https://medium.com/@Netflix_Techblog/building-the-new-netflix-experience-for-tv-920d71d875de">previous post</a>, we described how our TV application consists of a C++ SDK installed natively on the device, an updatable JavaScript user interface (UI) layer, and a custom rendering layer known as Gibbon. We ship the same UI to thousands of different devices in order to deliver a consistent user experience. As UI engineers we are excited about delivering creative and engaging experiences that help members choose the content they will love so we are always trying to push the limits of our UI.</p><p>In this post, we will discuss the development of the Rich Collection row and the iterations we went through to be able to support this experience across the majority of the TV ecosystem.</p><h3>Rich Collection Row</h3><p>One of our most ambitious UI projects to date on the TV app is the animated Rich Collection Row. The goal of this experience from a UX design perspective was to bring together a tightly-related set of original titles that, though distinct entities on their own, also share a connected universe. We hypothesized this design would net a far greater visual impact than if the titles were distributed individually throughout the page. We wanted the experience to feel less like scrolling through a row and more like exploring a connected world of stories.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*T2dIxWeZ4ui0BemEnf1DBg.png" /></figure><p>For the collections below, the row is composed of characters representing each title in a collected universe overlaid onto a shared, full-bleed background image which depicts the shared theme for the collection. When the user first scrolls down to the row, the characters are grouped into a lineup of four. The name of the collection animates in along with the logos for each title while a sound clip plays which evokes the mood of the shared world. The characters slide off screen to indicate the first title is selected. As the user scrolls horizontally, characters slide across the screen and the shared backdrop scrolls with a parallax effect. For some of the collections, the character images themselves animate and a full-screen tint is applied using a color that is representative of the show’s creative (see “Character Images” below).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*zGF60UjPeTcH7Q-HLLGJxQ.gif" /></figure><p>Once the user pauses on a title for more than two seconds, the trailer for that title cross-fades with the background image and begins playing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*c8AZd-c57DalD9rZ2TrdkQ.gif" /></figure><h3>Development</h3><p>As part of developing this type of UI experience on any platform, we knew we would need to think about creating smooth, performant animations with a balance between quality and download size for the images and video previews, all without degrading the performance of the app. Some of the <a href="https://medium.com/netflix-techblog/crafting-a-high-performance-tv-user-interface-using-react-3350e5a6ad3b">metrics we use to measure performance</a> on the Netflix TV app include animation frames per second (FPS), key input responsiveness (the amount of time before a member’s key press renders a change in the UI), video playback speed, and app start-up time.</p><p>UI developers on the Netflix TV app also need to consider some challenges that developers on other platforms often are able to take for granted. One such area is our graphics memory management. While web browsers and mobile phones have gigabytes of memory available for graphics, our devices are constrained to mere MBs. Our UI runs on top of a custom rendering engine which uses what we call a “surface cache” to optimize our use of graphics memory.</p><h3>Surface Cache</h3><p>Surface cache is a reserved pool in main memory (or separate graphics memory on a minority of systems) that the Netflix app uses for storing textures (decoded images and cached resources). This benefits performance as these resources do not need to be re-decoded on every frame, saving CPU time and giving us a higher frame-rate for animations.</p><p>Each device running the Netflix TV application has a limited surface cache pool available so the rendering engine tries to maximize the usage of the cache as much as possible. This is a positive for the end experience because it means more textures are ready for re-use as a customer navigates around the app.</p><p>The amount of space a texture requires in surface cache is calculated as:</p><p><strong>width * height * 4 bytes/pixel</strong> (for rgba)</p><p>Most devices currently run a 1280 x 720 Netflix UI. A full-screen image at this resolution will use 1280 * 720 * 4 = 3.5MB of surface cache. The majority of legacy devices run at 28MB of surface cache. At this size, you could fit the equivalent of 8 full-screen images in the cache. Reserving this amount of memory allows us to use transition effects between screens, layering/parallax effects, and to pre-render images for titles that are just outside the viewport to allow scrolling in any direction without images popping in. Devices in the Netflix TVUI ecosystem have a range of surface cache capacity, anywhere from 20MB to 96MB and we are able to enable/disable rich features based on that capacity.</p><p>When the limit of this memory pool is approached or exceeded, the Netflix TV app tries to free up space with resources it believes it can purge (i.e. images no longer in the viewport). If the cache is over budget with surfaces that cannot be purged, devices can behave in unpredictable ways ranging from application crashes, displaying garbage on the screen, or drastically slowing down animations.</p><h3>Surface Cache and the Rich Collection Row</h3><p>From developing previous rich UI features, we knew that surface cache usage was something to consider with the image-heavy design for the Rich Collection row. We made sure to test memory usage early on during manual testing and did not see any overages so we checked that box and proceeded with development. When we were approaching code-complete and preparing to roll out this experience to all users we ran our new code against our memory-usage automation suite as a sanity check.</p><p>The chart below shows an end-to-end automated test that navigates the Netflix app, triggering playbacks, searches, etc to simulate a user session. In this case, the test was measuring surface cache after every step. The red line shows a test run with the Rich Collection row and the yellow line shows a run without. The dotted red line is placed at 28MB which is the amount of memory reserved for surface cache on the test device.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7LPM1sS00AKRvpW5" /><figcaption>Automation run showing surface cache size vs test step</figcaption></figure><p>Uh oh! We found some massive peaks (marked in red) in surface cache that exceeded our maximum recommended surface cache usage of 28MB and indicated we had a problem. Exceeding the surface cache limit can have a variety of impacts (depending on the device implementation) to the user from missing images to out of memory crashes. Time to put the brakes on the rollout and debug!</p><h3>Assessing the Problem</h3><p>The first step in assessing the problem was to drill down into our automation results to make sure they were valid. We re-ran the automation tests and found the results were reproducible. We could see the peaks were happening on the home screen where the Rich Collection row was being displayed. It was odd that we hadn’t seen the surface cache over budget (SCOB) errors while doing manual testing.</p><p>To close the gap we took a look at the configuration settings we were using in our automation and adjusted them to match the settings we use in production for real devices. We then re-ran the automation and still saw the peaks but in the process we discovered that the issue seemed to only present itself on devices running a version of our SDK from 2015. The manual testing hadn’t caught it because we had only been manually testing surface cache on more recent versions of the SDK. Once we did manual testing on our older SDK version we were able to reproduce the issue in our development environment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E0A2zowLJlZpXRpD" /><figcaption>An example console output showing surface cache over budget errors</figcaption></figure><p>During brainstorming with our platform team, we came across an internal bug report from 2017 that described a similar issue to what we were seeing — surfaces that were marked as purgeable in the surface cache were not being fully purged in this older version of our SDK. From the ticket we could see that the inefficiency was fixed in the next release of our SDK but, because not all devices get Netflix SDK updates, the fix could not be back-ported to the 2015 version that had this issue. Considering that a significant share of our actively-used TV devices are running this 2015 version and won’t be updated to a newer SDK, we knew we needed to find a fix that would work for this specific version — a similar situation to the pre-2000 world before browsers auto-updated and developers had to code to specific browser versions.</p><h3>Finding a Solution</h3><p>The first step was to take a look at what textures were in the surface cache (especially those marked as un-purgeable) at the time of the overage and see where we might be able to make gains by reducing the size of images. For this we have a debug port that allows us to inspect which images are in the cache. This shows us information about the images in the surface cache including url. The links can then be hovered over to show a small thumbnail of the image.</p><p>From snapshots such as this one we could see the Rich Collection row alone filled about 15.3MB of surface cache which is &gt;50% of the 28MB total graphics memory available on devices running our 2015 SDK.</p><p>The largest un-purgeable images we found were:</p><ul><li>Character images (6 * 1MB)</li><li>Background images for the parallax background (2 * 2.9MB)</li><li>Unknown — a full screen blank white rectangle (3.5MB)</li></ul><h3>Character Images</h3><p>Some of our rich collections featured the use of animated character assets to give an even richer experience. We created these assets using a Netflix-proprietary animation format called a Scriptable Network Graphic (SNG) which was first supported in 2017 and is similar to an animated PNG. The SNG files have a relatively large download size at ~1.5MB each. In order to ensure these assets are available at the time the rich collection row enters the viewport, we preload the SNGs during app startup and save them to disk. If the user relaunches the app in the future and receives the same collection row, the SNG files can be read from the disk cache, avoiding the need to download them again. Devices running an older version of the SDK fallback to a static character image.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Vs27fB6-DxPCsGX0UxsDvQ.gif" /><figcaption>Marvel Collection row with animated character images</figcaption></figure><p>At the time of the overage we found that<strong> </strong>six character images were present in the cache — four on the screen and two preloaded off of the screen. Our first savings came from only preloading one image for a total of five characters in the cache. Right off the bat this saved us almost 7% in surface cache with no observable impact to the experience.</p><p>Next we created cropped versions of the static character images that did away with extra transparent pixels (that still count toward surface cache usage!). This required modifications to the image pipeline in order to trim the whitespace but still maintain the relative size of the characters — so the relative heights of the characters in the lineup would still be preserved. The cropped character assets used only half of the surface cache memory of the full-size images and again had no visible impact to the experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/358/1*qqjMn_0S4SxSz822Gyv2cA.png" /><figcaption>Full-size vs cropped character image</figcaption></figure><h3>Parallax Background</h3><p>In order to achieve the illusion of a continuously scrolling parallax background, we were using two full screen background images essentially placed side by side which together accounted for ~38% of the experience’s surface cache usage. We worked with design to create a new full-screen background image that could be used for a fallback experience (without parallax) on devices that couldn’t support loading both of the background images for the parallax effect. Using only one background image saved us 19% in surface cache for the fallback experience.</p><h3>Unknown Widget</h3><p>After trial and error removing React components from our local build and inspecting the surface cache we found that the unknown widget that showed as a full screen blank white rectangle in our debug tool was added by the full-screen tint effect we were using. In order to apply the tint, the graphics layer essentially creates a full screen texture that is colored dynamically and overlaid over the visible viewport. Removing the tint overlay saved us 23% in surface cache.</p><p>Removing the tint overlay and using a single background image gave us a fallback experience that used 42% less surface cache than the full experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*7xENs8Yk_ZvGyG7VJc-2Og.gif" /><figcaption>Marvel Collection row fallback experience with static characters, no full-screen tint, and single background</figcaption></figure><p>When all was said and done, the surface cache usage of the fallback experience (including fewer preloaded characters, cropped character images, a single background, and no tint overlay) clocked in at about 5MB which gave us a total savings of almost 67% over our initial implementation.</p><p>We were able to target this fallback experience to devices running the 2015 and older SDK, while still serving the full rich experience (23% lower surface cache usage than the original implementation) to devices running the new SDKs.</p><h3>Rollout</h3><p>At this point our automation was passing so we began slowly rolling out this experience to all members. As part of any rollout, we have a dashboard of near real-time metrics that we monitor. To our chagrin we saw that another class of devices — those running the 2017 SDK — also were reporting higher SCOB errors than the control.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/403/1*nM0L1GFF9PgI9nmUYXcHAw.png" /><figcaption>Total number of SCOB errors vs time</figcaption></figure><p>Thanks to our work on the fallback experience we were able to change the configuration for this class of devices on the fly to serve the fallback experience (without parallax background and tint). We found if we used the fallback experience we could still get away with using the animated characters. So yet another flavor of the experience was born.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*qqhVy8TJakRBDe9BmLa-0A.gif" /></figure><h3>Improvements and Takeaways</h3><p>At Netflix we strive to move fast in innovation and learn from all projects whether they are successes or failures. From this project, we learned that there were gaps in our understanding of how our underlying graphics memory worked and in the tooling we used to monitor that memory. We kicked off an effort to understand this graphics memory space at a low level and compiled a set of best practices for developers beginning work on a project. We also documented a set of tips and tools for debugging and optimizing surface cache should a problem arise.</p><p>As part of that effort, we expanded our suite of build-over-build automated tests to increase coverage across our different SDK versions on real and reference devices to detect spikes/regressions in our surface cache usage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BdyTuAEQOR_jdLV9" /><figcaption>Surface cache usage per build</figcaption></figure><p>We began logging SCOB errors with more detail in production so we can target the specific areas of the app that we need to optimize. We also are now surfacing surface cache errors as notifications in the dev environment so developers can catch them sooner.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v6yPXYggEg2tY_unpRkx5g.gif" /></figure><p>And we improved our surface cache inspector tool to be more user friendly and to integrate with our Chrome DevTools debugger:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yuPGBYz5gejEOZcK" /><figcaption>New internal tool for debugging surface cache</figcaption></figure><h3>Conclusion</h3><p>As UI engineers on the TVUI platform at Netflix, we have the challenge of delivering ambitious UI experiences to a highly fragmented ecosystem of devices with a wide range of performance characteristics. It’s important for us to reach as many devices as possible in order to give our members the best possible experience.</p><p>The solutions we developed while scaling the Rich Collection row have helped inform how we approach ambitious UI projects going forward. With our optimizations and fallback experiences we were able to almost double the number of devices that were able to get the Rich Collection row.</p><p>We are now more thoughtful about designing fallback experiences that degrade gracefully as part of the initial design phase instead of just as a reaction to problems we encounter in the development phase. This puts us in a position of being able to scale an experience very quickly with a set of knobs and levers that can be used to tune an experience for a specific class of devices.</p><p>Most importantly, we received feedback that our members enjoyed our Rich Collection row experience — both the full and fallback experiences — when we rolled them out globally at the end of 2018.</p><p>If this interests you and want to help build the future UIs for discovering and watching shows and movies, <a href="https://jobs.netflix.com/jobs/866978">join our team</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6de771eabb16" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/bringing-rich-experiences-to-memory-constrained-tv-devices-6de771eabb16">Bringing Rich Experiences to Memory-constrained TV Devices</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netflix Studio Hack Day — May 2019]]></title>
            <link>https://medium.com/netflix-techblog/netflix-studio-hack-day-may-2019-b4a0ecc629eb?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/b4a0ecc629eb</guid>
            <category><![CDATA[meetings]]></category>
            <category><![CDATA[computer-vision]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[hackathons]]></category>
            <category><![CDATA[voltron]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 20 Jun 2019 16:00:14 GMT</pubDate>
            <atom:updated>2019-06-25T13:12:42.263Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>Netflix Studio Hack Day</strong> — May 2019</h3><p><em>By </em><a href="https://www.linkedin.com/in/tomrichards"><em>Tom Richards</em></a><em>, </em><a href="https://twitter.com/careninam"><em>Carenina Garcia Motion</em></a><em>, and </em><a href="https://www.linkedin.com/in/marlee-tart-40b66417/"><em>Marlee Tart</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D1fC_i7VXd2PxbZ63soJGw.jpeg" /></figure><p>Hack Days are a big deal at Netflix. They’re a chance to bring together employees from all our different disciplines to explore new ideas and experiment with emerging technologies.</p><p>For the most recent hack day, we channeled our creative energy towards our studio efforts. The goal remained the same: team up with new colleagues and have fun while learning, creating, and experimenting. We know even the silliest idea can spur something more.</p><p>The most important value of hack days is that they support a culture of innovation. We believe in this work, even if it never ships, and love to share the creativity and thought put into these ideas.</p><p>Below, you can find videos made by the hackers of some of our favorite hacks from this event.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fl5LH2EDx7Xg%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dl5LH2EDx7Xg&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fl5LH2EDx7Xg%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/539c4997f0f7421c2d9b126815fae1d5/href">https://medium.com/media/539c4997f0f7421c2d9b126815fae1d5/href</a></iframe><h3>Project Rumble Pak</h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F_GcsNesbLBM%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D_GcsNesbLBM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F_GcsNesbLBM%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0e0921428e83f6bfc3d30dcedbbb1341/href">https://medium.com/media/0e0921428e83f6bfc3d30dcedbbb1341/href</a></iframe><p>You’re watching your favorite episode of Voltron when, after a suspenseful pause, there’s a huge explosion — and your phone starts to vibrate in your hands.</p><p>The Project Rumble Pak hack day project explores how haptics can enhance the content you’re watching. With every explosion, sword clank, and laser blast, you get force feedback to amp up the excitement.</p><p>For this project, we synchronized Netflix content with haptic effects using Immersion Corporation technology.</p><p>By <a href="http://twitter.com/verbiate">Hans van de Bruggen</a> and <a href="http://twitter.com/edbarker">Ed Barker</a></p><h3>The Voice of Netflix</h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FLONN8z2cHbc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DLONN8z2cHbc&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FLONN8z2cHbc%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/a4f401d91a205f55946daf700d3d5da3/href">https://medium.com/media/a4f401d91a205f55946daf700d3d5da3/href</a></iframe><p>Introducing The Voice of Netflix. We trained a neural net to spot words in Netflix content and reassemble them into new sentences on demand. For our stage demonstration, we hooked this up to a speech recognition engine to respond to our verbal questions in the voice of Netflix’s favorite characters. Try it out yourself at <a href="http://blogofsomeguy.com/v">blogofsomeguy.com/v</a>!</p><p>By <a href="https://twitter.com/guycirino">Guy Cirino</a> and <a href="http://@careninam">Carenina Garcia Motion</a></p><h3>TerraVision</h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fj5j4YMqNdoA%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dj5j4YMqNdoA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fj5j4YMqNdoA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0152080c3c764ac56d65d932dbc730f3/href">https://medium.com/media/0152080c3c764ac56d65d932dbc730f3/href</a></iframe><p>TerraVision re-envisions the creative process and revolutionizes the way our filmmakers can search and discover filming locations. Filmmakers can drop a photo of a look they like into an interface and find the closest visual matches from our centralized library of locations photos. We are using a computer vision model trained to recognize places to build reverse image search functionality. The model converts each image into a small dimensional vector, and the matches are obtained by computing the nearest neighbors of the query.</p><p><em>By </em><a href="https://www.linkedin.com/in/noessa/"><em>Noessa Higa</em></a><em>, </em><a href="https://www.linkedin.com/in/benjamin-klein-usa/"><em>Ben Klein</em></a><em>, </em><a href="https://www.linkedin.com/in/jonhyh/"><em>Jonathan Huang</em></a><em>, </em><a href="https://twitter.com/TylerChilds"><em>Tyler Childs</em></a><em>, </em><a href="https://www.linkedin.com/in/tzhong/"><em>Tie Zhong</em></a><em>, and </em><a href="https://twitter.com/kennahasson"><em>Kenna Hasson</em></a></p><h3>Get Out!</h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FVvtHPAc8T2g%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVvtHPAc8T2g&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVvtHPAc8T2g%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/e058129ee45a88b18280294e08a69863/href">https://medium.com/media/e058129ee45a88b18280294e08a69863/href</a></iframe><p>Have you ever found yourself needing to give the Evil Eye™ to colleagues who are hogging your conference room after their meeting has ended?</p><p>Our hack is a simple web application that allows employees to select a Netflix meeting room anywhere in the world, and press a button to kick people out of their meeting room if they have overstayed their meeting. First, the app looks up calendar events associated with the room and finds the latest meeting in the room that should have already ended. It then automatically calls in to that meeting and plays walk-off music similar to the Oscar’s to not-so-subtly encourage your colleagues to Get Out! We built this hack using Java (Springboot framework), the Google OAuth and Calendar APIs (for finding rooms) and Twilio API (for calling into the meeting), and deployed it on AWS.</p><p><em>By </em><a href="https://www.linkedin.com/in/abi-seshadri/"><em>Abi Seshadri</em></a><em> and </em><a href="https://www.linkedin.com/in/rachelrivera1/"><em>Rachel Rivera</em></a></p><p>You can also check out highlights from our past events: <a href="https://medium.com/netflix-techblog/netflix-hack-day-fall-2018-c05dda4b98c1">November 2018</a>, <a href="https://medium.com/netflix-techblog/netflix-hack-day-winter-2018-b36ee09699d6">March 2018</a>, <a href="https://medium.com/netflix-techblog/netflix-hack-day-summer-2017-ef3ba81a8a77">August 2017</a>, <a href="https://medium.com/netflix-techblog/netflix-hack-day-winter-2017-73590a2fe513">January 2017</a>, <a href="http://techblog.netflix.com/2016/05/netflix-hack-day-spring-2016.html">May 2016</a><a href="http://techblog.netflix.com/2015/11/netflix-hack-day-autumn-2015.html">, November 2015</a>,<a href="http://techblog.netflix.com/2015/03/netflix-hack-day-winter-2015.html"> March 2015</a>,<a href="http://techblog.netflix.com/2014/02/netflix-hack-day.html"> February 2014</a> &amp;<a href="http://techblog.netflix.com/2014/08/netflix-hack-day-summer-2014.html"> August 2014</a>.</p><p>Thanks to all the teams who put together a great round of hacks in 24 hours.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b4a0ecc629eb" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/netflix-studio-hack-day-may-2019-b4a0ecc629eb">Netflix Studio Hack Day — May 2019</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lerner — using RL agents for test case scheduling]]></title>
            <link>https://medium.com/netflix-techblog/lerner-using-rl-agents-for-test-case-scheduling-3e0686211198?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/3e0686211198</guid>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 22 May 2019 06:38:10 GMT</pubDate>
            <atom:updated>2019-05-22T06:38:10.356Z</atom:updated>
            <content:encoded><![CDATA[<h3>Lerner — using RL agents for test case scheduling</h3><p>By: <a href="https://www.linkedin.com/in/skirdey/">Stanislav Kirdey</a>, <a href="https://www.linkedin.com/in/kevincureton/">Kevin Cureton</a>, <a href="https://www.linkedin.com/in/scottsrick/">Scott Rick</a>, <a href="https://www.linkedin.com/in/rsankar/">Sankar Ramanathan</a></p><h3>Introduction</h3><p>Netflix brings delightful customer experiences to homes on a <a href="https://www.linkedin.com/pulse/team-behind-netflix-onanydevice-christophe-jouin/">variety of devices</a> that continues to grow each day. The device ecosystem is rich with partners ranging from Silicon-on-Chip (SoC) manufacturers, Original Design Manufacturer (ODM) and Original Equipment Manufacturer (OEM) vendors.</p><p>Partners across the globe leverage Netflix device certification process on a continual basis to ensure that quality products and experiences are delivered to their customers. The certification process involves the verification of partner’s implementation of features provided by the Netflix SDK.</p><p>The Partner Device Ecosystem organization in Netflix is responsible for ensuring successful integration and testing of the Netflix application on all partner devices. Netflix engineers run a series of tests and benchmarks to validate the device across multiple dimensions including compatibility of the device with the Netflix SDK, device performance, audio-video playback quality, license handling, encryption and security. All this leads to a plethora of test cases, most of them automated, that need to be executed to validate the functionality of a device running Netflix.</p><h3>Problem</h3><p>With a collection of tests that, by nature, are time consuming to run and sometimes require manual intervention, we need to prioritize and schedule test executions in a way that will expedite detection of test failures. There are several problems efficient test scheduling could help us solve:</p><ol><li>Quickly detect a regression in the integration of the Netflix SDK on a consumer electronic or MVPD (multichannel video programming distributor) device.</li><li>Detect a regression in a test case. Using the Netflix Reference Application and known good devices, ensure the test case continues to function and tests what is expected.</li><li>When code many test cases are dependent on has changed, choose the right test cases among thousands of affected tests to quickly validate the change before committing it and running extensive, and expensive, tests.</li><li>Choose the most promising subset of tests out of thousands of test cases available when running continuous integration against a device.</li><li>Recommend a set of test cases to execute against the device that would increase the probability of failing the device in real-time.</li></ol><p>Solving the above problems could help Netflix and our Partners save time and money during the entire lifecycle of device design, build, test, and certification.</p><p>These problems could be solved in several different ways. In our quest to be objective, scientific, and inline with the Netflix philosophy of using data to drive solutions for intriguing problems, we proceeded by leveraging machine learning.</p><p>Our inspiration was the findings in a research paper “<a href="https://arxiv.org/abs/1811.04122">Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration</a>” by Helge Spieker, et. al. We thought that reinforcement learning would be a promising approach that could provide great flexibility in the training process. Likewise it has very low requirements on the initial amount of training data.</p><p>In the case of continuously testing a Netflix SDK integration on a new device, we usually lack relevant data for model training in the early phases of integration. In this situation training an agent is a great fit as it allows us to start with very little input data and let the agent explore and exploit the patterns it learns in the process of SDK integration and regression testing. The agent in reinforcement learning is an entity that performs a decision on what action to take considering the current state of the environment, and gets a reward based on the quality of the action.</p><h3>Solution</h3><p>We built a system called Lerner that consists of a set of microservices and a python library that allows scalable agent training and inference for test case scheduling. We also provide an API client in Python.</p><p>Lerner works in tandem with our continuous integration framework that executes on-device tests using the <a href="https://medium.com/netflix-techblog/nts-real-time-streaming-for-test-automation-7cb000e933a1">Netflix Test Studio</a> platform. Tests are run on Netflix Reference Applications (running as containers on <a href="https://netflix.github.io/titus/">Titus</a>), as well as on physical devices.</p><p>There were several motivations that led to building a custom solution:</p><ol><li>We wanted to keep the APIs and integrations as simple as possible.</li><li>We needed a way to run agents and tie the runs to the internal infrastructure for analytics, reporting, and visualizations.</li><li>We wanted the to tool be available as a standalone library as well as scalable API service.</li></ol><p>Lerner provides ability to setup any number of agents making it the first component in our re-usable reinforcement learning framework for device certification.</p><p>Lerner, as a web-service, relies on Amazon Web Services (AWS) and Netflix’s Open Source Software (OSS) tools. We use <a href="https://medium.com/netflix-techblog/spinnaker-sets-sail-to-the-continuous-delivery-foundation-e81cd2cbbfeb">Spinnaker</a> to deploy instances and host the API containers on <a href="https://netflix.github.io/titus/">Titus</a> — which allows fast deployment times and rapid scalability. Lerner uses AWS services to store binary versions of the agents, agent configurations, and training data. To maintain the quality of Lerner APIs, we are using the <a href="https://en.m.wikipedia.org/wiki/Serverless_computing">server-less paradigm</a> for Lerner’s own integration testing by utilizing <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>.</p><p>The agent training library is written in Python and supports versions 2.7, 3.5, 3.6, and 3.7. The library is available in the artifactory repository for easy installation. It can be used in <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">Python notebooks</a> — allowing for rapid experimentation in isolated environments without a need to perform API calls. The agent training library exposes different types of learning agents that utilize neural networks to approximate action.</p><p>The neural network (NN)-based agent uses a deep net with fully connected layers. The NN gets the state of a particular test case (the input) and outputs a continuous value, where a higher number means an earlier position in a test execution schedule. The inputs to the neural network include: general historical features such as the last N executions and several domain specific features that provide meta-information about a test case.</p><p>The Lerner APIs are split into three areas:</p><ol><li>Storing execution results.</li><li>Getting recommendations based on the current state of the environment.</li><li>Assign reward to the agent based on the execution result and predicted recommendations.</li></ol><p>A process of getting recommendations and rewarding the agent using APIs consists of 4 steps:</p><ol><li>Out of all available test cases for a particular job — form a request that can be interpreted by Lerner. This involves aggregation of historical results and additional features.</li><li>Lerner returns a recommendation identified with a unique episode id.</li><li>A CI system can execute the recommendation and submit the execution results to Lerner based on the episode id.</li><li>Call an API to assign a reward based on the agent id and episode id.</li></ol><p>Below is a diagram of the services and persistence layers that support the functionality of the Lerner API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/501/0*Il2QJb_9TB8jaanO" /></figure><p>The self-service nature of the tool makes it easy for service owners to integrate with Lerner, create agents, ask agents for recommendations and reward them after execution results are available.</p><p>The metrics relevant to the training and recommendation process are reported to <a href="https://github.com/Netflix/atlas/wiki">Atlas</a> and visualized using Netflix’s <a href="https://medium.com/netflix-techblog/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a>. Users of the service can track the statistics specific to the agents they setup and deploy, which allows them to build their own dashboards.</p><p>We have identified some interesting patterns while doing online reinforcement learning.</p><ul><li>The recommendation/execution reward cycle can happen without any prior training data.</li><li>We can bootstrap several CI jobs that would use agents with different reward functions, and gain additional insight based on agents performance. It could help us design and implement more targeted reward functions.</li><li>We can keep a small amount of historical data to train agents. The data can be truncated after each execution and offloaded to a long-term storage for further analysis.</li></ul><p>Some of the downsides:</p><ul><li>It might take time for an agent to stop exploring and start exploiting the accumulated experience.</li><li>As agents stored in a binary format in the database, an update of an agent from multiple jobs could cause a race condition in its state. Handling concurrency in the training process is cumbersome and requires trade offs. We achieved the desired state by relying on the locking mechanisms of the underlying persistence layer that stores and serves agent binaries.</li></ul><p>Thus, we have the luxury of training as many agents as we want that could prioritize and recommend test cases based on their unique learning experiences.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/375/0*0YI1aeFxCj3pnJWh" /></figure><h3>Outcome</h3><p>We are currently piloting the system and have live agents serving predictions for various CI runs. At the moment we run Lerner-based CIs in parallel with CIs that either execute test cases in random order or use simple heuristics as sorting test cases by time and execute everything that previously failed.</p><p>The system was built with simplicity and performance in mind, so the set of APIs are minimal. We developed client libraries that allow seamless, but opinionated, integration with Lerner.</p><p>We collect several metrics to evaluate the performance of a recommendation, with main metrics being time taken to first failure and time taken to complete a whole scheduled run.</p><p>Lerner-based recommendations are proving to be different and more insightful than random runs, as they allow us to fit a particular time budget and detect patterns such as cases that tend to fail together in a cluster, cases that haven’t been run in a long time, and so on.</p><p>The below graphs shows more or less an artificial case when a schedule of 100+ test cases would contain several flaky tests. The Y-axis represents how many minutes it took to complete the schedule or reach a first failed test case. In blue, we have random recommendations with no time budget constraints. In green you can see executions based on Lerner recommendations under a time constraint of 60 minutes. The green spikes represent Lerner exploring the environment, where the wiggly lines around 0 are the executions that failed quickly as Lerner was exploiting its policy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LDMoSTMOZylwOPNP" /><figcaption>Execution of schedules that were randomly generated. Y-axis represents time to finish execution or reach first failure.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GKzsliE1w9-xG7Bk" /><figcaption>Execution of Lerner based schedules. You can see moments when Lerner was exploring the environment, and the wiggly lines represent when the schedule was generated based on exploiting existing knowledge.</figcaption></figure><h3>Next Steps</h3><p>The next phases of the project will focus on:</p><ul><li>Reward functions that are aware of a comprehensive domain context, such as assigning appropriate rewards to states where infrastructure is fragile and test case could not be run appropriately.</li><li>Administrative user-interface to manage agents.</li><li>More generic, simple, and user-friendly framework for reinforcement learning and agent deployment.</li><li>Using Lerner on all available CIs jobs against all SDK versions.</li><li>Experiment with different neural network architectures.</li></ul><p><strong>If you would like to be a part of our team, come </strong><a href="https://jobs.netflix.com/jobs/f412a426-3907-44b6-9f1e-0b802b235ae8"><strong>join us.</strong></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3e0686211198" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/lerner-using-rl-agents-for-test-case-scheduling-3e0686211198">Lerner — using RL agents for test case scheduling</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Predictive CPU isolation of containers at Netflix]]></title>
            <link>https://medium.com/netflix-techblog/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/91f014d856c7</guid>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[optimization]]></category>
            <category><![CDATA[containers]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[linux]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 04 Jun 2019 16:39:43 GMT</pubDate>
            <atom:updated>2019-06-04T16:39:43.534Z</atom:updated>
            <content:encoded><![CDATA[<h4>By Benoit Rostykus, Gabriel Hartmann</h4><h3>Noisy Neighbors</h3><p>We’ve all had noisy neighbors at one point in our life. Whether it’s at a cafe or through a wall of an apartment, it is always disruptive. The need for good manners in shared spaces turns out to be important not just for people, but for your Docker containers too.</p><p>When you’re running in the cloud your containers are in a shared space; in particular they share the CPU’s memory hierarchy of the host instance.</p><p>Because microprocessors are so fast, computer architecture design has evolved towards adding various levels of caching between compute units and the main memory, in order to hide the latency of bringing the bits to the brains. However, the key insight here is that these caches are partially shared among the CPUs, which means that perfect performance isolation of co-hosted containers is not possible. If the container running on the core next to your container suddenly decides to fetch a lot of data from the RAM, it will inevitably result in more cache misses for you (and hence a potential performance degradation).</p><h3>Linux to the rescue?</h3><p>Traditionally it has been the responsibility of the operating system’s task scheduler to mitigate this performance isolation problem. In Linux, the current mainstream solution is CFS (Completely Fair Scheduler). Its goal is to assign running processes to time slices of the CPU in a “fair” way.</p><p>CFS is widely used and therefore well tested and Linux machines around the world run with reasonable performance. So why mess with it? As it turns out, for the large majority of Netflix use cases, its performance is far from optimal. <a href="https://netflix.github.io/titus/">Titus</a> is Netflix’s container platform. Every month, we run millions of containers on thousands of machines on Titus, serving hundreds of internal applications and customers. These applications range from critical low-latency services powering our customer-facing video streaming service, to batch jobs for encoding or machine learning. Maintaining performance isolation between these different applications is critical to ensuring a good experience for internal and external customers.</p><p>We were able to meaningfully improve both the predictability and performance of these containers by taking some of the CPU isolation responsibility away from the operating system and moving towards a data driven solution involving combinatorial optimization and machine learning.</p><h3>The idea</h3><p>CFS operates by very frequently (every few microseconds) applying a set of heuristics which encapsulate a general concept of best practices around CPU hardware use.</p><p>Instead, what if we reduced the frequency of interventions (to every few seconds) but made better data-driven decisions regarding the allocation of processes to compute resources in order to minimize collocation noise?</p><p>One traditional way of mitigating CFS performance issues is for application owners to manually cooperate through the use of core pinning or nice values. However, we can automatically make better global decisions by detecting collocation opportunities based on actual usage information. For example if we predict that container A is going to become very CPU intensive soon, then maybe we should run it on a different <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> socket than container B which is very latency-sensitive. This avoids thrashing caches too much for B and evens out the pressure on the <a href="https://en.wikipedia.org/wiki/Memory_hierarchy">L3</a> caches of the machine.</p><h3>Optimizing placements through combinatorial optimization</h3><p>What the OS task scheduler is doing is essentially solving a resource allocation problem: I have X threads to run but only Y CPUs available, how do I allocate the threads to the CPUs to give the illusion of concurrency?</p><p>As an illustrative example, let’s consider a toy instance of 16 <a href="https://en.wikipedia.org/wiki/Hyper-threading">hyperthreads</a>. It has 8 physical hyperthreaded cores, split on 2 NUMA sockets. Each hyperthread shares its L1 and L2 caches with its neighbor, and shares its L3 cache with the 7 other hyperthreads on the socket:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*u2PbqBiv2TSQUvgBbsSGzg.png" /></figure><p>If we want to run container A on 4 threads and container B on 2 threads on this instance, we can look at what “bad” and “good” placement decisions look like:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/737/1*8zsLvMK0n_Cw2pzauiNf1A.png" /></figure><p>The first placement is intuitively bad because we potentially create collocation noise between A and B on the first 2 cores through their L1/L2 caches, and on the socket through the L3 cache while leaving a whole socket empty. The second placement looks better as each CPU is given its own L1/L2 caches, and we make better use of the two L3 caches available.</p><p>Resource allocation problems can be efficiently solved through a branch of mathematics called combinatorial optimization, used for example for airline scheduling or logistics problems.</p><p>We formulate the problem as a Mixed Integer Program (MIP). Given a set of K containers each requesting a specific number of CPUs on an instance possessing d threads, the goal is to find a binary assignment matrix M of size (d, K) such that each container gets the number of CPUs it requested. The loss function and constraints contain various terms expressing a priori good placement decisions such as:</p><ul><li>avoid spreading a container across multiple NUMA sockets (to avoid potentially slow cross-sockets memory accesses or page migrations)</li><li>don’t use hyper-threads unless you need to (to reduce L1/L2 thrashing)</li><li>try to even out pressure on the L3 caches (based on potential measurements of the container’s hardware usage)</li><li>don’t shuffle things too much between placement decisions</li></ul><p>Given the low-latency and low-compute requirements of the system (we certainly don’t want to spend too many CPU cycles figuring out how containers should use CPU cycles!), can we actually make this work in practice?</p><h3>Implementation</h3><p>We decided to implement the strategy through Linux <a href="http://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups</a> since they are fully supported by CFS, by modifying each container’s cpuset cgroup based on the desired mapping of containers to hyper-threads. In this way a user-space process defines a “fence” within which CFS operates for each container. In effect we remove the impact of CFS heuristics on performance isolation while retaining its core scheduling capabilities.</p><p>This user-space process is a Titus subsystem called <a href="https://github.com/Netflix-Skunkworks/titus-isolate"><strong>titus-isolate</strong></a> which works as follows. On each instance, we define three events that trigger a placement optimization:</p><ul><li><strong><em>add</em></strong>: A new container was allocated by the Titus scheduler to this instance and needs to be run</li><li><strong><em>remove</em></strong>: A running container just finished</li><li><strong><em>rebalance</em></strong>: CPU usage may have changed in the containers so we should reevaluate our placement decisions</li></ul><p>We periodically enqueue rebalance events when no other event has recently triggered a placement decision.</p><p>Every time a placement event is triggered, <strong>titus-isolate</strong> queries a remote optimization service (running as a Titus service, hence also isolating itself… <a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">turtles all the way down</a>) which solves the container-to-threads placement problem.</p><p>This service then queries a local <a href="https://en.wikipedia.org/wiki/Gradient_boosting">GBRT</a> model (retrained every couple of hours on weeks of data collected from the whole Titus platform) predicting the P95 CPU usage of each container in the coming 10 minutes (conditional quantile regression). The model contains both contextual features (metadata associated with the container: who launched it, image, memory and network configuration, app name…) as well as time-series features extracted from the last hour of historical CPU usage of the container collected regularly by the host from the kernel <a href="https://www.kernel.org/doc/Documentation/cgroup-v1/cpuacct.txt">CPU accounting controller</a>.</p><p>The predictions are then fed into a MIP which is solved on the fly. We’re using <a href="https://www.cvxpy.org/">cvxpy</a> as a nice generic symbolic front-end to represent the problem which can then be fed into various open-source or proprietary MIP solver backends. Since MIPs are NP-hard, some care needs to be taken. We impose a hard time budget to the solver to drive the branch-and-cut strategy into a low-latency regime, with guardrails around the MIP gap to control overall quality of the solution found.</p><p>The service then returns the placement decision to the host, which executes it by modifying the cpusets of the containers.</p><p>For example, at any moment in time, an r4.16xlarge with 64 logical CPUs might look like this (the color scale represents CPU usage):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4azVDtvp2npDbjGLlJWC7g.png" /></figure><h3>Results</h3><p>The first version of the system led to surprisingly good results. We reduced overall runtime of batch jobs by multiple percent on average while most importantly reducing job runtime variance (a reasonable proxy for isolation), as illustrated below. Here we see a real-world batch job runtime distribution with and without improved isolation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/647/1*6LUqmmzBoqFEbGEYqM4Lhw.png" /></figure><p>Notice how we mostly made the problem of long-running outliers disappear. The right-tail of unlucky noisy-neighbors runs is now gone.</p><p>For services, the gains were even more impressive. One specific Titus middleware service serving the Netflix streaming service saw a capacity reduction of 13% (a decrease of more than 1000 containers) needed at peak traffic to serve the same load with the required P99 latency SLA! We also noticed a sharp reduction of the CPU usage on the machines, since far less time was spent by the kernel in cache invalidation logic. Our containers are now more predictable, faster and the machine is less used! It’s not often that you can have your cake and eat it too.</p><h3>Next Steps</h3><p>We are excited with the strides made so far in this area. We are working on multiple fronts to extend the solution presented here.</p><p>We want to extend the system to support CPU oversubscription. Most of our users have challenges knowing how to properly size the numbers of CPUs their app needs. And in fact, this number varies during the lifetime of their containers. Since we already predict future CPU usage of the containers, we want to automatically detect and reclaim unused resources. For example, one could decide to auto-assign a specific container to a shared cgroup of underutilized CPUs, to better improve overall isolation and machine utilization, if we can detect the sensitivity threshold of our users along the various axes of the following graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/636/1*t0XZWYS4gE3SsF24Ll2MXQ.png" /></figure><p>We also want to leverage kernel <a href="http://www.brendangregg.com/blog/2017-05-04/the-pmcs-of-ec2.html">PMC events</a> to more directly optimize for minimal cache noise. One possible avenue is to use the Intel based bare metal instances <a href="https://aws.amazon.com/about-aws/whats-new/2018/05/announcing-general-availability-of-amazon-ec2-bare-metal-instances/">recently introduced</a> by Amazon that allow deep access to performance analysis tools. We could then feed this information directly into the optimization engine to move towards a more supervised learning approach. This would require a proper continuous randomization of the placements to collect unbiased counterfactuals, so we could build some sort of interference model (“what would be the performance of container A in the next minute, if I were to colocate one of its threads on the same core as container B, knowing that there’s also C running on the same socket right now?”).</p><h3>Conclusion</h3><p>If any of this piques your interest, reach out to us! We’re looking for <a href="https://jobs.netflix.com/jobs/860513">ML engineers</a> to help us push the boundary of containers performance and “machine learning for systems” and <a href="https://jobs.netflix.com/jobs/869888">systems engineers</a> for our core infrastructure and compute platform.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=91f014d856c7" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7">Predictive CPU isolation of containers at Netflix</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making our Android Studio Apps Reactive with UI Components & Redux]]></title>
            <link>https://medium.com/netflix-techblog/making-our-android-studio-apps-reactive-with-ui-components-redux-5e37aac3b244?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/5e37aac3b244</guid>
            <category><![CDATA[mobile-app-development]]></category>
            <category><![CDATA[components]]></category>
            <category><![CDATA[android-app-development]]></category>
            <category><![CDATA[reactive]]></category>
            <category><![CDATA[redux]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 30 May 2019 16:31:00 GMT</pubDate>
            <atom:updated>2019-05-30T17:01:45.342Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://twitter.com/juliano_moraes">Juliano Moraes</a>, <a href="http://www.linkedin.com/in/davidmatthewhenry">David Henry</a>, Corey Grunewald &amp; <a href="https://www.linkedin.com/in/jimpisaacs">Jim Isaacs</a></p><p>Recently Netflix has started building mobile apps to <a href="https://medium.com/netflix-techblog/netflixs-production-technology-voltron-ab0e091d232d">bring technology and innovation to our Studio Physical Productions</a>, the portion of the business responsible for producing our TV shows and movies.</p><p>Our very first mobile app is called Prodicle and was built for Android &amp; iOS using the same reactive architecture in both platforms, which allowed us to build 2 apps from scratch in 3 months with 4 software engineers.</p><p>The app helps production crews organize their shooting days through shooting milestones and keeps everyone in a production informed about what is currently happening.</p><p>Here is a shooting day for <a href="https://twitter.com/GlowNetflix">Glow</a> Season 3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/732/1*n897uPA-m5tiRIya1UM0Rg.png" /></figure><p>We’ve been experimenting with an idea to use reactive components on Android for the last two years. While there are some frameworks that implement this, we wanted to stay very close to the Android native framework. It was extremely important to the team that we did not completely change the way our engineers write Android code.</p><p>We believe reactive components are the key foundation to achieve composable UIs that are scalable, reusable, unit testable and AB test friendly. Composable UIs contribute to fast engineering velocity and produce less side effect bugs.</p><p>Our current player UI in the Netflix Android app is using our first iteration of this <a href="https://www.youtube.com/watch?v=dS9gho9Rxn4">componentization architecture</a>. We took the opportunity with building Prodicle to improve upon what we learned with the Player UI, and build the app from scratch using <a href="https://redux.js.org/">Redux</a>, Components, and 100% Kotlin.</p><h3>Overall Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*26UPuX_lrPMR9NlFIJn1WQ.png" /></figure><h3>Fragments &amp; Activities</h3><blockquote>— Fragment is not your view.</blockquote><p>Having large Fragments or Activities causes all sorts of problems, it makes the code hard to read, maintain, and extend. Keeping them small helps with code encapsulation and better separation of concerns — the presentation logic should be inside a component or a class that represents a view and not in the Fragment.</p><p>This is how a clean Fragment looks in our app, there is no business logic. During the <em>onViewCreated</em> we pass pre-inflated view containers and the global redux store’s dispatch function.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/65817af4e8072d6c38c5ad278d52f34e/href">https://medium.com/media/65817af4e8072d6c38c5ad278d52f34e/href</a></iframe><h3>UI Components</h3><p>Components are responsible for owning their own XML layout and inflating themselves into a container. They implement a single <strong>render(state: ComponentState)</strong> interface and have their state defined by a <a href="https://kotlinlang.org/docs/reference/data-classes.html">Kotlin data class</a>.</p><p>A component’s render method is a pure function that can easily be tested by creating a permutation of possible states variances.</p><p>Dispatch functions are the way components fire actions to change app state, make network requests, communicate with other components, etc.</p><p>A component defines its own state as a data class in the top of the file. That’s how its render() function is going to be invoked by the render loop.</p><p>It receives a ViewGroup container that will be used to inflate the component’s own layout file, <em>R.layout.list_header</em> in this example.</p><p>All the Android views are instantiated using a lazy approach and the render function is the one that will set all the values in the views.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/97113e3cc1ca56b683535ed3c8b5f704/href">https://medium.com/media/97113e3cc1ca56b683535ed3c8b5f704/href</a></iframe><h3>Layout</h3><p>All of these components are independent by design, which means they do not know anything about each other, but somehow we need to layout our components within our screens. The architecture is very flexible and provides different ways of achieving it:</p><ol><li><strong>Self Inflation into a Container</strong>: A Component receives a ViewGroup as a container in the constructor, it inflates itself using Layout Inflater. Useful when the screen has a skeleton of containers or is a Linear Layout.</li><li><strong>Pre inflated views.</strong> Component accepts a View in its constructor, no need to inflate it. This is used when the layout is owned by the screen in a single XML.</li><li><strong>Self Inflation into a Constraint Layout</strong>: Components inflate themselves into a Constraint Layout available in its constructor, it exposes a <em>getMainViewId</em> to be used by the parent to set constraints programmatically.</li></ol><h3>Redux</h3><p>Redux provides an event driven unidirectional data flow architecture through a global and centralized application state that can only be mutated by <em>Actions</em> followed by <em>Reducers</em>. When the app state changes it cascades down to all the subscribed components.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/762/1*FGmgFOUQhmUJ4y24yQ6GjA.png" /></figure><p>Having a centralized app state makes disk persistence very simple using serialization. It also provides the ability to rewind actions that have affected the state for free. After persisting the current state to the disk the next app launch will put the user in exactly the same state they were before. This removes the requirement for all the boilerplate associated with Android’s <em>onSaveInstanceState()</em> and <em>onRestoreInstanceState()</em>.</p><p>The Android <em>FragmentManager</em> has been abstracted away in favor of Redux managed navigation. Actions are fired to Push, Pop, and Set the current route. Another Component, <em>NavigationComponent</em> listens to changes to the <em>backStack</em> and handles the creation of new Screens.</p><h3>The Render Loop</h3><p>Render Loop is the mechanism which loops through all the components and invokes <strong>component.render()</strong> if it is needed.</p><p>Components need to subscribe to changes in the App State to have their <em>render()</em> called. For optimization purposes, they can specify a transformation function containing the portion of the App State they care about — using <em>selectWithSkipRepeats</em> prevents unnecessary render calls if a part of the state changes that the component does not care about.</p><p>The ComponentManager is responsible for subscribing and unsubscribing Components. It extends Android <em>ViewModel</em> to persist state on configuration change, and has a 1:1 association with Screens (<em>Fragments</em>). It is lifecycle aware and unsubscribes all the components when <em>onDestroy</em> is called.</p><p>Below is our fragment with its subscriptions and transformation functions:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/005c86a58d754b5fdfc51d92afd0d65f/href">https://medium.com/media/005c86a58d754b5fdfc51d92afd0d65f/href</a></iframe><p>ComponentManager code is below:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/92e0281c2f38f57f310088ed20eafd52/href">https://medium.com/media/92e0281c2f38f57f310088ed20eafd52/href</a></iframe><h3>Recycler Views</h3><p>Components should be flexible enough to work inside and outside of a list. To work together with Android’s <em>recyclerView</em> implementation we’ve created a <em>UIComponent</em> and <em>UIComponentForList</em>, the only difference is the second extends a <em>ViewHolder</em> and does not subscribe directly to the Redux Store.</p><p>Here is how all the pieces fit together.</p><p><strong>Fragment:</strong></p><p>The Fragment initializes a <em>MilestoneListComponent</em> subscribing it to the Store and implements its transformation function that will define how the global state is translated to the component state.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e196b69ae7f18ef923067719d7ba2736/href">https://medium.com/media/e196b69ae7f18ef923067719d7ba2736/href</a></iframe><p><strong>List Component:</strong></p><p>A List Component uses a custom adapter that supports multiple component types, provides async diff in the background thread through <em>adapter.update()</em> interface and invokes item components <em>render()</em> function during <em>onBind()</em> of the list item.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a52c04f81211cdff3e73631793938204/href">https://medium.com/media/a52c04f81211cdff3e73631793938204/href</a></iframe><p><strong>Item List Component:</strong></p><p>Item List Components can be used outside of a list, they look like any other component except for the fact that <em>UIComponentForList</em> extends Android’s <em>ViewHolder</em> class. As any other component it implements the render function based on a state data class it defines.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5ec6ad5eb14ac4d326c24a6daf38d8ab/href">https://medium.com/media/5ec6ad5eb14ac4d326c24a6daf38d8ab/href</a></iframe><h3>Unit Tests</h3><p>Unit tests on Android are generally hard to implement and slow to run. Somehow we need to mock all the dependencies — Activities, Context, Lifecycle, etc in order to start to test the code.</p><p>Considering our components render methods are pure functions we can easily test it by making up states without any additional dependencies.</p><p>In this unit test example we initialize a UI Component inside the <em>before() </em>and for every test we directly invoke the <em>render() </em>function with a state that we define. There is no need for activity initialization or any other dependency.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f554a92a038d591d1bb5aa3a80712395/href">https://medium.com/media/f554a92a038d591d1bb5aa3a80712395/href</a></iframe><h3>Conclusion &amp; Next Steps</h3><p>The first version of our app using this architecture was released a couple months ago and we are very happy with the results we’ve achieved so far. It has proven to be composable, reusable and testable — currently we have 60% unit test coverage.</p><p>Using a common architecture approach allows us to move very fast by having one platform implement a feature first and the other one follow. Once the data layer, business logic and component structure is figured out it becomes very easy for the following platform to implement the same feature by translating the code from Kotlin to Swift or vice versa.</p><p>To fully embrace this architecture we’ve had to think a bit outside of the platform’s provided paradigms. The goal is not to fight the platform, but instead to smooth out some rough edges.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5e37aac3b244" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/making-our-android-studio-apps-reactive-with-ui-components-redux-5e37aac3b244">Making our Android Studio Apps Reactive with UI Components &amp; Redux</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Android Rx onError Guidelines]]></title>
            <link>https://medium.com/netflix-techblog/android-rx-onerror-guidelines-e68e8dc7383f?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e68e8dc7383f</guid>
            <category><![CDATA[rx]]></category>
            <category><![CDATA[rxjava]]></category>
            <category><![CDATA[rxjava2]]></category>
            <category><![CDATA[api]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 01 May 2019 15:39:23 GMT</pubDate>
            <atom:updated>2019-05-01T22:52:22.392Z</atom:updated>
            <content:encoded><![CDATA[<h3>Rx onError Guidelines</h3><p>By Ed Ballot</p><p>“Creating a good API is hard.” — <em>anyone who has created an API used by others</em></p><p>As with any API, wrapping your data stream in a Rx observable requires consideration for reasonable error handling and intuitive behavior. The following guidelines are intended to help developers create consistent and intuitive API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/100/0*J5zoGiTTNPf1TeaL.png" /></figure><p>Since we frequently create Rx Observables in our Android app, we needed a common understanding of when to use onNext() and when to use onError() to make the API more consistent for subscribers. The divergent understanding is partially because the name “onError” is a bit misleading. The item emitted by onError() is not a simple error, but a throwable that can cause significant damage if not caught. Our app has a global handler that prevents it from crashing outright, but an uncaught exception can still leave parts of the app in an unpredictable state.</p><p><strong>TL;DR</strong> — Prefer onNext() and only use onError() for exceptional cases.</p><h3>Considerations for onNext / onError</h3><p>The following are points to consider when determining whether to use onNext() versus onError().</p><h3>The Contract</h3><p>First here are the definitions of the two from the ReactiveX <a href="http://reactivex.io/documentation/contract.html">contract page</a>:</p><blockquote><strong><em>OnNext<br></em></strong><em>conveys an </em>item<em> that is </em>emitted<em> by the Observable to the observer</em></blockquote><blockquote><strong><em>OnError<br></em></strong><em>indicates that the Observable has terminated with a specified error condition and that it will be emitting no further items</em></blockquote><p>As pointed out in the above definition, a subscription is automatically disposed after onError(), just like after onComplete(). Because of this, onError() should only be used to signal a fatal error and never to signal an intermittent problem where more data is expected to stream through the subscription after the error.</p><h3>Treat it like an Exception</h3><p>Limit using onError() for exceptional circumstances when you’d also consider throwing an Error or Exception. The reasoning is that the onError() parameter is a Throwable. An example for differentiating: a database query returning zero results is typically not an exception. The database returning zero results because it was forcibly closed (or otherwise put in a state that cancels the running query) would be an exceptional condition.</p><h3>Be Consistent</h3><p>Do not make your observable emit a mix of both deterministic and non-deterministic errors. Something is deterministic if the same input always result in the same output, such as dividing by 0 will fail every time. Something is non-deterministic if the same inputs may result in different outputs, such as a network request which may timeout or may return results before the timeout. Rx has convenience methods built around error handling, such as <a href="http://reactivex.io/documentation/operators/retry.html">retry()</a> (and our retryWithBackoff()). The primary use of retry() is to automatically re-subscribe an observable that has non-deterministic errors. When an observable mixes the two types of errors, it makes retrying less obvious since retrying a deterministic failures doesn’t make sense — or is wasteful since the retry is guaranteed to fail. (Two notes: 1. retry can also be used in certain deterministic cases like user login attempts, where the failure is caused by incorrectly entering credentials. 2. For mixed errors, retryWhen() could be used to only retry the non-deterministic errors.) If you find your observable needs to emit both types of errors, consider whether there is an appropriate separation of concerns. It may be that the observable can be split into several observables that each have a more targeted purpose.</p><h3>Be Consistent with Underlying APIs</h3><p>When wrapping an asynchronous API in Rx, consider maintaining consistency with the underlying API’s error handling. For example, if you are wrapping a touch event system that treats moving off the device’s touchscreen as an exception and terminates the touch session, then it may make sense to emit that error via onError(). On the other hand, if it treats moving off the touchscreen as a data event and allows the user to drag their finger back onto the screen, it makes sense to emit it via onNext().</p><h3>Avoid Business Logic</h3><p>Related to the previous point. Avoid adding business logic that interprets the data and converts it into errors. The code that the observable is wrapping should have the appropriate logic to perform these conversions. In the rare case that it does not, consider adding an abstraction layer that encapsulates this logic (for both normal and error cases) rather than building it into the observable.</p><h3>Passing Details in onError()</h3><p>If your code is going to use onError(), remember that the throwable it emits should include appropriate data for the subscriber to understand what went wrong and how to handle it.</p><p>For example, our Falcor response handler uses a FalcorError class that includes the Status from the callback. Repositories could also throw an extension of this class, if extra details need to be included.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e68e8dc7383f" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/android-rx-onerror-guidelines-e68e8dc7383f">Android Rx onError Guidelines</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Engineering a Studio Quality Experience With High-Quality Audio at Netflix]]></title>
            <link>https://medium.com/netflix-techblog/engineering-a-studio-quality-experience-with-high-quality-audio-at-netflix-eaa0b6145f32?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/eaa0b6145f32</guid>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 01 May 2019 13:01:01 GMT</pubDate>
            <atom:updated>2019-05-01T13:01:01.306Z</atom:updated>
            <content:encoded><![CDATA[<p><em>by Guillaume du Pontavice, Phill Williams and Kylee Peña (on behalf of our Streaming Algorithms, Audio Algorithms, and Creative Technologies teams)</em></p><p>Remember the epic opening sequence of <em>Stranger Things 2</em>? The thrill of that car chase through Pittsburgh not only introduced a whole new set of mysteries, but it returned us to a beloved and dangerous world alongside Dustin, Lucas, Mike, Will and Eleven. Maybe you were one of the millions of people who watched it in HDR, experiencing the brilliant imagery as it was meant to be seen by the creatives who dreamt it up.</p><p>Imagine this scene without the sound. Even taking away one part of the soundtrack — the brilliant synth-pop score or the perfectly mixed soundscape of a high speed chase — is the story nearly as thrilling and emotional?</p><p>Most conversations about streaming quality focus on <em>video</em>. In fact, Netflix has led the charge for most of the video technology that drives these conversations, from visual quality improvements like 4K and HDR, to behind-the-scenes technologies that make the streaming experience better for everyone, like adaptive streaming, complexity-based encoding, and AV1.</p><p>We’re really proud of the improvements we’ve brought to the video experience, but the focus on those makes it easy to overlook the importance of <em>sound</em>, and sound is every bit as important to entertainment as video. Variances in sound can be extremely subtle, but the impact on how the viewer perceives a scene differently is often measurable. For example, have you ever seen a TV show where the video and audio were a <em>little</em> out of sync?</p><p>Among those who understand the vital nature of sound are the Duffer brothers. In late 2017, we received some critical feedback from the brothers on the <em>Stranger Things 2</em> audio mix: in some scenes, there was a reduced sense of where sounds are located in the 5.1-channel stream, as well as audible degradation of high frequencies.</p><p>Our engineering team and Creative Technologies sound expert joined forces to quickly solve the issue, but a larger conversation about higher quality audio continued. Series mixes were getting bolder and more cinematic with tight levels between dialog, music and effects elements. Creative choices increasingly tested the limits of our encoding quality. We needed to support these choices better.</p><p>At Netflix, we work hard to bring great audio to our members. We began streaming 5.1 surround audio in 2010, and <a href="https://media.netflix.com/en/company-blog/dolby-atmos-coming-to-netflix">began streaming Dolby Atmos in 2016</a>, but wanted to bring <em>studio quality</em> sound to our members around the world. We want your experience to be brilliant even if you aren’t listening with a state-of-the-art home theater system. Just as we support initiatives like HDR and <a href="https://media.netflix.com/en/company-blog/netflix-calibrated-mode-brings-studio-quality-picture-mastering-to-the-living-room">Netflix Calibrated Mode</a> to maintain creative intent in streaming you <em>picture</em>, we wanted to do the same for the sound. That’s why we developed and launched high-quality audio.</p><p>To learn more about the people and inspiration behind this effort, <a href="https://www.youtube.com/watch?v=_eqdt_UOBAQ&amp;feature=youtu.be">check out this video</a>. In this tech blog, we’ll dive deep into what high-quality audio is, how we deliver it to members worldwide, and why it’s so important to us.</p><h3>What do we mean by “studio quality” sound?</h3><p>If you’ve ever been in a professional recording studio, you’ve probably noted the difference in how things sound. One reason for that is the files used in mastering sessions are 24-bit 48 kHz with a bitrate of around 1 Mbps per channel. Studio mixes are uncompressed, which is why we consider them to be the “master” version.</p><p>Our high-quality sound feature is not lossless, but it is <strong>perceptually transparent</strong>. That means that while the audio is compressed, it is indistinguishable from the original source. Based on internal listening tests, listening test results provided by Dolby, and scientific studies, we determined that for Dolby Digital Plus at and above 640 kbps, the audio coding quality is perceptually transparent. Beyond that, we would be sending you files that have a higher bitrate (and take up more bandwidth) without bringing any additional value to the listening experience.</p><p>In addition to deciding 640 kbps — a 10:1 compression ratio when compared to a 24-bit 5.1 channel studio master — was the perceptually transparent threshold for audio, we set up a bitrate ladder for 5.1-channel audio ranging from 192 up to 640 kbps. This ranges from “good” audio to “transparent” — there aren’t any bad audio experiences when you stream!</p><p>At the same time, we revisited our Dolby Atmos bitrates and increased the highest offering to 768 kbps. We expect these bitrates to evolve over time as we get more efficient with our encoding techniques.</p><p>Our high-quality sound is a great experience for our members even if they aren’t audiophiles. Sound helps to tell the story subconsciously, shaping our experience through subtle cues like the sharpness of a phone ring or the way a very dense flock of bird chirps can increase anxiety in a scene. Although variances in sound can be nuanced, the impact on the viewing and listening experience is often measurable.</p><p>And perhaps most of all, our “studio quality” sound is faithful to what the mixers are creating on the mix stage. For many years in the film and television industry, creatives would spend days on the stage perfecting the mix only to have it significantly degraded by the time it was broadcast to viewers. Sometimes critical sound cues might even be lost to the detriment of the story. By delivering studio quality sound, we’re preserving the creative intent from the mix stage.</p><h3>Adaptive Streaming for Audio</h3><p>Since we began streaming, we’ve used static audio streaming at a constant bitrate. This approach selects the audio bitrate based on network conditions at the start of playback. However, we have spent years optimizing our adaptive streaming engine for video, so we know adaptive streaming has obvious benefits. Until now, we’ve only used adaptive streaming for video.</p><p>Adaptive streaming is a technology designed to deliver media to the user in the most optimal way for their network connection. Media is split into many small segments (chunks) and each chunk contains a few seconds of playback data. Media is provided in several qualities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*TCrxb3YZ1gtzpvvj" /></figure><p>An adaptive streaming algorithm’s goal is to provide the best overall playback experience — even under a constrained environment. A great playback experience should provide the best overall quality, considering both audio and video, and avoid buffer starvation which leads to a rebuffering event — or playback interruption.</p><p>Constrained environments can be due to changing network conditions and device performance limitations. Adaptive streaming has to take all these into account. Delivering a great playback experience is difficult.</p><p>Let’s first look at how static audio streaming paired with adaptive video operates in a session with variable network conditions — in this case, a sudden throughput drop during the session.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*p9195Fk8I4BxGyTE" /></figure><p>The top graph shows both the audio and video bitrate, along with the available network throughput. The audio bitrate is fixed and has been selected at playback start whereas video bitrate varies and can adapt periodically.</p><p>The bottom graph shows audio and video buffer evolution: if we are able to fill the buffer faster than we play out, our buffer will grow. If not, our buffer will shrink.</p><p>In the first session above, the adaptive streaming algorithm for video has reacted to the throughput drop and was able to quickly stabilize both the audio and video buffer level by down-switching the video bitrate.</p><p>In the second scenario below, under the same network conditions we used a static <strong>high-quality</strong> audio bitrate at session start instead.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2pCm04mMfbEaCs3B" /></figure><p>Our adaptive streaming for video logic is reacting but in this case, the available throughput is becoming less than the sum of audio and video bitrate, and our buffer starts draining. This ultimately leads to a rebuffer.</p><p>In this scenario, the video bitrate dropped below the audio bitrate, which might not provide the best playback experience.</p><p>This simple example highlights that static audio streaming can lead to suboptimal playback experiences with fluctuating network conditions. This motivated us to use <strong>adaptive streaming for audio.</strong></p><p>By using adaptive streaming for audio, <strong>we allow audio quality to adjust during playback to bandwidth capabilities, just like we do for video.</strong></p><p>Let’s consider a playback session with exactly the same network conditions (a sudden throughput drop) to illustrate the benefit of adaptive streaming for audio.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*90pVfZ2UPHxoOJCX" /></figure><p>In this case we are able to select a higher audio bitrate when network conditions supported it and we are able to gracefully switch down the audio bitrate and avoid a rebuffer event by maintaining healthy audio and video buffer levels. Moreover, we were able to maintain a higher video bitrate when compared to the previous example.</p><p>The benefits are obvious in this simple case, but extending it to our broad streaming ecosystem was another challenge. There were many questions we had to answer in order to move forward with adaptive streaming for audio.</p><p><strong>What about device reach? </strong>We have hundreds of millions of TV devices in the field, with different CPU, network and memory profiles, and adaptive audio has never been certified. Do these devices even support audio stream switching?</p><ul><li>We had to assess this by testing adaptive audio switching on all Netflix supported devices.</li><li>We also added adaptive audio testing in our certification process so that every new certified device can benefit from it.</li></ul><p>Once we knew that adaptive streaming for audio was achievable on most of our TV devices, we had to answer the following questions as we <strong>designed the algorithm</strong>:</p><ul><li>How could we guarantee that we can improve audio subjective quality without degrading video quality and vice-versa?</li><li>How could we guarantee that we won’t introduce additional rebuffers or increase the startup delay with high-quality audio?</li><li>How could we guarantee that this algorithm will gracefully handle devices with different performance characteristics?</li></ul><p>We answered these questions via experimentation that led to fine-tuning the adaptive streaming for audio algorithm in order to increase audio quality without degrading the video experience. After a year of work, we were able to answer these questions and implement adaptive audio streaming on a majority of TV devices.</p><h3>Enjoying a Higher Quality Experience</h3><p>By using our listening tests and scientific data to choose an optimal “transparent” bitrate, and designing an adaptive audio algorithm that could serve it based on network conditions, we’ve been able to enable this feature on a wide variety of devices with different CPU, network and memory profiles: the vast majority of our members using 5.1 should be able to enjoy new high-quality audio.</p><p>And it won’t have any negative impact on the streaming experience. The adaptive bitrate switching happens seamlessly during a streaming experience, with the available bitrates ranging from good to transparent, so you shouldn’t notice a difference other than better sound. If your network conditions are good, you’ll be served up the best possible audio, and it will now likely sound like it did on the mixing stage. If your network has an issue — your sister starts a huge download or your cat unplugs your router — our adaptive streaming will help you out.</p><p>After years perfecting our adaptive video switching, we’re thrilled that a similar approach can enable studio quality sound to make it to members’ households, ensuring that every detail of the mix is preserved. Uniquely combining creative technology with engineering teams at Netflix, we’ve been able to not only solve a problem, but use that problem to improve the quality of audio for millions of our members worldwide.</p><p>Preserving the original creative intent of the hard-working people who make shows like <em>Stranger Things</em> is a top priority, and we know it enhances your viewing — and listening — experience for many more moments of joy. Whether you’ve fallen into the Upside Down or you’re being chased by the Demogorgon, get ready for a sound experience like never before.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eaa0b6145f32" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/engineering-a-studio-quality-experience-with-high-quality-audio-at-netflix-eaa0b6145f32">Engineering a Studio Quality Experience With High-Quality Audio at Netflix</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Python at Netflix]]></title>
            <link>https://medium.com/netflix-techblog/python-at-netflix-bba45dae649e?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/bba45dae649e</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[netflixoss]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[netflix]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 29 Apr 2019 16:12:01 GMT</pubDate>
            <atom:updated>2019-04-30T21:35:41.432Z</atom:updated>
            <content:encoded><![CDATA[<p><em>By Pythonistas at Netflix, coordinated by Amjith Ramanujam and edited by Ellen Livengood</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/601/1*PPIp7twJJUknfohZqtL8pQ.png" /></figure><p>As many of us prepare to go to PyCon, we wanted to share a sampling of how Python is used at Netflix. We use Python through the full content lifecycle, from deciding which content to fund all the way to operating the CDN that serves the final video to 148 million members. We use and contribute to many open-source Python packages, some of which are mentioned below. If any of this interests you, check out the <a href="https://jobs.netflix.com/search?q=python">jobs site</a> or find us at PyCon. We have donated a few Netflix Originals posters to the <a href="https://us.pycon.org/2019/events/auction/">PyLadies Auction</a> and look forward to seeing you all there.</p><h4>Open Connect</h4><p><a href="https://openconnect.netflix.com/en/">Open Connect</a> is Netflix’s content delivery network (CDN). An easy, though imprecise, way of thinking about Netflix infrastructure is that everything that happens before you press Play on your remote control (e.g., are you logged in? what plan do you have? what have you watched so we can recommend new titles to you? what do you want to watch?) takes place in Amazon Web Services (AWS), whereas everything that happens afterwards (i.e., video streaming) takes place in the Open Connect network. Content is placed on the network of servers in the Open Connect CDN as close to the end user as possible, improving the streaming experience for our customers and reducing costs for both Netflix and our Internet Service Provider (ISP) partners.</p><p>Various software systems are needed to design, build, and operate this CDN infrastructure, and a significant number of them are written in Python. The network devices that underlie a large portion of the CDN are mostly managed by Python applications. Such applications track the inventory of our network gear: what devices, of which models, with which hardware components, located in which sites. The configuration of these devices is controlled by several other systems including source of truth, application of configurations to devices, and back up. Device interaction for the collection of health and other operational data is yet another Python application. Python has long been a popular programming language in the networking space because it’s an intuitive language that allows engineers to quickly solve networking problems. Subsequently, many useful libraries get developed, making the language even more desirable to learn and use.</p><h4>Demand Engineering</h4><p><a href="https://www.linkedin.com/pulse/what-demand-engineering-aaron-blohowiak/">Demand Engineering</a> is responsible for <a href="https://opensource.com/article/18/4/how-netflix-does-failovers-7-minutes-flat">Regional Failovers</a>, Traffic Distribution, Capacity Operations, and Fleet Efficiency of the Netflix cloud. We are proud to say that our team’s tools are built primarily in Python. The service that orchestrates failover uses numpy and scipy to perform numerical analysis, boto3 to make changes to our AWS infrastructure, rq to run asynchronous workloads and we wrap it all up in a thin layer of Flask APIs. The ability to drop into a <a href="https://bpython-interpreter.org">bpython</a> shell and improvise has saved the day more than once.</p><p>We are heavy users of Jupyter Notebooks and <a href="https://nteract.io">nteract</a> to analyze operational data and prototype <a href="https://github.com/nteract/nteract/tree/master/packages/data-explorer">visualization tools </a>that help us detect capacity regressions.</p><h4>CORE</h4><p>The CORE team uses Python in our alerting and statistical analytical work. We lean on many of the statistical and mathematical libraries (numpy, scipy, ruptures, pandas) to help automate the analysis of 1000s of related signals when our alerting systems indicate problems. We’ve developed a time series correlation system used both inside and outside the team as well as a distributed worker system to parallelize large amounts of analytical work to deliver results quickly.</p><p>Python is also a tool we typically use for automation tasks, data exploration and cleaning, and as a convenient source for visualization work.</p><h4>Monitoring, alerting and auto-remediation</h4><p>The Insight Engineering team is responsible for building and operating the tools for operational insight, alerting, diagnostics, and auto-remediation. With the increased popularity of Python, the team now supports Python clients for most of their services. One example is the <a href="https://github.com/Netflix/spectator-py">Spectator</a> Python client library, a library for instrumenting code to record dimensional time series metrics. We build Python libraries to interact with other Netflix platform level services. In addition to libraries, the <a href="https://medium.com/netflix-techblog/introducing-winston-event-driven-diagnostic-and-remediation-platform-46ce39aa81cc">Winston</a> and <a href="https://medium.com/netflix-techblog/introducing-bolt-on-instance-diagnostic-and-remediation-platform-176651b55505">Bolt</a> products are also built using Python frameworks (Gunicorn + Flask + Flask-RESTPlus).</p><h4>Information Security</h4><p>The information security team uses Python to accomplish a number of high leverage goals for Netflix: security automation, risk classification, auto-remediation, and vulnerability identification to name a few. We’ve had a number of successful Python open sources, including <a href="https://github.com/Netflix/security_monkey">Security Monkey</a> (our team’s most active open source project). We leverage Python to protect our SSH resources using <a href="https://github.com/Netflix/bless">Bless</a>. Our Infrastructure Security team leverages Python to help with IAM permission tuning using <a href="https://github.com/Netflix/repokid">Repokid</a>. We use Python to help generate TLS certificates using <a href="https://github.com/Netflix/lemur">Lemur</a>.</p><p>Some of our more recent projects include Prism: a batch framework to help security engineers measure paved road adoption, risk factors, and identify vulnerabilities in source code. We currently provide Python and Ruby libraries for Prism. The <a href="https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698">Diffy</a> forensics triage tool is written entirely in Python. We also use Python to detect sensitive data using Lanius.</p><h4>Personalization Algorithms</h4><p>We use Python extensively within our broader <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">Personalization Machine Learning Infrastructure</a> to train some of the Machine Learning models for key aspects of the Netflix experience: from our <a href="https://research.netflix.com/research-area/recommendations">recommendation algorithms</a> to <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">artwork personalization</a> to <a href="https://medium.com/netflix-techblog/engineering-to-scale-paid-media-campaigns-84ba018fb3fa">marketing algorithms</a>. For example, some algorithms use TensorFlow, Keras, and PyTorch to learn Deep Neural Networks, XGBoost and LightGBM to learn Gradient Boosted Decision Trees or the broader scientific stack in Python (e.g. numpy, scipy, sklearn, matplotlib, pandas, cvxpy). Because we’re constantly trying out new approaches, we use Jupyter Notebooks to drive many of our experiments. We have also developed a number of higher-level libraries to help integrate these with the rest of our <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">ecosystem</a> (e.g. data access, fact logging and feature extraction, model evaluation, and publishing).</p><h4>Machine Learning Infrastructure</h4><p>Besides personalization, Netflix applies machine learning to hundreds of use cases across the company. Many of these applications are powered by <a href="https://www.youtube.com/watch?v=XV5VGddmP24">Metaflow</a>, a Python framework that makes it easy to execute ML projects from the prototype stage to production.</p><p>Metaflow pushes the limits of Python: We leverage well parallelized and optimized Python code to fetch data at 10Gbps, handle hundreds of millions of data points in memory, and orchestrate computation over tens of thousands of CPU cores.</p><h4>Notebooks</h4><p>We are avid users of Jupyter notebooks at Netflix, and we’ve written about the <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">reasons and nature of this investment</a> before.</p><p>But Python plays a huge role in how we provide those services. Python is a primary language when we need to develop, debug, explore, and prototype different interactions with the Jupyter ecosystem. We use Python to build custom extensions to the Jupyter server that allows us to manage tasks like logging, archiving, publishing, and cloning notebooks on behalf of our users.<br>We provide many flavors of Python to our users via different Jupyter kernels, and manage the deployment of those kernel specifications using Python.</p><h4>Orchestration</h4><p>The Big Data Orchestration team is responsible for providing all of the services and tooling to schedule and execute ETL and Adhoc pipelines.</p><p>Many of the components of the orchestration service are written in Python. Starting with our scheduler, which uses<a href="https://jupyter.org"> Jupyter Notebooks</a> with <a href="https://papermill.readthedocs.io/en/latest/">papermill</a> to provide templatized job types (Spark, Presto, …). This allows our users to have a standardized and easy way to express work that needs to be executed. You can see some deeper details on the subject <a href="https://medium.com/netflix-techblog/scheduling-notebooks-348e6c14cfd6">here</a>. We have been using notebooks as real runbooks for situations where human intervention is required — for example: to restart everything that has failed in the last hour.</p><p>Internally, we also built an event-driven platform that is fully written in Python. We have created streams of events from a number of systems that get unified into a single tool. This allows us to define conditions to filter events, and actions to react or route them. As a result of this, we have been able to decouple microservices and get visibility into everything that happens on the data platform.</p><p>Our team also built the <a href="https://github.com/Netflix/pygenie">pygenie</a> client which interfaces with <a href="https://netflix.github.io/genie/">Genie</a>, a federated job execution service. Internally, we have additional extensions to this library that apply business conventions and integrate with the Netflix platform. These libraries are the primary way users interface programmatically with work in the Big Data platform.</p><p>Finally, it’s been our team’s commitment to contribute to <a href="https://papermill.readthedocs.io/en/latest/">papermill</a> and <a href="https://nteract-scrapbook.readthedocs.io/en/latest/">scrapbook</a> open source projects. Our work there has been both for our own and external use cases. These efforts have been gaining a lot of traction in the open source community and we’re glad to be able to contribute to these shared projects.</p><h4>Experimentation Platform</h4><p>The scientific computing team for experimentation is creating a platform for scientists and engineers to analyze AB tests and other experiments. Scientists and engineers can contribute new innovations on three fronts, data, statistics, and visualizations.</p><p>The Metrics Repo is a Python framework based on <a href="https://pypika.readthedocs.io/en/latest/">PyPika</a> that allows contributors to write reusable parameterized SQL queries. It serves as an entry point into any new analysis.</p><p>The Causal Models library is a Python &amp; R framework for scientists to contribute new models for causal inference. It leverages <a href="https://arrow.apache.org/docs/python/">PyArrow</a> and <a href="https://rpy2.readthedocs.io/en/version_2.8.x/">RPy2</a> so that statistics can be calculated seamlessly in either language.</p><p>The Visualizations library is based on <a href="https://plot.ly">Plotly</a>. Since Plotly is a widely adopted visualization spec, there are a variety of tools that allow contributors to produce an output that is consumable by our platforms.</p><h4>Partner Ecosystem</h4><p>The Partner Ecosystem group is expanding its use of Python for testing Netflix applications on devices. Python is forming the core of a new CI infrastructure, including controlling our orchestration servers, controlling Spinnaker, test case querying and filtering, and scheduling test runs on devices and containers. Additional post-run analysis is being done in Python using TensorFlow to determine which tests are most likely to show problems on which devices.</p><h4>Video Encoding and Media Cloud Engineering</h4><p>Our team takes care of encoding (and re-encoding) the Netflix catalog, as well as leveraging machine learning for insights into that catalog.<br>We use Python for ~50 projects such as <a href="https://github.com/Netflix/vmaf/blob/master/resource/doc/references.md">vmaf</a> and <a href="https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba">mezzfs</a>, we build <a href="https://medium.com/netflix-techblog/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6">computer vision solutions</a> using a media map-reduce platform called <a href="https://medium.com/netflix-techblog/simplifying-media-innovation-at-netflix-with-archer-3f8cbb0e2bcb">Archer</a>, and we use Python for many internal projects.<br>We have also open sourced a few tools to ease development/distribution of Python projects, like <a href="https://pypi.org/project/setupmeta/">setupmeta</a> and <a href="https://pypi.org/project/pickley/">pickley</a>.</p><h4>Netflix Animation and NVFX</h4><p>Python is the industry standard for all of the major applications we use to create Animated and VFX content, so it goes without saying that we are using it very heavily. All of our integrations with Maya and Nuke are in Python, and the bulk of our Shotgun tools are also in Python. We’re just getting started on getting our tooling in the cloud, and anticipate deploying many of our own custom Python AMIs/containers.</p><h4>Content Machine Learning, Science &amp; Analytics</h4><p>The Content Machine Learning team uses Python extensively for the development of machine learning models that are the core of forecasting audience size, viewership, and other demand metrics for all content.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bba45dae649e" width="1" height="1"><hr><p><a href="https://medium.com/netflix-techblog/python-at-netflix-bba45dae649e">Python at Netflix</a> was originally published in <a href="https://medium.com/netflix-techblog">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>